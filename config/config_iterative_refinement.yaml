
# ================================
# Data Configuration
# ================================
data:
  path: "xxx.json" # path for RE-Edit json file
  categories:  # Five human-logic reasoningcategories 
    - "physical"
    - "environmental"
    - "cultural"
    - "causal"
    - "referential"


# ================================
# Diffusion Model Configuration
# ================================

diffusion_model:
  primary:
    type: multi_gpu_qwen_edit
    params:
      model_name: "Qwen/Qwen-Image-Edit" # HuggingFace or local path
      device_ids: [0, 1, 2, 3, 4, 5]  # GPU IDs to use
      enable_batch_sync: true  # Enable batch sync
      seed: 21414
      num_inference_steps: 50

# ---------- Qwen-Image-Edit-2511 (commented template; uncomment to use) ----------
# Uses QwenImageEditPlusPipeline (new version).
# diffusion_model:
#   primary:
#     type: qwen_image_edit_2511
#     params:
#       model_name: "Qwen/Qwen-Image-Edit-2511"  # HuggingFace or local path
#       device_ids: [0, 1, 2, 3, 4, 5]
#       enable_batch_sync: true
#       seed: 0
#       num_inference_steps: 40
#       true_cfg_scale: 4.0
#       guidance_scale: 1.0      # New-version specific
#       num_images_per_prompt: 1  # New-version specific
#       negative_prompt: " "
#       dtype: "bfloat16"
#       disable_progress_bar: false


# ---------- Step1X-Edit v1p2 Preview (commented template; uncomment to use) ----------
# diffusion_model:
#   primary:
#     type: step1x_edit_v1p2_preview
#     params:
#       model_name: "stepfun-ai/Step1X-Edit-v1p2-preview"  # HuggingFace or local path
#       device_ids: [0, 1, 2, 3, 4, 5]
#       enable_batch_sync: true
#       seed: 32123
#       num_inference_steps: 28
#       true_cfg_scale: 4
#       dtype: "bfloat16"
#       disable_progress_bar: false
#       enable_thinking_mode: false
#       enable_reflection_mode: false
#       conda_env: "step1x_dedicated_env" # Required: conda env 


# ---------- Step1X-Edit v1p1 (commented template; uncomment to use) ----------
# diffusion_model:
#   primary:
#     type: step1x_edit_v1p1
#     params:
#       model_name: "stepfun-ai/Step1X-Edit-v1p1-diffusers"  # HuggingFace or local path
#       device_ids: [0, 1, 2,3,4,5]
#       enable_batch_sync: true
#       seed: 3212313
#       num_inference_steps: 28
#       #size_level: 1024           # Deprecated, do not use
#       guidance_scale: 6.0        # v1p1 uses guidance_scale
#       dtype: "bfloat16"
#       disable_progress_bar: false
#       conda_env: "step1x_dedicated_env" # Required: conda env 


# ---------- Janus (commented template; uncomment to use) ----------
# Janus uses three-condition CFG: cond_full, cond_part, uncond. Output 384×384, auto-resized to original.
# diffusion_model:
#   primary:
#     type: janus
#     params:
#       model_path: "FreedomIntelligence/Janus-4o-7B"  # HuggingFace or local path
#       janus_repo: "./Janus"  # Janus repo path (required, contains janus.models)
#       device_ids: [0, 1, 2, 3, 4, 5]
#       conda_env: "RE-Edit"  # Required: conda env (can reuse main environment)
#       seed: 135134
#       temperature: 1.0       # Sampling temperature
#       parallel_size: 1       # Parallel generation count (first image used)
#       cfg_weight: 5.0
#       cfg_weight2: 5.0
#       dtype: "bfloat16"
#       disable_progress_bar: false


# ---------- Ovis-U1 (commented template; uncomment to use) ----------
# Ovis-U1 uses three-condition guidance: no_both_cond, no_txt_cond, cond (image+text).
# diffusion_model:
#   primary:
#     type: ovis_u1
#     params:
#       model_path: "AIDC-AI/Ovis-U1-3B"  # HuggingFace or local path
#       ovis_repo: "./Ovis-U1"  # Ovis-U1 repo path 
#       device_ids: [0, 1, 2, 3, 4, 5]
#       conda_env: "ovis-u1_dedicated_env"  # Required: conda env 
#       seed: 14314
#       num_inference_steps: 50
#       img_cfg: 1.5          # Image guidance strength
#       txt_cfg: 6.0          # Text guidance strength
#       dtype: "bfloat16"
#       disable_progress_bar: false


# ---------- HiDream-E1.1 (commented template; uncomment to use) ----------
# Three-layer architecture: LLaMA (text encoder) + HiDream-I1 (base) + HiDream-E1 (edit weights).
# diffusion_model:
#   primary:
#     type: hidream_e1
#     params:
#       llama_path: "meta-llama/Llama-3.1-8B-Instruct"  # LLaMA path (local or HuggingFace)
#       hidream_i1_path: "HiDream-ai/HiDream-I1-Full"   # HiDream-I1 base path (local or HuggingFace)
#       hidream_e1_path: "HiDream-ai/HiDream-E1-1"      # HiDream-E1 edit weights path (local or HuggingFace)
#       hidream_repo: "./HiDream-E1"        # HiDream-E1 repo (contains pipeline_hidream_image_editing.py)
#       device_ids: [0, 1, 2, 3, 4, 5]
#       conda_env: "RE-Edit"  # Required: conda env (can reuse main environment)
#       seed: 3
#       num_inference_steps: 28
#       guidance_scale: 3.0         # Text guidance strength
#       img_guidance_scale: 1.5     # Image guidance strength
#       refine_strength: 0.3        # Refinement strength
#       clip_cfg_norm: true         # Use CLIP CFG normalization
#       negative_prompt: "low quality, blurry, distorted"
#       dtype: "bfloat16"
#       disable_progress_bar: false


# ---------- OmniGen2 (commented template; uncomment to use) ----------
# diffusion_model:
#   primary:
#     type: omnigen2
#     params:
#       model_path: "OmniGen2/OmniGen2"  # HuggingFace or local path
#       transformer_path: null  # Optional: custom transformer path
#       transformer_lora_path: null  # Optional: LoRA path
#       omnigen2_repo: "./OmniGen2"  # OmniGen2 repo path
#       device_ids: [0, 1, 2, 3, 4, 5]
#       conda_env: "RE-Edit"  # Required: conda env (can reuse main environment)
#       seed: 52323
#       num_inference_steps: 50
#       text_guidance_scale: 5.0
#       image_guidance_scale: 2.0
#       height: 1024
#       width: 1024
#       scheduler: "euler"  # or "dpmsolver++"
#       dtype: "bf16"
#       enable_teacache: false
#       teacache_rel_l1_thresh: 0.05
#       enable_taylorseer: false


# ---------- FLUX.1-Kontext (commented template; uncomment to use) ----------
# diffusion_model:
#   primary:
#     type: flux_kontext
#     params:
#       model_name: "black-forest-labs/FLUX.1-Kontext-dev" # HuggingFace or local path
#       device_ids: [0, 1, 2, 3, 4, 5]
#       enable_batch_sync: true
#       seed: 2131
#       num_inference_steps: 50
#       disable_progress_bar: false


# ---------- DreamOmni2 (commented template; uncomment to use) ----------
# Dual architecture: VLM (Qwen2.5-VL) + Diffusion (FLUX.1-Kontext + LoRA). Flow: VLM prompt → Diffusion image.
# diffusion_model:
#   primary:
#     type: dreamomni2
#     params:
#       vlm_path: "xiabs/DreamOmni2/vlm-model"  # VLM path (grab xiabs/DreamOmni2 from huggingface firs)
#       base_model_path: "black-forest-labs/FLUX.1-Kontext-dev"  # Base diffusion path
#       edit_lora_path: "xiabs/DreamOmni2/edit_lora"  # Edit LoRA path
#       dreamomni2_repo: "./DreamOmni2"  # DreamOmni2 repo path
#       device_ids: [0, 1, 2, 3, 4, 5]
#       conda_env: "RE-Edit"  # Required: conda env (can reuse main environment)
#       seed: 5311
#       num_inference_steps: 30
#       guidance_scale: 3.5
#       dtype: "bfloat16"
#       disable_progress_bar: false


# ---------- FLUX.2-dev (commented template; uncomment to use) ----------
# Image editing model with multi-GPU (multiprocessing).
# diffusion_model:
#   primary:
#     type: flux2_dev
#     params:
#       model_name: "black-forest-labs/FLUX.2-dev"  # HuggingFace or local path
#       device_ids: [0, 1, 2, 3, 4, 5]
#       seed: 2131
#       num_inference_steps: 50
#       guidance_scale: 4.0
#       dtype: "bfloat16"  # or "float16", "float32"
#       use_cpu_offload: true  # CPU offload to save VRAM
#       disable_progress_bar: false
#       enable_batch_sync: true


  # Refinement (secondary edit) model. Fixed with MLLM; rarely changed. Can match primary or use custom fine-tuned model.
  refinement:
    # type: multi_gpu_qwen_edit
    # params:
    #   model_name: "Qwen/Qwen-Image-Edit" # HuggingFace or local path
    #   device_ids: [0, 1,2,3,4,5]
    #   enable_batch_sync: true
    #   seed: 42
    #   num_inference_steps: 50

    type: flux2_dev
    params:
      model_name: "black-forest-labs/FLUX.2-dev"  # HuggingFace or local path
      device_ids: [0, 1, 2, 3, 4, 5]
      seed: 42
      num_inference_steps: 50
      guidance_scale: 4.0
      dtype: "bfloat16"  # or "float16", "float32"
      use_cpu_offload: true  # CPU offload to save VRAM
      disable_progress_bar: false
      enable_batch_sync: true


# ================================
# MLLM Configuration (Multimodal LLM)
# ================================
mllm:
  type: qwen25_vl
  params:
    model_name: "Yixuan-Ding-ZJU/EditRefine"  # HuggingFace or local path
    device: "auto"  # or specific GPU e.g. "cuda:0"
    dtype: "bfloat16"  # or "float16", "auto"
    batch_size: 16
    max_new_tokens: 512
    use_flash_attention: false  # Enable for speed; requires flash-attn
    # The prompt templates for five categories are same, designed separately for easy coding
    prompts:
      physical:
        system: |
          You are a helpful assistant for visual thinking, design, and editing. Given a source image, an editing instruction, and the resulting edited image, do two tasks: 1. Provide step-by-step reasoning for all categories where issues exist: (a) visual realism (geometry, lighting, physics)(e.g., the image in the mirror does not match the actual situation.), (b) contextual consistency (scene logic, attribute coherence), (c) environmental consistency (e.g., sunny sky but wet ground), (d) cultural/traditional consistency (e.g., Japanese wedding with Western dress). Skip categories without issues. The number of reasoning points is not limited — include as many as needed for clarity. 2. Suggest re-editing instructions that are directly based on and summarized from the step-by-step CoT reasoning. Each re-edit instruction should correspond to one or more CoT points. The number and length of re-editing instructions are not limited. Each should describe a clear, executable editing action derived from your reasoning.
          
          OUTPUT FORMAT (STRICT): Use XML-style tags with each tag on its own separate line. Format: <CoT>content</CoT> for reasoning and <Re_edit>content</Re_edit> for instructions. Each tag MUST be on its own line with NO other content on that line.
          
          Examples:
          Example 1:
          <CoT>The lighting on the added person is inconsistent with the sunny background.</CoT>
          <CoT>The shadow direction contradicts the main light source.</CoT>
          <Re_edit>Adjust the lighting on the person to match the sun direction.</Re_edit>
          <Re_edit>Add a consistent shadow extending to the left, matching the scene's sunlight angle.</Re_edit>
          
          Example 2:
          <CoT>The reflected figure of the person on the shiny floor is not vertically aligned with the real figure — it leans slightly to the right instead of mirroring directly below the feet.</CoT>
          <CoT>The reflection starts a few pixels away from the soles, creating a visible gap that breaks mirror symmetry.</CoT>
          <CoT>The reflection appears shorter than the person, suggesting incorrect scaling during the mirroring process.</CoT>
          <Re_edit>Realign the reflection vertically so that it mirrors the person exactly beneath their feet, ensuring perfect symmetry along the floor plane.</Re_edit>
          <Re_edit>Remove the gap between the soles and the start of the reflection by adjusting the pivot point to the exact contact line.</Re_edit>
          <Re_edit>Rescale the reflection to match the full height of the person, maintaining a true 1:1 mirror ratio.</Re_edit>
          
          RULES: - Output ONLY the tag blocks, each on its own line - No JSON, no code fences, no explanations, no extra text - Each <CoT> and <Re_edit> tag must be on a separate line - The number of <CoT> tags is unlimited - The number of <Re_edit> tags is unlimited - No length restrictions on tag content - Use imperative voice in <Re_edit> tags - CRITICAL: Each tag must start on a new line with NO preceding text - CRITICAL: Each tag must end on its line with NO following text - SPECIAL CASE: If your CoT analysis determines that the preliminary edited image has already perfectly completed the editing instruction with no issues in any category, then output <Re_edit>Improve the image quality</Re_edit>
          RULES: - One editing instruction should be output inside one <Re_edit> tag 
          RULES: - Please do not use vague or ambiguous expressions such as "simulate", "analog", "imitation" in the <Re_edit> tag
        user_template: |
          <desired_editing_instruction>{edit_instruction}</desired_editing_instruction>
          Return reasoning and one re-editing instruction as specified.
      
      environmental:
        system: |
          You are a helpful assistant for visual thinking, design, and editing. Given a source image, an editing instruction, and the resulting edited image, do two tasks: 1. Provide step-by-step reasoning for all categories where issues exist: (a) visual realism (geometry, lighting, physics)(e.g., the image in the mirror does not match the actual situation.), (b) contextual consistency (scene logic, attribute coherence), (c) environmental consistency (e.g., sunny sky but wet ground), (d) cultural/traditional consistency (e.g., Japanese wedding with Western dress). Skip categories without issues. The number of reasoning points is not limited — include as many as needed for clarity. 2. Suggest re-editing instructions that are directly based on and summarized from the step-by-step CoT reasoning. Each re-edit instruction should correspond to one or more CoT points. The number and length of re-editing instructions are not limited. Each should describe a clear, executable editing action derived from your reasoning.
          
          OUTPUT FORMAT (STRICT): Use XML-style tags with each tag on its own separate line. Format: <CoT>content</CoT> for reasoning and <Re_edit>content</Re_edit> for instructions. Each tag MUST be on its own line with NO other content on that line.
          
          Examples:
          Example 1:
          <CoT>The lighting on the added person is inconsistent with the sunny background.</CoT>
          <CoT>The shadow direction contradicts the main light source.</CoT>
          <Re_edit>Adjust the lighting on the person to match the sun direction.</Re_edit>
          <Re_edit>Add a consistent shadow extending to the left, matching the scene's sunlight angle.</Re_edit>
          
          Example 2:
          <CoT>The reflected figure of the person on the shiny floor is not vertically aligned with the real figure — it leans slightly to the right instead of mirroring directly below the feet.</CoT>
          <CoT>The reflection starts a few pixels away from the soles, creating a visible gap that breaks mirror symmetry.</CoT>
          <CoT>The reflection appears shorter than the person, suggesting incorrect scaling during the mirroring process.</CoT>
          <Re_edit>Realign the reflection vertically so that it mirrors the person exactly beneath their feet, ensuring perfect symmetry along the floor plane.</Re_edit>
          <Re_edit>Remove the gap between the soles and the start of the reflection by adjusting the pivot point to the exact contact line.</Re_edit>
          <Re_edit>Rescale the reflection to match the full height of the person, maintaining a true 1:1 mirror ratio.</Re_edit>
          
          RULES: - Output ONLY the tag blocks, each on its own line - No JSON, no code fences, no explanations, no extra text - Each <CoT> and <Re_edit> tag must be on a separate line - The number of <CoT> tags is unlimited - The number of <Re_edit> tags is unlimited - No length restrictions on tag content - Use imperative voice in <Re_edit> tags - CRITICAL: Each tag must start on a new line with NO preceding text - CRITICAL: Each tag must end on its line with NO following text - SPECIAL CASE: If your CoT analysis determines that the preliminary edited image has already perfectly completed the editing instruction with no issues in any category, then output <Re_edit>Improve the image quality</Re_edit>
          RULES: - One editing instruction should be output inside one <Re_edit> tag 
          RULES: - Please do not use vague or ambiguous expressions such as "simulate", "analog", "imitation" in the <Re_edit> tag
        user_template: |
          <desired_editing_instruction>{edit_instruction}</desired_editing_instruction>
          Return reasoning and one re-editing instruction as specified.
      
      cultural:
        system: |
          You are a helpful assistant for visual thinking, design, and editing. Given a source image, an editing instruction, and the resulting edited image, do two tasks: 1. Provide step-by-step reasoning for all categories where issues exist: (a) visual realism (geometry, lighting, physics)(e.g., the image in the mirror does not match the actual situation.), (b) contextual consistency (scene logic, attribute coherence), (c) environmental consistency (e.g., sunny sky but wet ground), (d) cultural/traditional consistency (e.g., Japanese wedding with Western dress). Skip categories without issues. The number of reasoning points is not limited — include as many as needed for clarity. 2. Suggest re-editing instructions that are directly based on and summarized from the step-by-step CoT reasoning. Each re-edit instruction should correspond to one or more CoT points. The number and length of re-editing instructions are not limited. Each should describe a clear, executable editing action derived from your reasoning.
          
          OUTPUT FORMAT (STRICT): Use XML-style tags with each tag on its own separate line. Format: <CoT>content</CoT> for reasoning and <Re_edit>content</Re_edit> for instructions. Each tag MUST be on its own line with NO other content on that line.
          
          Examples:
          Example 1:
          <CoT>The lighting on the added person is inconsistent with the sunny background.</CoT>
          <CoT>The shadow direction contradicts the main light source.</CoT>
          <Re_edit>Adjust the lighting on the person to match the sun direction.</Re_edit>
          <Re_edit>Add a consistent shadow extending to the left, matching the scene's sunlight angle.</Re_edit>
          
          Example 2:
          <CoT>The reflected figure of the person on the shiny floor is not vertically aligned with the real figure — it leans slightly to the right instead of mirroring directly below the feet.</CoT>
          <CoT>The reflection starts a few pixels away from the soles, creating a visible gap that breaks mirror symmetry.</CoT>
          <CoT>The reflection appears shorter than the person, suggesting incorrect scaling during the mirroring process.</CoT>
          <Re_edit>Realign the reflection vertically so that it mirrors the person exactly beneath their feet, ensuring perfect symmetry along the floor plane.</Re_edit>
          <Re_edit>Remove the gap between the soles and the start of the reflection by adjusting the pivot point to the exact contact line.</Re_edit>
          <Re_edit>Rescale the reflection to match the full height of the person, maintaining a true 1:1 mirror ratio.</Re_edit>
          
          RULES: - Output ONLY the tag blocks, each on its own line - No JSON, no code fences, no explanations, no extra text - Each <CoT> and <Re_edit> tag must be on a separate line - The number of <CoT> tags is unlimited - The number of <Re_edit> tags is unlimited - No length restrictions on tag content - Use imperative voice in <Re_edit> tags - CRITICAL: Each tag must start on a new line with NO preceding text - CRITICAL: Each tag must end on its line with NO following text - SPECIAL CASE: If your CoT analysis determines that the preliminary edited image has already perfectly completed the editing instruction with no issues in any category, then output <Re_edit>Improve the image quality</Re_edit>
          RULES: - One editing instruction should be output inside one <Re_edit> tag 
          RULES: - Please do not use vague or ambiguous expressions such as "simulate", "analog", "imitation" in the <Re_edit> tag
        user_template: |
          <desired_editing_instruction>{edit_instruction}</desired_editing_instruction>
          Return reasoning and one re-editing instruction as specified.
      
      causal:
        system: |
          You are a helpful assistant for visual thinking, design, and editing. Given a source image, an editing instruction, and the resulting edited image, do two tasks: 1. Provide step-by-step reasoning for all categories where issues exist: (a) visual realism (geometry, lighting, physics)(e.g., the image in the mirror does not match the actual situation.), (b) contextual consistency (scene logic, attribute coherence), (c) environmental consistency (e.g., sunny sky but wet ground), (d) cultural/traditional consistency (e.g., Japanese wedding with Western dress). Skip categories without issues. The number of reasoning points is not limited — include as many as needed for clarity. 2. Suggest re-editing instructions that are directly based on and summarized from the step-by-step CoT reasoning. Each re-edit instruction should correspond to one or more CoT points. The number and length of re-editing instructions are not limited. Each should describe a clear, executable editing action derived from your reasoning.
          
          OUTPUT FORMAT (STRICT): Use XML-style tags with each tag on its own separate line. Format: <CoT>content</CoT> for reasoning and <Re_edit>content</Re_edit> for instructions. Each tag MUST be on its own line with NO other content on that line.
          
          Examples:
          Example 1:
          <CoT>The lighting on the added person is inconsistent with the sunny background.</CoT>
          <CoT>The shadow direction contradicts the main light source.</CoT>
          <Re_edit>Adjust the lighting on the person to match the sun direction.</Re_edit>
          <Re_edit>Add a consistent shadow extending to the left, matching the scene's sunlight angle.</Re_edit>
          
          Example 2:
          <CoT>The reflected figure of the person on the shiny floor is not vertically aligned with the real figure — it leans slightly to the right instead of mirroring directly below the feet.</CoT>
          <CoT>The reflection starts a few pixels away from the soles, creating a visible gap that breaks mirror symmetry.</CoT>
          <CoT>The reflection appears shorter than the person, suggesting incorrect scaling during the mirroring process.</CoT>
          <Re_edit>Realign the reflection vertically so that it mirrors the person exactly beneath their feet, ensuring perfect symmetry along the floor plane.</Re_edit>
          <Re_edit>Remove the gap between the soles and the start of the reflection by adjusting the pivot point to the exact contact line.</Re_edit>
          <Re_edit>Rescale the reflection to match the full height of the person, maintaining a true 1:1 mirror ratio.</Re_edit>
          
          RULES: - Output ONLY the tag blocks, each on its own line - No JSON, no code fences, no explanations, no extra text - Each <CoT> and <Re_edit> tag must be on a separate line - The number of <CoT> tags is unlimited - The number of <Re_edit> tags is unlimited - No length restrictions on tag content - Use imperative voice in <Re_edit> tags - CRITICAL: Each tag must start on a new line with NO preceding text - CRITICAL: Each tag must end on its line with NO following text - SPECIAL CASE: If your CoT analysis determines that the preliminary edited image has already perfectly completed the editing instruction with no issues in any category, then output <Re_edit>Improve the image quality</Re_edit>
          RULES: - One editing instruction should be output inside one <Re_edit> tag 
          RULES: - Please do not use vague or ambiguous expressions such as "simulate", "analog", "imitation" in the <Re_edit> tag
        user_template: |
          <desired_editing_instruction>{edit_instruction}</desired_editing_instruction>
          Return reasoning and one re-editing instruction as specified.
      
      referential:
        system: |
          You are a helpful assistant for visual thinking, design, and editing. Given a source image, an editing instruction, and the resulting edited image, do two tasks: 1. Provide step-by-step reasoning for all categories where issues exist: (a) visual realism (geometry, lighting, physics)(e.g., the image in the mirror does not match the actual situation.), (b) contextual consistency (scene logic, attribute coherence), (c) environmental consistency (e.g., sunny sky but wet ground), (d) cultural/traditional consistency (e.g., Japanese wedding with Western dress). Skip categories without issues. The number of reasoning points is not limited — include as many as needed for clarity. 2. Suggest re-editing instructions that are directly based on and summarized from the step-by-step CoT reasoning. Each re-edit instruction should correspond to one or more CoT points. The number and length of re-editing instructions are not limited. Each should describe a clear, executable editing action derived from your reasoning.
          
          OUTPUT FORMAT (STRICT): Use XML-style tags with each tag on its own separate line. Format: <CoT>content</CoT> for reasoning and <Re_edit>content</Re_edit> for instructions. Each tag MUST be on its own line with NO other content on that line.
          
          Examples:
          Example 1:
          <CoT>The lighting on the added person is inconsistent with the sunny background.</CoT>
          <CoT>The shadow direction contradicts the main light source.</CoT>
          <Re_edit>Adjust the lighting on the person to match the sun direction.</Re_edit>
          <Re_edit>Add a consistent shadow extending to the left, matching the scene's sunlight angle.</Re_edit>
          
          Example 2:
          <CoT>The reflected figure of the person on the shiny floor is not vertically aligned with the real figure — it leans slightly to the right instead of mirroring directly below the feet.</CoT>
          <CoT>The reflection starts a few pixels away from the soles, creating a visible gap that breaks mirror symmetry.</CoT>
          <CoT>The reflection appears shorter than the person, suggesting incorrect scaling during the mirroring process.</CoT>
          <Re_edit>Realign the reflection vertically so that it mirrors the person exactly beneath their feet, ensuring perfect symmetry along the floor plane.</Re_edit>
          <Re_edit>Remove the gap between the soles and the start of the reflection by adjusting the pivot point to the exact contact line.</Re_edit>
          <Re_edit>Rescale the reflection to match the full height of the person, maintaining a true 1:1 mirror ratio.</Re_edit>
          
          RULES: - Output ONLY the tag blocks, each on its own line - No JSON, no code fences, no explanations, no extra text - Each <CoT> and <Re_edit> tag must be on a separate line - The number of <CoT> tags is unlimited - The number of <Re_edit> tags is unlimited - No length restrictions on tag content - Use imperative voice in <Re_edit> tags - CRITICAL: Each tag must start on a new line with NO preceding text - CRITICAL: Each tag must end on its line with NO following text - SPECIAL CASE: If your CoT analysis determines that the preliminary edited image has already perfectly completed the editing instruction with no issues in any category, then output <Re_edit>Improve the image quality</Re_edit>
          RULES: - One editing instruction should be output inside one <Re_edit> tag 
          RULES: - Please do not use vague or ambiguous expressions such as "simulate", "analog", "imitation" in the <Re_edit> tag
        user_template: |
          <desired_editing_instruction>{edit_instruction}</desired_editing_instruction>
          Return reasoning and one re-editing instruction as specified.

# ================================
# Reward Model - vLLM version (faster; uncomment Transformer block to switch)
# ================================
# vLLM benefits: faster inference (3-5x), higher throughput, tensor parallelism.
# Note: Install vllm, qwen_vl_utils>=0.0.14 in conda env. tensor_parallel_size must be a divisor of model attention heads (Qwen3-VL-30B: 32 heads → 1, 2, 4, 8, 16, 32).
reward_model:
  type: qwen3_vl_vllm_subprocess
  params:
    model_name: "Qwen/Qwen3-VL-30B-A3B-Instruct" # HuggingFace or local path
    tensor_parallel_size: 4  # 4 GPUs (32 heads / 4 = 8 heads per GPU)
    batch_size: 6  # vLLM supports larger batch (recommend 4-16)
    max_new_tokens: 128
    temperature: 0
    dtype: "bfloat16"
    conda_env: "RE-Edit"  # Required: conda env (can reuse main environment)
    timeout: 3600  # Timeout (seconds); vLLM first load is slow

# ================================
# Reward Model - Transformer version (commented; uncomment to use)
# ================================
# reward_model:
#   type: qwen3_vl_multi_gpu_subprocess
#   params:
#    model_name: "Qwen/Qwen3-VL-30B-A3B-Instruct" # HuggingFace or local path
#     device_ids: [0,1,2,3,4,5]
#     batch_size: 2
#     max_new_tokens: 128
#     use_batch_inference: true
#     dtype: "bfloat16"
#     conda_env: "RE-Edit"  # Required: conda env (can reuse main environment)
#     # python_path: "/path/to/python"  # Optional: Python path
#     # script_path: "./src/models/reward/qwen3_vl_standalone.py"  # Optional: script path

# ================================
# Evaluation Configuration
# ================================
evaluation:
  output_dir: "./results_for_RE-Edit/image_edit_model_name"
  # If primary_images_dir is set, skip Stage 1 and load primary images from that dir.
  primary_images_dir: null  # Set to null to run Stage 1 (Primary Editing)
  primary_image_suffix: "_primary.png"  # Primary image filename suffix (when primary_images_dir enabled). Format: {pair_id}{primary_image_suffix}
  skip_stage4: false  # Set false to run Stage 4 (Comparative Scoring)
  save_images: true  # Save generated images

  # Metric toggles. If false, that metric is skipped and omitted from final JSON.
  enable_sc_metric: true   # Semantic Consistency （SC)
  enable_instruction_following_metric: true   # Instruction Following (IF)
  enable_primary_scoring: true  # Score primary images (category/SC) and compute improvement_rate
  skip_refinement: false  # If true: Primary Edit only then Scoring; report uses primary_* fields, without EditRefine

# ================================
# Reward Model Prompt Configuration (Specifically designed for five human-logic reasoning categories separately)
# ================================
prompts:
  physical:
    system_prompt: |
      You are an image editing reward model evaluator assessing Physical & Geometric Consistency.
      You will have to give your output in this json way (Keep your reasoning concise and short.):
      {
      "score" : "yes" or "no",
      "reasoning" : "your reasoning for the score"
      }  

      Task Challenge Consideration

      You may receive a "Task Challenge" note, please pay special attention to the Task Challenge and use it as a critical evaluation focus—if the described challenge manifests as an error in the edited image, penalize strictly.

      Goal

      Determine whether the edited image satisfies essential physical and geometric realism.
      Your judgement must be strict:
        •	When uncertain → choose NO.
        •	If a case is borderline between YES and NO → choose NO.
        •	Prefer false negatives (NO on a valid case) over false positives.

      Evaluation Scope

      Evaluate focusing on physical & geometric plausibility, including:
        1.	Lighting & Shadow: direction, intensity, and consistency
        2.	Contact & Support: no floating, no penetration, correct surface interaction
        3.	Scale & Perspective: depth, size, vanishing alignment
        4.	Material & Reflection: mirror/water realism, correct specular behavior
        5.	Motion & Gravity: posture, inertia, and gravitational plausibility
        6.	Visual Aesthetic Harmony: the edited elements must maintain natural visual beauty, color harmony, and overall aesthetic appeal that aligns with human aesthetic preferences

      Special Strict Rule

      If the edited image shows extreme or obviously impossible global color distortion that breaks natural realism (e.g., unnatural hue shifts, impossible light coloration, globally corrupted tones),
      → Immediately output NO.

      Neutrality Rule

      If the edit instruction intentionally requires unrealistic physical behavior, do not penalize for that.
      If the edit is unrelated to physical/geometric consistency, remain neutral, but:
        •	Neutral does not mean automatic YES
        •	Neutral → follow strict policy → default to NO unless fully certain

      Decision Procedure
        1.	Internally evaluate each criterion.
        2.	Pay special attention to the Task Challenge if provided—check carefully for the mentioned error type.
        3.	If all relevant criteria are clearly plausible → output YES.
        4.	If any criterion is questionable, ambiguous, or implausible → output NO.

      You must never output YES unless physical/geometric consistency is confidently satisfied.
      
    user_prompt_template: |

      Context:
        •	Original Description: {original_description}
        •	Edit Instruction: {edit_instruction}

      You have received the original image, the edited image, editing task specification, and the rationale for the editing task.
      Please evaluate the edited image strictly based on Physical & Geometric Consistency.
      Output JSON format:
      {{
      "score" : "yes" or "no",
      "reasoning" : "your reasoning for the score"
      }}
  cultural:
    system_prompt: |
      You are an image editing reward model evaluator assessing Cultural & Social Norm Consistency.
      You will have to give your output in this json way (Keep your reasoning concise and short.):
      {
      "score" : "yes" or "no",
      "reasoning" : "your reasoning for the score"
      }  

      Goal

      Determine whether the edited image remains consistent with cultural conventions, social norms, attire logic, symbolic meaning, and context-dependent appropriateness.

      Your judgment must be extremely strict:
        •	If uncertain → choose NO.
        •	If borderline → choose NO.
        •	Avoid false positives at all costs.

      Task Challenge Consideration

      You may receive a "Task Challenge" note, please pay special attention to the Task Challenge and use it as a critical evaluation focus—if the described challenge manifests as an error in the edited image, penalize strictly.

      Evaluation Scope

      Assess focusing on cultural and social correctness, including:
        1.	Culturally Appropriate Attire & Objects
      Clothing, accessories, gestures, and artifacts must match regional, temporal, and cultural expectations.
        2.	Social Behavior Appropriateness
      Characters' actions must align with contextually appropriate norms
      (formal settings, rituals, ceremonies, public etiquette, etc.).
        3.	Cultural Symbol Accuracy
      Colors, symbols, patterns, and iconography must retain correct cultural meaning
      (e.g., religious symbols, traditional motifs).
        4.	Contextual Identity Alignment
      Background, environment, and character identity must form a coherent cultural context.
        5.	Cultural-Temporal Coherence
      Cultural elements from different eras or regions must not conflict unless the instruction explicitly requires mixing.
        6.	Cultural Aesthetic Value: the edited image should preserve or enhance the cultural aesthetic beauty, traditional visual harmony, and artistic value that aligns with human aesthetic appreciation of cultural elements

      Strict Color Distortion Rule

      If the edited image shows severe global color distortion that biases or confuses cultural meaning (e.g., modifying colors that compromise cultural symbolism),
      → Immediately output NO.

      Neutrality Rule

      If the instruction explicitly asks for culturally unrealistic, stylized, or fantastical elements, do not penalize for that.
      If the edit is unrelated to cultural/social elements, remain neutral, but:

      Neutral ≠ YES
      Neutral → default to NO unless absolutely certain that cultural logic remains intact.

      Decision Procedure
        1.	Internally assess all cultural and social elements.
        2.	Pay special attention to the Task Challenge if provided—check carefully for the mentioned error type.
        3.	If every element is clearly coherent, appropriate, and culturally consistent → output YES.
        4.	If any element is ambiguous, inaccurate, contextually inappropriate, or culturally contradictory → output NO.

      Never output YES unless cultural & social consistency is fully certain.

    user_prompt_template: |

      Context:
        •	Original Description: {original_description}
        •	Edit Instruction: {edit_instruction}
      You have received the original image, the edited image, editing task specification, and the rationale for the editing task.
      Please evaluate the edited image strictly based on Cultural & Social Norm Consistency.
      Output JSON format:
      {{
      "score" : "yes" or "no",
      "reasoning" : "your reasoning for the score"
      }}
  environmental:
    system_prompt: |
      You are an image editing reward model evaluator assessing Environment & Context Consistency.
      You will have to give your output in this json way (Keep your reasoning concise and short.):
      {
      "score" : "yes" or "no",
      "reasoning" : "your reasoning for the score"
      }  

      Goal

      Determine whether the edited image remains consistent with the environmental context, including time, weather, climate, surroundings, atmosphere, and overall scene logic.

      Your evaluation must be extremely strict:
        •	If uncertain → output NO.
        •	If borderline → output NO.
        •	Prefer false negatives over false positives.

      Task Challenge Consideration

      You may receive a "Task Challenge" note, please pay special attention to the Task Challenge and use it as a critical evaluation focus—if the described challenge manifests as an error in the edited image, penalize strictly.

      Evaluation Scope

      Assess focusing on environmental and contextual plausibility, including:
        1.	Weather & Climate:
      Temperature, humidity, seasonal cues, and weather conditions must remain logical.
        2.	Lighting & Time:
      Light color, direction, and intensity must align with the implied time of day.
        3.	Environmental Elements:
      Plants, terrain, water, sky, architectural style, and spatial background must agree with the environment.
        4.	Atmosphere & Context:
      The overall emotional tone and ambience must match the scene (e.g., foggy mood, warm sunset).
        5.	Temporal Continuity:
      Time or seasonal shifts must appear natural and consistent, not contradictory.
        6.	Environmental Aesthetic Appeal: the edited scene should maintain or enhance the natural environmental beauty, atmospheric visual harmony, and scenic aesthetic quality that aligns with human aesthetic preferences for environmental scenes

      Strict Color Distortion Rule

      If the edited image shows severe global color distortion that breaks natural atmospheric realism
      (e.g., unnatural hue shifts, impossible light coloration, globally corrupted tones),
      → Immediately output NO.

      Neutrality Rule

      If the instruction intentionally requires an unrealistic environmental effect, do not penalize for that.
      If the edit has nothing to do with environment/context, remain neutral, but:

      Neutral ≠ YES
      Neutral → default to NO unless absolutely certain that context remains perfectly intact.

      Decision Procedure
        1.	Internally assess each criterion.
        2.	Pay special attention to the Task Challenge if provided—check carefully for the mentioned error type.
        3.	If every relevant environmental and contextual cue is clearly consistent → output YES.
        4.	If any cue is questionable, contradictory, ambiguous, or physically/contextually implausible → output NO.

      Never output YES unless the image is fully consistent with environmental logic.

      
    user_prompt_template: |

      Context:
        •	Original Description: {original_description}
        •	Edit Instruction: {edit_instruction}
      You have received the original image, the edited image, editing task specification, and the rationale for the editing task.
      Please evaluate the edited image strictly based on Environment & Context Consistency.
      Output JSON format:
      {{
      "score" : "yes" or "no",
      "reasoning" : "your reasoning for the score"
      }}
  
  causal:
    system_prompt: |
      You are an image editing reward model evaluator assessing Logical & Causal Consistency.
      You will have to give your output in this json way (Keep your reasoning concise and short.):
      {
      "score" : "yes" or "no",
      "reasoning" : "your reasoning for the score"
      }  

      Goal

      Determine whether the edited image maintains logical reasoning and correct cause-and-effect relationships.

      Your judgement must be extremely strict:
        •	If uncertain → choose NO.
        •	If borderline → choose NO.
        •	A false positive is unacceptable; false negatives are preferred.

      Task Challenge Consideration

      You may receive a "Task Challenge" note, please pay special attention to the Task Challenge and use it as a critical evaluation focus—if the described challenge manifests as an error in the edited image, penalize strictly.

      Evaluation Scope

      Assess focusing on logical and causal realism, including:
        1.	Action–Outcome Logic
      Actions must lead to plausible results
      (e.g., spilled water → visible wetness; cutting fruit → cut marks).
        2.	Event Transition Continuity
      The before–after change must be smooth and coherent.
        3.	Causal Chain Validity
      Conditions must produce logical effects
      (rain → wet ground; fire → smoke; broken glass → fragments).
        4.	Actor–Object Relations
      The agent's action must logically affect the correct target in a plausible manner.
        5.	Temporal Flow
      Cause must precede effect; effects must not appear without causes.
        6.	Visual Logic Aesthetic: the causal changes and logical transitions should maintain visual coherence and aesthetic harmony, ensuring the edited result aligns with human aesthetic preferences for logically consistent visual narratives

      Strict Color Distortion Rule

      If the edited image displays severe global color distortion that disrupts natural causal interpretation or environmental logic,
      → Immediately output NO.

      Neutrality Rule

      If the instruction intentionally requests surreal, magical, or illogical effects, do not penalize for those.
      If the edit is unrelated to causal reasoning, stay neutral, but:

      Neutral ≠ YES
      Neutral → default to NO unless absolutely certain that logical consistency is fully preserved.

      Decision Procedure
        1.	Internally check all causal and logical relations.
        2.	Pay special attention to the Task Challenge if provided—check carefully for the mentioned error type.
        3.	If every relevant causal link is clearly coherent → output YES.
        4.	If any causal link is ambiguous, implausible, inconsistent, or missing → output NO.

      Never output YES unless logical and causal coherence is entirely clear.

    user_prompt_template: |

      Context:
        •	Original Description: {original_description}
        •	Edit Instruction: {edit_instruction}
      You have received the original image, the edited image, editing task specification, and the rationale for the editing task.
      Please evaluate the edited image strictly based on Logical & Causal Consistency.
      Output JSON format:
      {{
      "score" : "yes" or "no",
      "reasoning" : "your reasoning for the score"
      }}
  
  referential:
    system_prompt: |
      You are an image editing reward model evaluator assessing Target Attribution & Referential Reasoning Consistency.
      You will have to give your output in this json way (Keep your reasoning concise and short.):
      {
      "score" : "yes" or "no",
      "reasoning" : "your reasoning for the score"
      }  

      Goal

      Determine whether the edited image correctly identifies the intended target and applies the edit to the correct entity, region, or attribute—while preserving relational and positional logic.

      Your judgment must be extremely strict:
        •	If uncertain → choose NO.
        •	If borderline → choose NO.
        •	False positives are unacceptable; false negatives are preferred.

      Task Challenge Consideration

      You may receive a "Task Challenge" note, please pay special attention to the Task Challenge and use it as a critical evaluation focus—if the described challenge manifests as an error in the edited image, penalize strictly.

      Evaluation Scope

      Assess focusing on referential reasoning and target attribution, including:
        1.	Target Identification
      The correct object, person, or region must be selected and edited.
      Misidentification, entity swap, or editing the wrong target → fail.
        2.	Spatial Reasoning
      Spatial cues such as "left," "right," "behind," "closest," etc. must be correctly resolved.
        3.	Attribute Consistency
      Edited attributes (color, shape, pose, style) must match the specific instruction.
      Wrong attribute assignment → fail.
        4.	Referential Resolution
      Multi-entity references or relational references ("the cup next to the laptop") must be interpreted correctly.
        5.	Edit Scope Control
      The edit must stay within the referenced area; no unintended edits to unrelated regions.
        6.	Edit Aesthetic Integration: the edited target should seamlessly integrate with the overall image composition, maintaining visual balance, color harmony, and aesthetic completeness that aligns with human aesthetic preferences for well-integrated edits

      Strict Color Distortion Rule

      If the edited image shows severe global color distortion that disrupts the ability to judge target identity or attribute correctness,
      → Immediately output NO.

      Neutrality Rule

      If the instruction intentionally uses ambiguous or abstract references, do not penalize for that.
      If the edit is unrelated to referential or attribution reasoning, remain neutral, but:

      Neutral ≠ YES
      Neutral → default to NO unless absolutely certain the target logic remains correct.

      Decision Procedure
        1.	Internally check all referential links and target mappings.
        2.	Pay special attention to the Task Challenge if provided—check carefully for the mentioned error type.
        3.	If every target-related element (identification, relation, attribute, spatial logic) is fully correct → output YES.
        4.	If any element is ambiguous, incorrect, mismatched, or overgeneralized → output NO.

      Never output YES unless target attribution correctness is completely certain.
    user_prompt_template: |

      Context:
        •	Original Description: {original_description}
        •	Edit Instruction: {edit_instruction}
      You have received the original image, the edited image, editing task specification, and the rationale for the editing task.
      Please evaluate the edited image strictly based on Target Attribution & Referential Reasoning Consistency.
      Output JSON format:
      {{
      "score" : "yes" or "no",
      "reasoning" : "your reasoning for the score"
      }}
  
  # ===== PQ Metric Evaluation =====
  # Category-agnostic; follows VIEScore standard.
  pq_metric:
    system_prompt: |
      You are a professional digital artist. You
      will have to evaluate the effectiveness of the
      AI-generated image(s) based on the given
      rules. You will have to give your output in
      this way (Keep your reasoning concise and
      short.):

      {
      "score" : [...],
      "reasoning" : "your reasoning for the score"
      }

      RULES:
      The edited image is an AI-generated image. The
      objective is to evaluate how successfully the
      image has been edited.

      On a scale 0 to 10:

      A score from 0 to 10 will be given based on
      image naturalness.
      ( 0 indicates that the scene in the image does
      not look natural at all or gives an unnatural
      feeling such as a wrong sense of distance,
      wrong shadow, or wrong lighting. 10 indicates that the image looks natural. )

      A second score from 0 to 10 will rate the
      image artifacts.
      ( 0 indicates that the image contains a large
      portion of distortion, watermarks, scratches,
      blurred faces, unusual body parts, or subjects not harmonized. 10 indicates the image has no artifacts. )

      Put the score in a list such that output score
      = [naturalness, artifacts]
    
    user_prompt_template: |
      Edit Instruction: {edit_instruction}
      Now You have received original image, edited image,and edit instruction tripple combination. Give your evaluation output in desired json way 

      {{
      "score" : [...],
      "reasoning" : "your reasoning for the score"
      }}

      Inside json format, put the score in a list such that output score
      = [naturalness, artifacts]
  
  # ===== SC Metric Evaluation =====
  # Category-agnostic; follows VIEScore standard.
  sc_metric:
    system_prompt: |
      You are a professional digital artist. You
      will have to evaluate the effectiveness of the
      AI-generated image(s) based on the given
      rules. You will have to give your output in
      this way (Keep your reasoning concise and
      short.):

      {
      "score" : [...],
      "reasoning" : "your reasoning for the score"
      }

      RULES:

      Two images will be provided: The first being the original image and the second being
      an edited version of the first. The objective is to evaluate how successfully the editing instruction
      has been executed in the second image. Note that sometimes the two images might look identical
      due to the failure of the image edit.

      On scale of 0 to 10:

      A score from 0 to 10 will be given based on the success of the editing. (0 indicates that the scene
      in the edited image does not follow the editing instructions at all. 10 indicates that the scene in the
      edited image follows the editing instruction text perfectly.)

      A second score from 0 to 10 will rate the degree of overediting in the second image. (0 indicates
      that the scene in the edited image is completely different from the original. 10 indicates that the
      edited image can be recognized as a minimally edited yet effective version of the original.)

      Put the score in a list such that output score = [score1, score2], where 'score1' evaluates the editing
      success and 'score2' evaluates the degree of overediting.
    
    user_prompt_template: |
      Edit Instruction: {edit_instruction}

      Now You have received original image, edited image,and edit instruction tripple combination. Give your evaluation output in desired json way 

      {{
      "score" : [...],
      "reasoning" : "your reasoning for the score"
      }}

      Inside json format, put the score in a list such that output score
      = [editing success, degree of overediting]

  # ===== Instruction Following Metric Evaluation =====
  # Category-agnostic; evaluates how well the edit follows the instruction.
  instruction_following:
    system_prompt: |
      **Precision Image Editing - Instruction Following Evaluation Protocol**
      # SYSTEM ROLE
      You are an expert visual evaluator specialized in image transformation analysis. Your
      task is to rigorously assess how well the edited image adheres to the original
      instruction by identifying all visual changes with precision, with a focus on
      accuracy in human-related edits.
      # INPUT DATA
      - 'Original Image': Reference image before editing.
      - 'Edited Image': Result image after editing.
      - 'Editing Instruction': Text description of required changes (provided for context).
      # OUTPUT FORMAT
      You MUST output a JSON object with exactly two keys: "score" (a number between 0 and
      10) and "reason" (a concise string). Do not include any other text or formatting.
      Example:
      {
      "score": 8,
      "reason": "concise factual summary"
      }
      # EXECUTION STEPS (perform internally)
      ## 1. INSTRUCTION ANALYSIS
      - Parse the instruction to extract core edit requirements (targets, actions,
      expected outcomes).
      - Determine whether the task involves modification, addition, removal,
      replacement, or extraction.
      ## 2. IMAGE COMPARISON
      - Describe key visual elements in both images: objects, people, layout, colors,
      lighting, and background.
      - Identify and list all visible differences between the original and edited images.
      - Pay special attention to:
      - **Objects**: position, size, shape, color, texture, or count changes.
      - **People**: facial expressions, limb completeness, count, and posture.
      - **Background**: any added, removed, or replaced components.
      - **Extraction**: verify that the specified target is **cleanly separated and
      isolated** from other elements, with **background or irrelevant regions removed**.
      - **Spatial**: viewpoint transformation, perspective alteration, focal length
      adjustment and location of objects.
      ## 3. INSTRUCTION-RESULT ALIGNMENT
      - Check if each required change appears in the edited image and matches the
      instruction exactly.
      - Identify:
      - **Missing Edits**: instructed changes not reflected in the image.
      - **Extra Edits**: unintended or unrelated modifications.
      - **Incorrect Edits**: wrong objects or attributes edited.
      - For human-related instructions, confirm correct facial expressions, body
      integrity, and person counts.
      - For extraction tasks, if remnants of the original background or unrelated
      content remain, treat this as an incomplete or incorrect extraction. If the extracted
      object shows significant deviation from the original, treat this as an incorrect
      extraction.
      - For tasks involving extraction, removing, or altering specific targets, evaluate
      whether the result visually aligns with the instruction's intent.
      - If irrelevant or residual background elements remain where they should have
      been removed or replaced, consider the edit incomplete.
      - If the target's appearance deviates substantially from what is expected (e.g.,
      distorted, missing key parts, or visually inconsistent with the instruction),
      consider the edit incorrect.
      - Only treat a target's complete removal as a completely incorrect edit when the
      instruction explicitly requires its preservation or extraction.
      ## 4. SCORING CRITERIA
      - Start from base_score = 10.
      - Deduct points for:
      - Assign score = 0 if edits are completely incorrect or unrelated.
      - Missing required edits: -3 per key omission.
      - Extra or unrelated edits: -3 per occurrence.
      - Incorrectly applied edits (wrong area/object): -2 each.
      - Human-related errors:
      - Incorrect expression: -2 to -4
      - Limb anomalies (missing/extra/distorted): -3 to -5
      - Wrong person count: -3 to -5
      - Object Extraction not cleanly performed (e.g., background retained or partial
      extraction): -3 to -5 per occurrence.
      - The edited object has been altered or damaged compared to the original image:
      -3 per occurrence.
      - Score = 10 only if all instruction points are perfectly implemented with no
      unintended edits.
      - Round score to the nearest integer within [0,10].
      ## 5. FINAL OUTPUT
      - Summarize the main adherence and deviations in one short sentence (for "reason").
      - Output JSON only, no additional text.
      # KEY EMPHASIS (prioritize)
      - When the instruction involves people, prioritize analysis of facial expressions,
      limb integrity, and count changes. For other subjects, focus on attributes specified
      in the instruction.
      - Always verify that edits match the instruction precisely, with no unintended
      alterations.
      - For extraction tasks, emphasize complete separation of the target from the
      background while ensuring the extracted object matches the original one.
      - Begin by deconstructing the instruction into key points, then verify each visually.
      # PROHIBITIONS
      - Do not assign a score of 10 unless adherence is perfect across all points.
      - Do not output any text beyond the JSON object.
      - Avoid assumptions; base analysis solely on visual evidence.
      - Do not ignore severe errors like limb anomalies or incorrect counts.
    
    user_prompt_template: |
      # INPUT
      **Editing instruction**: {edit_instruction}

# ================================
# Logging Configuration
# ================================
log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR

