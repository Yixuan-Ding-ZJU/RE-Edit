"""
HiDream-E1.1 å›¾åƒç¼–è¾‘æ¨¡å‹å®ç°ï¼ˆæ”¯æŒå¤šGPUå¹¶è¡Œ - Subprocessæ¨¡å¼ï¼‰

åŸºäº HiDream-ai/HiDream-E1-1
ä½¿ç”¨subprocessæ¶æ„å®ç°å¤šGPUå¹¶è¡Œï¼ˆéœ€è¦é…ç½®condaç¯å¢ƒï¼‰

æ¶æ„ç‰¹ç‚¹ï¼š
- ä¸‰å±‚æ¨¡å‹æ¶æ„ï¼šLLaMA (text encoder) + HiDream-I1 (base) + HiDream-E1 (edit weights)
- è‡ªå®šä¹‰ HiDreamImageEditingPipeline
- æ”¯æŒ refine_strength å’Œ clip_cfg_norm å‚æ•°
- é€šè¿‡ä¸´æ—¶æ–‡ä»¶ä¼ é€’æ•°æ®ï¼Œæ”¯æŒåœ¨ç‰¹å®šcondaç¯å¢ƒä¸­è¿è¡Œ
"""

import subprocess
import tempfile
import json
import torch
from PIL import Image
from typing import List, Dict, Any
from pathlib import Path
from tqdm import tqdm
import base64
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor

from ..base_diffusion import BaseDiffusionModel


def _image_to_base64(image: Image.Image) -> str:
    """å°†PIL Imageè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²"""
    buffer = BytesIO()
    image.save(buffer, format='PNG')
    return base64.b64encode(buffer.getvalue()).decode('utf-8')


def _base64_to_image(b64_str: str) -> Image.Image:
    """å°†base64å­—ç¬¦ä¸²è½¬æ¢ä¸ºPIL Image"""
    image_data = base64.b64decode(b64_str)
    return Image.open(BytesIO(image_data))


class HiDreamE1Model(BaseDiffusionModel):
    """
    HiDream-E1.1 å›¾åƒç¼–è¾‘æ¨¡å‹ï¼ˆå¤šGPUå¹¶è¡Œ - Subprocessæ¨¡å¼ï¼‰
    
    ä½¿ç”¨subprocesså®ç°æ•°æ®å¹¶è¡Œï¼š
    - æ¯ä¸ªGPUå¯åŠ¨ç‹¬ç«‹çš„subprocess
    - æ¯ä¸ªsubprocessåŠ è½½å®Œæ•´çš„æ¨¡å‹å‰¯æœ¬
    - ä»»åŠ¡æŒ‰è½®è¯¢æ–¹å¼åˆ†é…åˆ°å„ä¸ªGPU
    - æ‰€æœ‰GPUå¹¶è¡Œå¤„ç†ä¸åŒçš„å›¾åƒ
    
    ç‰¹ç‚¹ï¼š
    - ä»…æ”¯æŒsubprocessæ¨¡å¼ï¼ˆéœ€è¦é…ç½®conda_envï¼‰
    - æ”¯æŒå¤šGPUå¹¶è¡Œå¤„ç†
    - ä½¿ç”¨è‡ªå®šä¹‰çš„HiDreamImageEditingPipeline
    - æ”¯æŒrefine_strengthå’Œclip_cfg_normå‚æ•°
    """
    
    def _initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹é…ç½®"""
        # è·å–æ¨¡å‹è·¯å¾„é…ç½®
        self.llama_path = self.config.get("llama_path", "meta-llama/Llama-3.1-8B-Instruct")
        self.hidream_i1_path = self.config.get("hidream_i1_path", "HiDream-ai/HiDream-I1-Full")
        self.hidream_e1_path = self.config.get("hidream_e1_path", "HiDream-ai/HiDream-E1-1")
        self.hidream_repo = self.config.get("hidream_repo", "/data2/yixuan/HiDream-E1")
        
        # GPUé…ç½®
        device_ids = self.config.get("device_ids", None)
        
        # æ£€æµ‹condaç¯å¢ƒé…ç½®
        self.conda_env = self.config.get("conda_env", None)
        if self.conda_env is None:
            raise ValueError("HiDream-E1 requires 'conda_env' parameter (subprocess mode only)")
        
        # ç¡®å®šä½¿ç”¨å“ªäº›GPU
        if device_ids is None:
            self.num_gpus = torch.cuda.device_count()
            self.device_ids = list(range(self.num_gpus))
        else:
            self.device_ids = device_ids
            self.num_gpus = len(device_ids)
        
        print(f"[HiDream-E1] æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU")
        print(f"[HiDream-E1] å°†ä½¿ç”¨ {self.num_gpus} ä¸ªGPU: {self.device_ids}")
        
        # åˆå§‹åŒ–subprocessæ¨¡å¼
        self._initialize_subprocess_mode()
    
    def _initialize_subprocess_mode(self):
        """åˆå§‹åŒ–subprocessæ¨¡å¼"""
        print(f"[HiDream-E1] ä½¿ç”¨Subprocessæ¨¡å¼ï¼ˆcondaç¯å¢ƒ: {self.conda_env}ï¼‰\n")
        
        # æŸ¥æ‰¾workerè„šæœ¬è·¯å¾„
        current_dir = Path(__file__).parent.parent
        self.worker_script = current_dir / "hidream_e1_subprocess_worker.py"
        
        if not self.worker_script.exists():
            raise FileNotFoundError(f"Worker script not found: {self.worker_script}")
        
        # éªŒè¯HiDreamä»“åº“è·¯å¾„
        hidream_repo = Path(self.hidream_repo)
        if not hidream_repo.exists():
            raise FileNotFoundError(f"HiDream-E1 repository not found: {self.hidream_repo}")
        
        # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶
        pipeline_file = hidream_repo / "pipeline_hidream_image_editing.py"
        if not pipeline_file.exists():
            raise FileNotFoundError(f"Pipeline file not found: {pipeline_file}")
        
        print("=" * 70)
        print("ğŸš€ HiDream-E1.1 Subprocess Mode Initialized")
        print("=" * 70)
        print(f"  Conda Environment: {self.conda_env}")
        print(f"  Worker Script: {self.worker_script}")
        print(f"  HiDream Repo: {self.hidream_repo}")
        print(f"  LLaMA Path: {self.llama_path}")
        print(f"  HiDream-I1 Path: {self.hidream_i1_path}")
        print(f"  HiDream-E1 Path: {self.hidream_e1_path}")
        print(f"  GPUs: {self.device_ids}")
        print("  âš¡ Models will be loaded on-demand in subprocess")
        print("=" * 70)
        print()
    
    def edit_image(self, original_image: Image.Image, 
                   edit_instruction: str,
                   **kwargs) -> Image.Image:
        """
        ç¼–è¾‘å•å¼ å›¾åƒï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªGPUï¼‰
        
        Args:
            original_image: åŸå§‹PILå›¾åƒ
            edit_instruction: ç¼–è¾‘æŒ‡ä»¤
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç¼–è¾‘åçš„PILå›¾åƒ
        """
        results = self.batch_edit([original_image], [edit_instruction], **kwargs)
        return results[0]
    
    def batch_edit(self, images: List[Image.Image],
                   instructions: List[str],
                   **kwargs) -> List[Image.Image]:
        """
        å¤šGPUå¹¶è¡Œæ‰¹é‡ç¼–è¾‘å›¾åƒ
        
        Args:
            images: åŸå§‹å›¾åƒåˆ—è¡¨
            instructions: ç¼–è¾‘æŒ‡ä»¤åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç¼–è¾‘åçš„å›¾åƒåˆ—è¡¨
        """
        if len(images) != len(instructions):
            raise ValueError("Number of images must match number of instructions")
        
        return self._batch_edit_subprocess(images, instructions, **kwargs)
    
    def _batch_edit_subprocess(self, images: List[Image.Image],
                                instructions: List[str],
                                **kwargs) -> List[Image.Image]:
        """
        Subprocessæ¨¡å¼æ‰¹é‡ç¼–è¾‘
        
        å°†ä»»åŠ¡åˆ†é…åˆ°å¤šä¸ªGPUï¼Œæ¯ä¸ªGPUå¯åŠ¨ä¸€ä¸ªsubprocessæ‰§è¡Œ
        """
        n = len(images)
        num_gpus = self.num_gpus
        base_seed = kwargs.get("seed", self.config.get("seed", 3))
        
        print(f"\n[HiDream-E1-Subprocess] Starting batch edit: {n} images on {num_gpus} GPUs")
        print(f"  ğŸ Conda Environment: {self.conda_env}")
        
        # ç¼–ç æ‰€æœ‰å›¾åƒ
        print(f"[HiDream-E1-Subprocess] Encoding {n} images to base64...")
        image_b64s = [_image_to_base64(img) for img in images]
        
        # æŒ‰GPUåˆ†é…ä»»åŠ¡
        gpu_tasks = [[] for _ in range(num_gpus)]
        for i in range(n):
            gpu_idx = i % num_gpus
            gpu_tasks[gpu_idx].append({
                'task_id': i,
                'image_b64': image_b64s[i],
                'instruction': instructions[i],
                'seed': base_seed + i
            })
        
        # æ˜¾ç¤ºä»»åŠ¡åˆ†é…
        print("=" * 70)
        print("ğŸ“‹ Task Assignment (Subprocess Mode):")
        print("=" * 70)
        for gpu_idx, gpu_id in enumerate(self.device_ids):
            num_tasks = len(gpu_tasks[gpu_idx])
            print(f"  GPU {gpu_id}: {num_tasks} tasks")
        print("=" * 70)
        print()
        
        # ç»“æœåˆ—è¡¨
        results = [None] * n
        
        # å¹¶è¡Œå¯åŠ¨subprocess
        with ThreadPoolExecutor(max_workers=num_gpus) as executor:
            futures = []
            for gpu_idx, gpu_id in enumerate(self.device_ids):
                if gpu_tasks[gpu_idx]:
                    future = executor.submit(
                        self._call_subprocess_single_gpu,
                        gpu_tasks[gpu_idx],
                        gpu_id
                    )
                    futures.append((future, gpu_idx, gpu_id))
            
            # æ”¶é›†ç»“æœ
            for future, gpu_idx, gpu_id in futures:
                try:
                    gpu_results = future.result()
                    for result in gpu_results:
                        task_id = result['task_id']
                        if result['success']:
                            results[task_id] = _base64_to_image(result['image_b64'])
                        else:
                            print(f"âŒ Task {task_id} failed on GPU {gpu_id}: {result['error']}")
                            results[task_id] = images[task_id]  # fallback to original
                except Exception as e:
                    print(f"âŒ Error in GPU {gpu_id} subprocess: {e}")
                    # å¯¹è¯¥GPUçš„æ‰€æœ‰ä»»åŠ¡ä½¿ç”¨åŸå›¾ä½œä¸ºfallback
                    for task in gpu_tasks[gpu_idx]:
                        task_id = task['task_id']
                        if results[task_id] is None:
                            results[task_id] = images[task_id]
        
        print(f"âœ… Batch edit (subprocess) completed: {n} images\n")
        return results
    
    def _call_subprocess_single_gpu(self, tasks: List[Dict], gpu_id: int) -> List[Dict]:
        """
        åœ¨æŒ‡å®šGPUä¸Šè°ƒç”¨subprocessæ‰§è¡Œç¼–è¾‘ä»»åŠ¡
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«task_id, image_b64, instruction, seed
            gpu_id: GPU ID
            
        Returns:
            ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«task_id, success, image_b64, error
        """
        if not tasks:
            return []
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        input_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)
        output_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)
        
        try:
            # å†™å…¥è¾“å…¥æ•°æ®
            input_data = {'tasks': tasks}
            json.dump(input_data, input_file)
            input_file.close()
            output_file.close()
            
            # æ„å»ºå‘½ä»¤
            cmd = [
                'conda', 'run', '-n', self.conda_env, '--no-capture-output',
                'python', str(self.worker_script)
            ]
            
            # æ·»åŠ å‚æ•°
            cmd.extend([
                '--input', input_file.name,
                '--output', output_file.name,
                '--llama-path', self.llama_path,
                '--hidream-i1-path', self.hidream_i1_path,
                '--hidream-e1-path', self.hidream_e1_path,
                '--hidream-repo', self.hidream_repo,
                '--device', f'cuda:{gpu_id}',
                '--dtype', self.config.get('dtype', 'bfloat16'),
                '--num-inference-steps', str(self.config.get('num_inference_steps', 28)),
                '--guidance-scale', str(self.config.get('guidance_scale', 3.0)),
                '--img-guidance-scale', str(self.config.get('img_guidance_scale', 1.5)),
                '--refine-strength', str(self.config.get('refine_strength', 0.3)),
                '--seed', str(self.config.get('seed', 3)),
            ])
            
            # å¯é€‰å‚æ•°
            if self.config.get('negative_prompt'):
                cmd.extend(['--negative-prompt', self.config.get('negative_prompt')])
            
            if self.config.get('clip_cfg_norm', True):
                cmd.append('--clip-cfg-norm')
            
            if self.config.get('disable_progress_bar', True):
                cmd.append('--disable-progress-bar')
            
            print(f"[GPU {gpu_id}] Starting subprocess with {len(tasks)} tasks...")
            
            # æ‰§è¡Œsubprocessï¼ˆå®æ—¶è¾“å‡ºï¼‰
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # å®æ—¶æ‰“å°stderr
            while True:
                stderr_line = process.stderr.readline()
                if stderr_line:
                    print(f"[GPU {gpu_id}] {stderr_line.rstrip()}")
                elif process.poll() is not None:
                    break
            
            # è·å–å‰©ä½™è¾“å‡º
            remaining_stderr = process.stderr.read()
            if remaining_stderr:
                for line in remaining_stderr.split('\n'):
                    if line.strip():
                        print(f"[GPU {gpu_id}] {line}")
            
            # ç­‰å¾…å®Œæˆ
            return_code = process.wait(timeout=7200)  # 2å°æ—¶è¶…æ—¶ï¼ˆæ¨¡å‹è¾ƒå¤§ï¼‰
            
            if return_code != 0:
                raise RuntimeError(f"Subprocess failed with return code {return_code}")
            
            # è¯»å–è¾“å‡º
            with open(output_file.name, 'r') as f:
                output_data = json.load(f)
            
            if output_data.get('status') != 'success':
                raise RuntimeError(f"Worker error: {output_data.get('error', 'Unknown')}")
            
            return output_data['results']
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            Path(input_file.name).unlink(missing_ok=True)
            Path(output_file.name).unlink(missing_ok=True)
    
    def load_to_gpu(self, parallel: bool = True):
        """
        Subprocessæ¨¡å¼ä¸éœ€è¦é¢„åŠ è½½æ¨¡å‹
        """
        print(f"[HiDream-E1] Subprocess mode: models loaded on-demand")
    
    def unload_from_gpu(self):
        """
        Subprocessæ¨¡å¼ï¼šèµ„æºåœ¨subprocessç»“æŸæ—¶è‡ªåŠ¨é‡Šæ”¾
        """
        print(f"[HiDream-E1] Subprocess mode: resources auto-released")














