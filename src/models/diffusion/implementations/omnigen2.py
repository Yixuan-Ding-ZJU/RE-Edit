"""
OmniGen2 diffusion model implementation
OmniGen2 å›¾åƒç¼–è¾‘æ¨¡å‹å®ç°ï¼ˆæ”¯æŒå¤šGPUå¹¶è¡Œ - Subprocessæ¨¡å¼ï¼‰

åŸºäº OmniGen2 å®˜æ–¹ä»“åº“
ä½¿ç”¨Subprocessæ¨¡å¼å®ç°å¤šGPUå¹¶è¡Œ

OmniGen2 ç‰¹ç‚¹ï¼š
- ç»Ÿä¸€çš„å›¾åƒç”Ÿæˆä¸ç¼–è¾‘Pipeline
- æ”¯æŒtext_guidance_scaleå’Œimage_guidance_scale
- ä½¿ç”¨Acceleratorè¿›è¡Œè®¾å¤‡ç®¡ç†
- éœ€è¦ç‰¹å®šcondaç¯å¢ƒè¿è¡Œ
"""

import subprocess
import tempfile
import json
import torch
from PIL import Image
from typing import List, Dict, Any, Optional
from pathlib import Path
from tqdm import tqdm
import base64
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor

from ..base_diffusion import BaseDiffusionModel
from ....utils import setup_logger


def _image_to_base64(image: Image.Image) -> str:
    """å°†PIL Imageè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²"""
    buffer = BytesIO()
    image.save(buffer, format='PNG')
    return base64.b64encode(buffer.getvalue()).decode('utf-8')


def _base64_to_image(b64_str: str) -> Image.Image:
    """å°†base64å­—ç¬¦ä¸²è½¬æ¢ä¸ºPIL Image"""
    image_data = base64.b64decode(b64_str)
    return Image.open(BytesIO(image_data))


class OmniGen2Model(BaseDiffusionModel):
    """
    OmniGen2 å›¾åƒç¼–è¾‘æ¨¡å‹ï¼ˆå¤šGPUå¹¶è¡Œ - Subprocessæ¨¡å¼ï¼‰
    
    ä½¿ç”¨subprocesså®ç°æ•°æ®å¹¶è¡Œï¼š
    - æ¯ä¸ªGPUå¯åŠ¨ç‹¬ç«‹subprocess
    - æ¯ä¸ªsubprocessåŠ è½½OmniGen2Pipeline
    - ä»»åŠ¡æŒ‰è½®è¯¢æ–¹å¼åˆ†é…åˆ°å„ä¸ªGPU
    
    ç‰¹ç‚¹ï¼š
    - ç»Ÿä¸€çš„å›¾åƒç”Ÿæˆä¸ç¼–è¾‘æ¶æ„
    - æ”¯æŒtext/imageåŒå¼•å¯¼
    - ç¯å¢ƒéš”ç¦»ï¼Œæ”¯æŒç‰¹å®šcondaç¯å¢ƒ
    """
    
    def _initialize(self):
        """åˆå§‹åŒ–OmniGen2æ¨¡å‹"""
        # è·å–é…ç½®
        self.model_path = self.config.get("model_path")
        self.transformer_path = self.config.get("transformer_path", None)
        self.transformer_lora_path = self.config.get("transformer_lora_path", None)
        self.omnigen2_repo = self.config.get("omnigen2_repo", "/data2/yixuan/OmniGen2")
        
        # éªŒè¯å¿…è¦é…ç½®
        if not self.model_path:
            raise ValueError("model_path is required for OmniGen2")
        
        device_ids = self.config.get("device_ids", None)
        
        # OmniGen2 åªæ”¯æŒ subprocess æ¨¡å¼
        self.conda_env = self.config.get("conda_env", None)
        if self.conda_env is None:
            raise ValueError("conda_env is required for OmniGen2 (subprocess mode only)")
        
        # ç¡®å®šä½¿ç”¨å“ªäº›GPU
        if device_ids is None:
            self.num_gpus = torch.cuda.device_count()
            self.device_ids = list(range(self.num_gpus))
        else:
            self.device_ids = device_ids
            self.num_gpus = len(device_ids)
        
        print(f"[OmniGen2] æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU")
        print(f"[OmniGen2] å°†ä½¿ç”¨ {self.num_gpus} ä¸ªGPU: {self.device_ids}")
        
        # åˆå§‹åŒ–subprocessæ¨¡å¼
        self._initialize_subprocess_mode()
    
    def _initialize_subprocess_mode(self):
        """åˆå§‹åŒ–subprocessæ¨¡å¼"""
        print(f"[OmniGen2] ä½¿ç”¨Subprocessæ¨¡å¼ï¼ˆcondaç¯å¢ƒ: {self.conda_env}ï¼‰\n")
        
        # æŸ¥æ‰¾workerè„šæœ¬è·¯å¾„
        current_dir = Path(__file__).parent.parent
        self.worker_script = current_dir / "omnigen2_subprocess_worker.py"
        
        if not self.worker_script.exists():
            raise FileNotFoundError(f"Worker script not found: {self.worker_script}")
        
        # éªŒè¯OmniGen2ä»“åº“è·¯å¾„
        omnigen2_path = Path(self.omnigen2_repo)
        if not omnigen2_path.exists():
            raise FileNotFoundError(f"OmniGen2 repo not found: {self.omnigen2_repo}")
        
        print("=" * 70)
        print("ğŸš€ OmniGen2 Subprocess Mode Initialized")
        print("=" * 70)
        print(f"  Conda Environment: {self.conda_env}")
        print(f"  Worker Script: {self.worker_script}")
        print(f"  OmniGen2 Repo: {self.omnigen2_repo}")
        print(f"  Model Path: {self.model_path}")
        if self.transformer_path:
            print(f"  Transformer Path: {self.transformer_path}")
        if self.transformer_lora_path:
            print(f"  Transformer LoRA: {self.transformer_lora_path}")
        print(f"  GPUs: {self.device_ids}")
        print("  âš¡ Models will be loaded on-demand in subprocess")
        print("=" * 70)
        print()
    
    def edit_image(self, original_image: Image.Image, 
                   edit_instruction: str,
                   **kwargs) -> Image.Image:
        """
        ç¼–è¾‘å•å¼ å›¾åƒ
        
        Args:
            original_image: åŸå§‹PILå›¾åƒ
            edit_instruction: ç¼–è¾‘æŒ‡ä»¤
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç¼–è¾‘åçš„PILå›¾åƒ
        """
        results = self.batch_edit([original_image], [edit_instruction], **kwargs)
        return results[0]
    
    def batch_edit(self, images: List[Image.Image],
                   instructions: List[str],
                   **kwargs) -> List[Image.Image]:
        """
        å¤šGPUå¹¶è¡Œæ‰¹é‡ç¼–è¾‘å›¾åƒ
        
        Args:
            images: åŸå§‹å›¾åƒåˆ—è¡¨
            instructions: ç¼–è¾‘æŒ‡ä»¤åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç¼–è¾‘åçš„å›¾åƒåˆ—è¡¨
        """
        if len(images) != len(instructions):
            raise ValueError("Number of images must match number of instructions")
        
        return self._batch_edit_subprocess(images, instructions, **kwargs)
    
    def _batch_edit_subprocess(self, images: List[Image.Image],
                                instructions: List[str],
                                **kwargs) -> List[Image.Image]:
        """
        Subprocessæ¨¡å¼æ‰¹é‡ç¼–è¾‘
        
        å°†ä»»åŠ¡åˆ†é…åˆ°å¤šä¸ªGPUï¼Œæ¯ä¸ªGPUå¯åŠ¨ä¸€ä¸ªsubprocessæ‰§è¡Œ
        """
        n = len(images)
        num_gpus = self.num_gpus
        base_seed = kwargs.get("seed", self.config.get("seed", 0))
        
        print(f"\n[OmniGen2-Subprocess] Starting batch edit: {n} images on {num_gpus} GPUs")
        print(f"  ğŸ Conda Environment: {self.conda_env}")
        
        # ç¼–ç æ‰€æœ‰å›¾åƒ
        print(f"[OmniGen2-Subprocess] Encoding {n} images to base64...")
        image_b64s = [_image_to_base64(img) for img in images]
        
        # æŒ‰GPUåˆ†é…ä»»åŠ¡
        gpu_tasks = [[] for _ in range(num_gpus)]
        for i in range(n):
            gpu_idx = i % num_gpus
            gpu_tasks[gpu_idx].append({
                'task_id': i,
                'image_b64': image_b64s[i],
                'instruction': instructions[i],
                'seed': base_seed + i
            })
        
        # æ˜¾ç¤ºä»»åŠ¡åˆ†é…
        print("=" * 70)
        print("ğŸ“‹ Task Assignment (OmniGen2 Subprocess Mode):")
        print("=" * 70)
        for gpu_idx, gpu_id in enumerate(self.device_ids):
            num_tasks = len(gpu_tasks[gpu_idx])
            print(f"  GPU {gpu_id}: {num_tasks} tasks")
        print("=" * 70)
        print()
        
        # ç»“æœåˆ—è¡¨
        results = [None] * n
        
        # å¹¶è¡Œå¯åŠ¨subprocess
        with ThreadPoolExecutor(max_workers=num_gpus) as executor:
            futures = []
            for gpu_idx, gpu_id in enumerate(self.device_ids):
                if gpu_tasks[gpu_idx]:
                    future = executor.submit(
                        self._call_subprocess_single_gpu,
                        gpu_tasks[gpu_idx],
                        gpu_id
                    )
                    futures.append((future, gpu_idx, gpu_id))
            
            # æ”¶é›†ç»“æœ
            for future, gpu_idx, gpu_id in futures:
                try:
                    gpu_results = future.result()
                    for result in gpu_results:
                        task_id = result['task_id']
                        if result['success']:
                            results[task_id] = _base64_to_image(result['image_b64'])
                        else:
                            print(f"âŒ Task {task_id} failed on GPU {gpu_id}: {result['error']}")
                            results[task_id] = images[task_id]  # fallback to original
                except Exception as e:
                    print(f"âŒ Error in GPU {gpu_id} subprocess: {e}")
                    import traceback
                    traceback.print_exc()
                    # å¯¹è¯¥GPUçš„æ‰€æœ‰ä»»åŠ¡ä½¿ç”¨åŸå›¾ä½œä¸ºfallback
                    for task in gpu_tasks[gpu_idx]:
                        task_id = task['task_id']
                        if results[task_id] is None:
                            results[task_id] = images[task_id]
        
        print(f"âœ… Batch edit (OmniGen2 subprocess) completed: {n} images\n")
        return results
    
    def _call_subprocess_single_gpu(self, tasks: List[Dict], gpu_id: int) -> List[Dict]:
        """
        åœ¨æŒ‡å®šGPUä¸Šè°ƒç”¨subprocessæ‰§è¡Œç¼–è¾‘ä»»åŠ¡
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«task_id, image_b64, instruction, seed
            gpu_id: GPU ID
            
        Returns:
            ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«task_id, success, image_b64, error
        """
        if not tasks:
            return []
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        input_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)
        output_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)
        
        try:
            # å†™å…¥è¾“å…¥æ•°æ®
            input_data = {'tasks': tasks}
            json.dump(input_data, input_file)
            input_file.close()
            output_file.close()
            
            # æ„å»ºå‘½ä»¤
            cmd = [
                'conda', 'run', '-n', self.conda_env, '--no-capture-output',
                'python', str(self.worker_script)
            ]
            
            # æ·»åŠ å‚æ•°
            cmd.extend([
                '--input', input_file.name,
                '--output', output_file.name,
                '--model-path', self.model_path,
                '--omnigen2-repo', self.omnigen2_repo,
                '--device', f'cuda:{gpu_id}',
                '--dtype', self.config.get('dtype', 'bf16'),
                '--num-inference-steps', str(self.config.get('num_inference_steps', 50)),
                '--text-guidance-scale', str(self.config.get('text_guidance_scale', 5.0)),
                '--image-guidance-scale', str(self.config.get('image_guidance_scale', 2.0)),
                '--height', str(self.config.get('height', 1024)),
                '--width', str(self.config.get('width', 1024)),
                '--seed', str(self.config.get('seed', 0)),
                '--scheduler', self.config.get('scheduler', 'euler'),
            ])
            
            # å¯é€‰å‚æ•°
            if self.transformer_path:
                cmd.extend(['--transformer-path', self.transformer_path])
            
            if self.transformer_lora_path:
                cmd.extend(['--transformer-lora-path', self.transformer_lora_path])
            
            if self.config.get('enable_teacache', False):
                cmd.append('--enable-teacache')
                cmd.extend(['--teacache-rel-l1-thresh', 
                           str(self.config.get('teacache_rel_l1_thresh', 0.05))])
            
            if self.config.get('enable_taylorseer', False):
                cmd.append('--enable-taylorseer')
            
            if self.config.get('disable_progress_bar', True):
                cmd.append('--disable-progress-bar')
            
            print(f"[GPU {gpu_id}] Starting OmniGen2 subprocess with {len(tasks)} tasks...")
            
            # æ‰§è¡Œsubprocessï¼ˆå®æ—¶è¾“å‡ºï¼‰
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # å®æ—¶æ‰“å°stderr
            while True:
                stderr_line = process.stderr.readline()
                if stderr_line:
                    print(f"[GPU {gpu_id}] {stderr_line.rstrip()}")
                elif process.poll() is not None:
                    break
            
            # è·å–å‰©ä½™è¾“å‡º
            remaining_stderr = process.stderr.read()
            if remaining_stderr:
                for line in remaining_stderr.split('\n'):
                    if line.strip():
                        print(f"[GPU {gpu_id}] {line}")
            
            # ç­‰å¾…å®Œæˆ
            return_code = process.wait(timeout=7200)  # 2å°æ—¶è¶…æ—¶
            
            if return_code != 0:
                raise RuntimeError(f"Subprocess failed with return code {return_code}")
            
            # è¯»å–è¾“å‡º
            with open(output_file.name, 'r') as f:
                output_data = json.load(f)
            
            if output_data.get('status') != 'success':
                raise RuntimeError(f"Worker error: {output_data.get('error', 'Unknown')}")
            
            return output_data['results']
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            Path(input_file.name).unlink(missing_ok=True)
            Path(output_file.name).unlink(missing_ok=True)
    
    def unload_from_gpu(self):
        """
        Subprocessæ¨¡å¼ï¼šèµ„æºåœ¨subprocessç»“æŸæ—¶è‡ªåŠ¨é‡Šæ”¾
        """
        print(f"[OmniGen2] Subprocess mode: resources auto-released")
    
    def load_to_gpu(self, parallel: bool = True):
        """
        Subprocessæ¨¡å¼ï¼šæ¨¡å‹åœ¨subprocesså¯åŠ¨æ—¶æŒ‰éœ€åŠ è½½
        """
        print(f"[OmniGen2] Subprocess mode: models loaded on-demand")
















