"""
Flux.1-Kontext diffusion model implementation
Flux.1-Kontext å›¾åƒç¼–è¾‘æ¨¡å‹å®ç°ï¼ˆæ”¯æŒå¤šGPUå¹¶è¡Œï¼‰

åŸºäº black-forest-labs/FLUX.1-Kontext-dev
"""

import threading
import torch
from PIL import Image
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed
from tqdm import tqdm

from ..base_diffusion import BaseDiffusionModel
from ....utils import setup_logger


# å…¨å±€é”ï¼Œç”¨äºåºåˆ—åŒ–æ¨¡å‹åŠ è½½è¿‡ç¨‹ï¼ˆé¿å…OOMï¼‰
_model_load_lock = threading.Lock()


class FluxKontextGPUWorker:
    """Flux Kontext GPUå·¥ä½œå™¨ç±»ï¼Œæ¯ä¸ªå®ä¾‹ç»‘å®šåˆ°ä¸€ä¸ªGPU"""
    
    def __init__(self, gpu_id: int, model_name: str, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–GPU Worker
        
        Args:
            gpu_id: GPU ID
            model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
            config: æ¨¡å‹é…ç½®å‚æ•°
        """
        self.gpu_id = gpu_id
        self.device = f"cuda:{gpu_id}"
        self.model_name = model_name
        self.config = config
        self.pipeline = None
        self._model_loaded = False
        
        # æå–é…ç½®å‚æ•°
        self.dtype = config.get("dtype", "bfloat16")
        self.num_inference_steps = config.get("num_inference_steps", 28)  # Fluxé»˜è®¤28æ­¥
        self.guidance_scale = config.get("guidance_scale", 2.5)
        self.seed = config.get("seed", 0)
        self.disable_progress_bar = config.get("disable_progress_bar", True)
        self.height = config.get("height", None)  # å¯é€‰ï¼šè¾“å‡ºé«˜åº¦
        self.width = config.get("width", None)   # å¯é€‰ï¼šè¾“å‡ºå®½åº¦
    
    def _load_model_serial(self):
        """
        ä¸²è¡ŒåŠ è½½æ¨¡å‹ï¼šä½¿ç”¨å…¨å±€é”ç¡®ä¿ä¸€æ¬¡åªæœ‰ä¸€ä¸ªGPUåœ¨åŠ è½½
        
        è¿™æ ·å¯ä»¥é¿å…å¤šä¸ªGPUåŒæ—¶åŠ è½½å¯¼è‡´OOM
        """
        if self._model_loaded:
            return True
        
        # ä½¿ç”¨å…¨å±€é”ï¼Œç¡®ä¿ä¸€æ¬¡åªæœ‰ä¸€ä¸ªGPUåœ¨åŠ è½½æ¨¡å‹
        with _model_load_lock:
            # åŒé‡æ£€æŸ¥
            if self._model_loaded:
                return True
            
            print(f"[GPU {self.gpu_id}] ğŸ”„ Loading Flux.1-Kontext model...")
            try:
                from diffusers import FluxKontextPipeline
                
                # è®¾ç½®å½“å‰è®¾å¤‡
                torch.cuda.set_device(self.gpu_id)
                
                # æ¸…ç©ºGPUç¼“å­˜
                print(f"[GPU {self.gpu_id}] ğŸ§¹ Clearing GPU cache...")
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
                # åŠ è½½æ¨¡å‹ - ä½¿ç”¨low_cpu_mem_usageå‡å°‘å†…å­˜å ç”¨
                print(f"[GPU {self.gpu_id}] ğŸ”¹ Loading model to {self.device}...")
                self.pipeline = FluxKontextPipeline.from_pretrained(
                    self.model_name,
                    torch_dtype=getattr(torch, self.dtype),
                )
                
                # ç§»åŠ¨åˆ°ç›®æ ‡GPU
                self.pipeline.to(self.device)
                
                # ç¦ç”¨è¿›åº¦æ¡
                if self.disable_progress_bar:
                    self.pipeline.set_progress_bar_config(disable=True)
                
                self._model_loaded = True
                print(f"[GPU {self.gpu_id}] âœ… Model loaded successfully")
                return True
                
            except Exception as e:
                print(f"[GPU {self.gpu_id}] âŒ Error loading model: {e}")
                import traceback
                traceback.print_exc()
                return False
    
    def _ensure_model_loaded(self):
        """ç¡®ä¿æ¨¡å‹å·²åŠ è½½"""
        if self._model_loaded:
            return True
        
        print(f"[GPU {self.gpu_id}] âš ï¸  Model not pre-loaded, loading now...")
        return self._load_model_serial()
    
    def edit_image(self, original_image: Image.Image, 
                   edit_instruction: str, 
                   seed: int = None,
                   show_progress: bool = True,
                   **kwargs) -> Image.Image:
        """
        ç¼–è¾‘å•å¼ å›¾åƒ
        
        Args:
            original_image: åŸå§‹PILå›¾åƒ
            edit_instruction: ç¼–è¾‘æŒ‡ä»¤
            seed: éšæœºç§å­ï¼ˆå¯é€‰ï¼‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºå»å™ªè¿›åº¦æ¡
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç¼–è¾‘åçš„PILå›¾åƒ
        """
        # ç¡®ä¿æ¨¡å‹å·²åŠ è½½
        if not self._ensure_model_loaded():
            raise RuntimeError(f"[GPU {self.gpu_id}] Failed to load model")
        
        # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è®¾å¤‡
        torch.cuda.set_device(self.gpu_id)
        
        # ç¡®ä¿å›¾åƒæ˜¯RGBæ ¼å¼
        if original_image.mode != 'RGB':
            original_image = original_image.convert('RGB')
        
        # è·å–å‚æ•°
        num_steps = kwargs.get("num_inference_steps", self.num_inference_steps)
        guidance = kwargs.get("guidance_scale", self.guidance_scale)
        use_seed = seed if seed is not None else self.seed
        height = kwargs.get("height", self.height)
        width = kwargs.get("width", self.width)
        
        # å‡†å¤‡è¾“å…¥
        inputs = {
            "image": original_image,
            "prompt": edit_instruction,
            "generator": torch.Generator(device=self.device).manual_seed(use_seed),
            "guidance_scale": guidance,
            "num_inference_steps": num_steps,
        }
        
        # å¯é€‰ï¼šè®¾ç½®è¾“å‡ºå°ºå¯¸
        if height is not None:
            inputs["height"] = height
        if width is not None:
            inputs["width"] = width
        
        # æ·»åŠ å»å™ªè¿›åº¦æ¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if show_progress:
            from tqdm import tqdm
            pbar = tqdm(total=num_steps, desc=f"[GPU {self.gpu_id}] Denoising", 
                       unit="step", leave=False, position=self.gpu_id)
            
            def callback(pipe, step_index, timestep, callback_kwargs):
                pbar.update(1)
                return callback_kwargs
            
            inputs["callback_on_step_end"] = callback
        
        # æ‰§è¡Œç¼–è¾‘
        try:
            with torch.inference_mode():
                output = self.pipeline(**inputs)
                edited_image = output.images[0]
        finally:
            if show_progress:
                pbar.close()
        
        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        
        return edited_image
    
    def unload_from_gpu(self):
        """å°†æ¨¡å‹ä»GPUå¸è½½åˆ°CPU"""
        if self._model_loaded and self.pipeline is not None:
            print(f"[GPU {self.gpu_id}] ğŸ”„ Unloading model from GPU...")
            self.pipeline.to('cpu')
            torch.cuda.empty_cache()
            print(f"[GPU {self.gpu_id}] âœ… Model unloaded")
    
    def load_to_gpu(self):
        """å°†æ¨¡å‹ä»CPUåŠ è½½åˆ°GPU"""
        if self._model_loaded and self.pipeline is not None:
            print(f"[GPU {self.gpu_id}] ğŸ”„ Loading model to GPU...")
            self.pipeline.to(self.device)
            print(f"[GPU {self.gpu_id}] âœ… Model loaded to GPU")


class FluxKontextModel(BaseDiffusionModel):
    """
    Flux.1-Kontext å›¾åƒç¼–è¾‘æ¨¡å‹ï¼ˆå¤šGPUå¹¶è¡Œï¼‰
    
    ä½¿ç”¨ThreadPoolExecutorå®ç°æ•°æ®å¹¶è¡Œï¼š
    - æ¯ä¸ªGPUåŠ è½½ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹å‰¯æœ¬
    - ä»»åŠ¡æŒ‰è½®è¯¢æ–¹å¼åˆ†é…åˆ°å„ä¸ªGPU
    - æ‰€æœ‰GPUå¹¶è¡Œå¤„ç†ä¸åŒçš„å›¾åƒ
    """
    
    def _initialize(self):
        """åˆå§‹åŒ–å¤šGPUæ¨¡å‹"""
        # è·å–é…ç½®
        self.model_name = self.config.get("model_name", "black-forest-labs/FLUX.1-Kontext-dev")
        device_ids = self.config.get("device_ids", None)
        
        # ç¡®å®šä½¿ç”¨å“ªäº›GPU
        if device_ids is None:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
            self.num_gpus = torch.cuda.device_count()
            self.device_ids = list(range(self.num_gpus))
        else:
            self.device_ids = device_ids
            self.num_gpus = len(device_ids)
        
        print(f"[FluxKontext] æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU")
        print(f"[FluxKontext] å°†ä½¿ç”¨ {self.num_gpus} ä¸ªGPU: {self.device_ids}")
        
        # åˆ›å»ºGPUå·¥ä½œå™¨
        self.workers = []
        for gpu_id in self.device_ids:
            worker = FluxKontextGPUWorker(
                gpu_id=gpu_id,
                model_name=self.model_name,
                config=self.config
            )
            self.workers.append(worker)
        
        print(f"[FluxKontext] åˆ›å»ºäº† {len(self.workers)} ä¸ªGPU workers\n")
        
        # ===== ä¸²è¡ŒåŠ è½½æ‰€æœ‰GPUçš„æ¨¡å‹ =====
        print("=" * 70)
        print("ğŸš€ Sequential Model Loading Phase (Flux.1-Kontext)")
        print("=" * 70)
        print(f"Loading models to {len(self.workers)} GPUs sequentially...")
        print("(All GPUs will be loaded before any processing starts)")
        print()
        
        loaded_workers = []
        for i, worker in enumerate(self.workers):
            gpu_id = worker.gpu_id
            print(f"[{i+1}/{len(self.workers)}] Loading model to GPU {gpu_id}...")
            
            try:
                success = worker._load_model_serial()
                if success:
                    loaded_workers.append(worker)
                    print(f"  âœ… GPU {gpu_id}: Model loaded and ready\n")
                else:
                    print(f"  âŒ GPU {gpu_id}: Failed to load model\n")
            except Exception as e:
                print(f"  âŒ GPU {gpu_id}: Error - {str(e)[:100]}\n")
        
        if not loaded_workers:
            raise RuntimeError("âŒ ERROR: No GPUs available! Failed to load model on any GPU.")
        
        self.workers = loaded_workers
        print(f"âœ… Successfully loaded models on {len(self.workers)} GPUs")
        print(f"  âš¡ All {len(self.workers)} GPUs are now ready to start processing")
        print("=" * 70)
        print()
    
    def edit_image(self, original_image: Image.Image, 
                   edit_instruction: str,
                   **kwargs) -> Image.Image:
        """
        ç¼–è¾‘å•å¼ å›¾åƒï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªGPUï¼‰
        
        Args:
            original_image: åŸå§‹PILå›¾åƒ
            edit_instruction: ç¼–è¾‘æŒ‡ä»¤
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç¼–è¾‘åçš„PILå›¾åƒ
        """
        # å•å¼ å›¾åƒä½¿ç”¨ç¬¬ä¸€ä¸ªGPU
        return self.workers[0].edit_image(original_image, edit_instruction, **kwargs)
    
    def batch_edit(self, images: List[Image.Image],
                   instructions: List[str],
                   **kwargs) -> List[Image.Image]:
        """
        å¤šGPUå¹¶è¡Œæ‰¹é‡ç¼–è¾‘å›¾åƒï¼ˆå¸¦æ‰¹æ¬¡åŒæ­¥ï¼‰
        
        å®ç°æ‰¹æ¬¡åŒæ­¥æœºåˆ¶ï¼š
        - å°†ä»»åŠ¡åˆ†æˆå¤šä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹å¤§å° = GPUæ•°é‡
        - æ¯æ‰¹ä»»åŠ¡æäº¤åï¼Œç­‰å¾…æ‰€æœ‰GPUå®Œæˆ
        - å†æäº¤ä¸‹ä¸€æ‰¹ï¼Œç¡®ä¿GPUä¹‹é—´ä¿æŒåŒæ­¥
        
        Args:
            images: åŸå§‹å›¾åƒåˆ—è¡¨
            instructions: ç¼–è¾‘æŒ‡ä»¤åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
                - enable_batch_sync: æ˜¯å¦å¯ç”¨æ‰¹æ¬¡åŒæ­¥ï¼ˆé»˜è®¤Trueï¼‰
            
        Returns:
            ç¼–è¾‘åçš„å›¾åƒåˆ—è¡¨
        """
        if len(images) != len(instructions):
            raise ValueError("Number of images must match number of instructions")
        
        n = len(images)
        num_gpus = len(self.workers)
        enable_sync = kwargs.pop("enable_batch_sync", True)  # é»˜è®¤å¯ç”¨æ‰¹æ¬¡åŒæ­¥
        
        print(f"\n[FluxKontext] Starting batch edit: {n} images on {num_gpus} GPUs")
        print(f"  ğŸ”„ Batch synchronization: {'ENABLED âœ…' if enable_sync else 'DISABLED âš ï¸'}")
        
        # é¢„å…ˆåˆ†é…ä»»åŠ¡å¹¶æ˜¾ç¤º
        print("=" * 70)
        print("ğŸ“‹ Task Assignment:")
        print("=" * 70)
        from collections import defaultdict
        gpu_assignments = defaultdict(list)
        
        for idx in range(n):
            gpu_id = self.device_ids[idx % num_gpus]
            gpu_assignments[gpu_id].append(idx)
        
        for gpu_id in sorted(gpu_assignments.keys()):
            assigned = gpu_assignments[gpu_id]
            print(f"  GPU {gpu_id}: {len(assigned)} images")
            preview = ", ".join(map(str, assigned[:5]))
            if len(assigned) > 5:
                preview += f", ... +{len(assigned) - 5} more"
            print(f"           â†’ [{preview}]")
        
        print("=" * 70)
        print()
        
        # ç»“æœåˆ—è¡¨ï¼ˆä¿æŒåŸå§‹é¡ºåºï¼‰
        results = [None] * n
        
        # è·å–åŸºç¡€seed
        base_seed = kwargs.get("seed", self.workers[0].seed)
        
        if enable_sync:
            # æ‰¹æ¬¡åŒæ­¥æ¨¡å¼ï¼šæ¯æ‰¹num_gpusä¸ªä»»åŠ¡ï¼Œæ‰¹æ¬¡é—´åŒæ­¥
            results = self._batch_edit_with_sync(
                images, instructions, n, num_gpus, base_seed, **kwargs
            )
        else:
            # åŸå§‹æ¨¡å¼ï¼šä¸€æ¬¡æ€§æäº¤æ‰€æœ‰ä»»åŠ¡ï¼ˆå‘åå…¼å®¹ï¼‰
            results = self._batch_edit_no_sync(
                images, instructions, n, num_gpus, base_seed, **kwargs
            )
        
        print(f"âœ… Batch edit completed: {n} images\n")
        return results
    
    def _batch_edit_with_sync(self, images, instructions, n, num_gpus, base_seed, **kwargs):
        """
        æ‰¹æ¬¡åŒæ­¥æ¨¡å¼ï¼šç¡®ä¿æ¯æ‰¹æ‰€æœ‰GPUå®Œæˆåå†å¼€å§‹ä¸‹ä¸€æ‰¹
        
        è¿™æ ·å¯ä»¥é¿å…GPUä¹‹é—´è¿›åº¦å·®å¼‚ç´¯ç§¯ï¼Œé˜²æ­¢å¡é—´é€šä¿¡æ··ä¹±
        """
        results = [None] * n
        
        # è®¡ç®—æ‰¹æ¬¡æ•°
        num_batches = (n + num_gpus - 1) // num_gpus
        
        print(f"ğŸ”„ Batch synchronization mode:")
        print(f"   - Total batches: {num_batches}")
        print(f"   - Batch size: {num_gpus} (one task per GPU)")
        print(f"   - All GPUs will stay synchronized at batch boundaries\n")
        
        # ä½¿ç”¨ThreadPoolExecutor
        with ThreadPoolExecutor(max_workers=num_gpus) as executor:
            # æ€»è¿›åº¦æ¡
            with tqdm(total=n, desc="[SYNC] Editing images", unit="img") as pbar:
                # é€æ‰¹å¤„ç†
                for batch_idx in range(num_batches):
                    batch_start = batch_idx * num_gpus
                    batch_end = min(batch_start + num_gpus, n)
                    batch_size = batch_end - batch_start
                    
                    # æäº¤å½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡
                    futures = []
                    indices = []
                    
                    for i in range(batch_start, batch_end):
                        worker = self.workers[(i - batch_start) % num_gpus]
                        current_seed = base_seed + i
                        
                        future = executor.submit(
                            worker.edit_image,
                            images[i],
                            instructions[i],
                            current_seed,
                            show_progress=False,  # ç¦ç”¨å•ç‹¬çš„è¿›åº¦æ¡
                            **kwargs
                        )
                        futures.append(future)
                        indices.append(i)
                    
                    # ç­‰å¾…å½“å‰æ‰¹æ¬¡æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆåŒæ­¥ç‚¹ï¼‰
                    for future, idx in zip(futures, indices):
                        try:
                            result = future.result()
                            results[idx] = result
                        except Exception as e:
                            print(f"\nâŒ Error editing image {idx}: {e}")
                            results[idx] = images[idx]  # fallback
                        finally:
                            pbar.update(1)
                    
                    # å½“å‰æ‰¹æ¬¡å®Œæˆï¼Œæ‰€æœ‰GPUå·²åŒæ­¥ï¼Œå¯ä»¥å¼€å§‹ä¸‹ä¸€æ‰¹
                    if batch_idx < num_batches - 1:
                        pbar.set_postfix_str(f"Batch {batch_idx+1}/{num_batches} done, GPUs synced âœ“")
        
        return results
    
    def _batch_edit_no_sync(self, images, instructions, n, num_gpus, base_seed, **kwargs):
        """
        æ— åŒæ­¥æ¨¡å¼ï¼šä¸€æ¬¡æ€§æäº¤æ‰€æœ‰ä»»åŠ¡ï¼ˆåŸå§‹å®ç°ï¼‰
        
        é€‚ç”¨äºGPUæ€§èƒ½ä¸€è‡´æˆ–ä¸å…³å¿ƒåŒæ­¥çš„åœºæ™¯
        """
        results = [None] * n
        
        print(f"âš¡ No-sync mode: All {n} tasks submitted at once\n")
        
        with ThreadPoolExecutor(max_workers=num_gpus) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_index = {}
            
            for idx in range(n):
                worker = self.workers[idx % num_gpus]
                current_seed = base_seed + idx
                
                future = executor.submit(
                    worker.edit_image,
                    images[idx],
                    instructions[idx],
                    current_seed,
                    show_progress=False,
                    **kwargs
                )
                future_to_index[future] = idx
            
            # æ”¶é›†ç»“æœï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
            with tqdm(total=n, desc="[NO-SYNC] Editing images", unit="img") as pbar:
                for future in as_completed(future_to_index):
                    idx = future_to_index[future]
                    try:
                        result = future.result()
                        results[idx] = result
                    except Exception as e:
                        print(f"\nâŒ Error editing image {idx}: {e}")
                        results[idx] = images[idx]
                    finally:
                        pbar.update(1)
        
        return results
    
    def unload_from_gpu(self):
        """
        å°†æ‰€æœ‰GPUä¸Šçš„æ¨¡å‹å¸è½½åˆ°CPU
        
        æ³¨æ„ï¼šå¸è½½æ“ä½œå¯ä»¥å¹¶è¡Œæ‰§è¡Œï¼Œå› ä¸ºåªæ˜¯é‡Šæ”¾æ˜¾å­˜ï¼Œä¸æ¶‰åŠèµ„æºç«äº‰
        """
        print(f"[FluxKontext] Unloading models from {len(self.workers)} GPUs (parallel)...")
        
        # ä½¿ç”¨ThreadPoolExecutorå¹¶è¡Œå¸è½½
        from concurrent.futures import ThreadPoolExecutor, as_completed
        with ThreadPoolExecutor(max_workers=len(self.workers)) as executor:
            futures = [executor.submit(worker.unload_from_gpu) for worker in self.workers]
            # ç­‰å¾…æ‰€æœ‰å¸è½½å®Œæˆ
            for future in as_completed(futures):
                try:
                    future.result()
                except Exception as e:
                    print(f"[FluxKontext] âš ï¸ Error during unload: {e}")
        
        print(f"[FluxKontext] All models unloaded")
    
    def load_to_gpu(self, parallel: bool = True):
        """
        å°†æ‰€æœ‰GPUä¸Šçš„æ¨¡å‹ä»CPUåŠ è½½å›GPU
        
        Args:
            parallel: æ˜¯å¦å¹¶è¡ŒåŠ è½½ã€‚é»˜è®¤Trueï¼ˆæ¨èï¼‰ã€‚
                     - True: å¹¶è¡ŒåŠ è½½ï¼Œé€Ÿåº¦å¿«
                     - False: ä¸²è¡ŒåŠ è½½ï¼Œæ›´ä¿å®ˆä½†æ…¢
        
        æ³¨æ„ï¼šä¸é¦–æ¬¡åŠ è½½ä¸åŒï¼Œè¿™é‡Œæ˜¯å°†å·²åœ¨å†…å­˜ä¸­çš„æ¨¡å‹ç§»å›GPUï¼Œ
             å¯ä»¥å®‰å…¨åœ°å¹¶è¡Œæ‰§è¡Œï¼ˆä¸ä¼šåƒé¦–æ¬¡åŠ è½½é‚£æ ·æœ‰OOMé£é™©ï¼‰
        """
        if parallel:
            print(f"[FluxKontext] Loading models to {len(self.workers)} GPUs (parallel)...")
            from concurrent.futures import ThreadPoolExecutor, as_completed
            with ThreadPoolExecutor(max_workers=len(self.workers)) as executor:
                futures = [executor.submit(worker.load_to_gpu) for worker in self.workers]
                # ç­‰å¾…æ‰€æœ‰åŠ è½½å®Œæˆ
                for future in as_completed(futures):
                    try:
                        future.result()
                    except Exception as e:
                        print(f"[FluxKontext] âš ï¸ Error during load: {e}")
        else:
            print(f"[FluxKontext] Loading models to {len(self.workers)} GPUs (serial)...")
            for worker in self.workers:
                worker.load_to_gpu()
        
        print(f"[FluxKontext] All models loaded to GPU")
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'workers'):
            for worker in self.workers:
                if hasattr(worker, 'pipeline') and worker.pipeline is not None:
                    del worker.pipeline
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

