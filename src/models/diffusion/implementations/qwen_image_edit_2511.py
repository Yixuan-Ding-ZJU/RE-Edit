"""
Qwen-Image-Edit-2511 diffusion model implementation
Qwenå›¾åƒç¼–è¾‘æ¨¡å‹2511ç‰ˆæœ¬å®ç°ï¼ˆæ”¯æŒå¤šGPUå¹¶è¡Œ - å¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰

åŸºäºå®˜æ–¹ Qwen-Image-Edit-2511 æ¨¡å‹ï¼ˆQwenImageEditPlusPipelineï¼‰

å¤šè¿›ç¨‹æ¶æ„ï¼š
- æ¯ä¸ªGPUå¯¹åº”ä¸€ä¸ªç‹¬ç«‹è¿›ç¨‹
- è¿›ç¨‹é—´å®Œå…¨éš”ç¦»ï¼Œé¿å…GILå’Œèµ„æºç«äº‰
- ä½¿ç”¨Queueè¿›è¡Œè¿›ç¨‹é—´é€šä¿¡

================================================================================
âš ï¸  æ–°å¢æ‰©æ•£æ¨¡å‹å®ç°æ—¶çš„é‡è¦æ³¨æ„äº‹é¡¹ (CRITICAL FOR NEW IMPLEMENTATIONS)
================================================================================

Pipelineä¼šåœ¨å¤šè½®è¯„æµ‹ä¸­å¤ç”¨æ¨¡å‹å®ä¾‹ï¼Œå¿…é¡»æ­£ç¡®å®ç°ç”Ÿå‘½å‘¨æœŸç®¡ç†ï¼Œå¦åˆ™ä¼šå¯¼è‡´å¡æ­»ï¼

ã€é—®é¢˜åœºæ™¯ã€‘
åœ¨iterative_pipelineä¸­ï¼Œå¤šä¸ªç±»åˆ«ä¼šä¾æ¬¡å¤„ç†ï¼š
  ç¬¬1è½®(ç‰©ç†): _initialize() â†’ batch_edit() â†’ unload_from_gpu()
  ç¬¬2è½®(ç¯å¢ƒ): load_to_gpu() â†’ batch_edit() â†’ unload_from_gpu()
  ç¬¬3è½®(ç¤¾ä¼š): load_to_gpu() â†’ batch_edit() â†’ unload_from_gpu()
  ...

ã€æ ¸å¿ƒé—®é¢˜ã€‘
  å¯¹è±¡å®ä¾‹å¤ç”¨ â‰  åº•å±‚èµ„æºï¼ˆè¿›ç¨‹/çº¿ç¨‹/æ¨¡å‹ï¼‰å­˜æ´»
  
  é”™è¯¯åšæ³•ï¼š
    def load_to_gpu(self):
        print("Model loaded")  # âŒ åªæ‰“å°æ—¥å¿—ï¼Œè¿›ç¨‹å·²æ­»ä½†æœªé‡å¯
        return
  
  ç»“æœï¼šç¬¬2è½®è°ƒç”¨batch_edit()æ—¶ï¼Œé˜Ÿåˆ—/è¿›ç¨‹å·²å¤±æ•ˆ â†’ æ°¸ä¹…å¡æ­»

ã€æ­£ç¡®å®ç°ã€‘

1. **Multiprocessingæ¨¡å¼**ï¼ˆæœ¬æ–‡ä»¶é‡‡ç”¨çš„æ–¹å¼ï¼‰ï¼š
   
   def load_to_gpu(self):
       # âœ… æ£€æŸ¥è¿›ç¨‹å¥åº·çŠ¶æ€
       if hasattr(self, 'processes') and len(self.processes) > 0:
           alive = [p for _, p in self.processes if p.is_alive()]
           if len(alive) < len(self.processes):  # å‘ç°æ­»è¿›ç¨‹
               self._cleanup_processes()  # æ¸…ç†æ­»è¿›ç¨‹å’Œé˜Ÿåˆ—
               self._initialize()  # é‡æ–°å¯åŠ¨æ‰€æœ‰è¿›ç¨‹
       else:
           self._initialize()  # é¦–æ¬¡åŠ è½½
   
   def unload_from_gpu(self):
       # âœ… å®Œå…¨ç»ˆæ­¢æ‰€æœ‰è¿›ç¨‹ï¼ˆä¼˜é›…é€€å‡º â†’ terminate â†’ killï¼‰
       for task_queue in self.task_queues:
           task_queue.put(None)  # å‘é€é€€å‡ºä¿¡å·
       for gpu_id, p in self.processes:
           p.join(timeout=5)
           if p.is_alive():
               p.terminate()
               p.join(timeout=3)
               if p.is_alive():
                   p.kill()

================================================================================
"""

import multiprocessing as mp
import torch
from PIL import Image
from typing import List, Dict, Any
from tqdm import tqdm
import base64
from io import BytesIO
import sys

from ..base_diffusion import BaseDiffusionModel

# å¿…é¡»è®¾ç½®ï¼Œå¦åˆ™å¤šè¿›ç¨‹ä¼šå‡ºé”™
mp.set_start_method('spawn', force=True)


def _load_model_in_process(gpu_id: int, model_name: str, config: Dict[str, Any]):
    """
    åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­åŠ è½½Qwen-Image-Edit-2511æ¨¡å‹
    
    Args:
        gpu_id: GPU ID
        model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
        config: æ¨¡å‹é…ç½®å‚æ•°
    
    Returns:
        pipelineå¯¹è±¡
    """
    print(f"[GPU {gpu_id}] ğŸ”„ Loading Qwen-Image-Edit-2511 model...")
    try:
        from diffusers import QwenImageEditPlusPipeline
        
        # è®¾ç½®å½“å‰è®¾å¤‡
        torch.cuda.set_device(gpu_id)
        device = f"cuda:{gpu_id}"
        
        # æ¸…ç©ºGPUç¼“å­˜
        print(f"[GPU {gpu_id}] ğŸ§¹ Clearing GPU cache...")
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        # è§£ædtype
        dtype = config.get("dtype", "bfloat16")
        if dtype == "bfloat16":
            torch_dtype = torch.bfloat16
        elif dtype == "float16":
            torch_dtype = torch.float16
        else:
            torch_dtype = torch.float32
        
        # åŠ è½½æ¨¡å‹
        print(f"[GPU {gpu_id}] ğŸ”¹ Loading Qwen-Image-Edit-2511 pipeline...")
        pipeline = QwenImageEditPlusPipeline.from_pretrained(model_name, torch_dtype=torch_dtype)
        
        # è®¾ç½®è®¾å¤‡
        pipeline.to(device)
        
        # ç¦ç”¨è¿›åº¦æ¡ï¼ˆå¯é€‰ï¼‰
        if config.get("disable_progress_bar", True):
            pipeline.set_progress_bar_config(disable=True)
        else:
            pipeline.set_progress_bar_config(disable=None)
        
        print(f"[GPU {gpu_id}] âœ… Model loaded successfully")
        return pipeline
        
    except Exception as e:
        print(f"[GPU {gpu_id}] âŒ Error loading model: {e}")
        import traceback
        traceback.print_exc()
        return None


def _image_to_base64(image: Image.Image) -> str:
    """å°†PIL Imageè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²"""
    buffer = BytesIO()
    image.save(buffer, format='PNG')
    return base64.b64encode(buffer.getvalue()).decode('utf-8')


def _base64_to_image(b64_str: str) -> Image.Image:
    """å°†base64å­—ç¬¦ä¸²è½¬æ¢ä¸ºPIL Image"""
    image_data = base64.b64decode(b64_str)
    return Image.open(BytesIO(image_data))


def _process_worker(gpu_id: int, model_name: str, config: Dict[str, Any], 
                    task_queue: mp.Queue, result_queue: mp.Queue):
    """
    è¿›ç¨‹å·¥ä½œå‡½æ•°ï¼šåœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­å¤„ç†å›¾åƒç¼–è¾‘ä»»åŠ¡
    
    Args:
        gpu_id: GPU ID
        model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
        config: æ¨¡å‹é…ç½®å‚æ•°
        task_queue: ä»»åŠ¡é˜Ÿåˆ—ï¼ˆæ¥æ”¶ä»»åŠ¡ï¼‰
        result_queue: ç»“æœé˜Ÿåˆ—ï¼ˆå‘é€ç»“æœï¼‰
    """
    try:
        # åŠ è½½æ¨¡å‹
        pipeline = _load_model_in_process(gpu_id, model_name, config)
        if pipeline is None:
            print(f"[GPU {gpu_id}] âŒ Failed to load model, exiting...")
            return
        
        # æå–é…ç½®å‚æ•°
        num_inference_steps = config.get("num_inference_steps", 40)
        true_cfg_scale = config.get("true_cfg_scale", 4.0)
        guidance_scale = config.get("guidance_scale", 1.0)
        num_images_per_prompt = config.get("num_images_per_prompt", 1)
        negative_prompt = config.get("negative_prompt", " ")
        seed = config.get("seed", 0)
        
        print(f"[GPU {gpu_id}] âœ… Worker ready, waiting for tasks...")
        
        # å¤„ç†ä»»åŠ¡å¾ªç¯
        while True:
            # ä»ä»»åŠ¡é˜Ÿåˆ—è·å–ä»»åŠ¡
            task = task_queue.get()
            
            # æ£€æŸ¥ç»“æŸä¿¡å·
            if task is None:
                print(f"[GPU {gpu_id}] ğŸ›‘ Received stop signal, exiting...")
                break
            
            task_id, image_b64, instruction, current_seed, kwargs = task
            
            try:
                # è§£ç å›¾åƒ
                image = _base64_to_image(image_b64)
                
                # ç¡®ä¿å›¾åƒæ˜¯RGBæ ¼å¼
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                
                # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è®¾å¤‡
                torch.cuda.set_device(gpu_id)
                
                # å‡†å¤‡å‚æ•°
                num_steps = kwargs.get("num_inference_steps", num_inference_steps)
                cfg_scale = kwargs.get("true_cfg_scale", true_cfg_scale)
                guid_scale = kwargs.get("guidance_scale", guidance_scale)
                num_images = kwargs.get("num_images_per_prompt", num_images_per_prompt)
                neg_prompt = kwargs.get("negative_prompt", negative_prompt)
                use_seed = current_seed if current_seed is not None else seed
                show_progress = kwargs.get("show_progress", True)  # é»˜è®¤æ˜¾ç¤ºè¿›åº¦æ¡
                
                # å‡†å¤‡pipelineè¾“å…¥
                # æ³¨æ„ï¼šQwen-Image-Edit-2511 ä½¿ç”¨ torch.Generatorï¼Œä¸éœ€è¦æŒ‡å®šdevice
                generator = torch.Generator()
                generator.manual_seed(use_seed)
                
                # QwenImageEditPlusPipeline è¦æ±‚ image æ˜¯åˆ—è¡¨æ ¼å¼
                pipeline_inputs = {
                    "image": [image],  # æ³¨æ„ï¼šå¿…é¡»æ˜¯åˆ—è¡¨
                    "prompt": instruction,
                    "generator": generator,
                    "true_cfg_scale": cfg_scale,
                    "negative_prompt": neg_prompt,
                    "num_inference_steps": num_steps,
                    "guidance_scale": guid_scale,
                    "num_images_per_prompt": num_images,
                }
                
                # æ·»åŠ å»å™ªè¿›åº¦æ¡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                pbar = None
                if show_progress:
                    # ä¸ºæ¯ä¸ªGPUè¿›ç¨‹åˆ›å»ºç‹¬ç«‹çš„è¿›åº¦æ¡
                    pbar = tqdm(
                        total=num_steps,
                        desc=f"[GPU {gpu_id}] Task {task_id} Denoising",
                        unit="step",
                        leave=False,
                        file=sys.stdout,
                        ncols=100,
                        dynamic_ncols=False,
                        bar_format='{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]',
                    )
                    
                    def callback(pipe, step_index, timestep, callback_kwargs):
                        if pbar is not None:
                            pbar.update(1)
                        return callback_kwargs
                    
                    # å°è¯•æ·»åŠ å›è°ƒï¼ˆå¦‚æœpipelineæ”¯æŒï¼‰
                    try:
                        pipeline_inputs["callback_on_step_end"] = callback
                    except:
                        # å¦‚æœpipelineä¸æ”¯æŒcallbackï¼Œåˆ™ä½¿ç”¨pipelineè‡ªå¸¦çš„è¿›åº¦æ¡
                        pass
                
                # æ‰§è¡Œæ¨ç†
                try:
                    with torch.inference_mode():
                        output = pipeline(**pipeline_inputs)
                        # å–ç¬¬ä¸€å¼ ç”Ÿæˆçš„å›¾åƒ
                        edited_image = output.images[0]
                finally:
                    # å…³é—­è¿›åº¦æ¡
                    if pbar is not None:
                        pbar.close()
                
                # ç¼–ç ç»“æœå›¾åƒ
                result_b64 = _image_to_base64(edited_image)
                
                # å‘é€ç»“æœ
                result_queue.put((task_id, True, result_b64, None))
                
            except Exception as e:
                print(f"[GPU {gpu_id}] âŒ Error processing task {task_id}: {e}")
                import traceback
                traceback.print_exc()
                # å‘é€é”™è¯¯ç»“æœ
                result_queue.put((task_id, False, None, str(e)))
            
            # æ¸…ç†GPUç¼“å­˜
            torch.cuda.empty_cache()
        
        print(f"[GPU {gpu_id}] ğŸ‘‹ Worker exiting...")
        
    except Exception as e:
        print(f"[GPU {gpu_id}] âŒ Fatal error in worker: {e}")
        import traceback
        traceback.print_exc()


class QwenImageEdit2511Model(BaseDiffusionModel):
    """
    Qwen-Image-Edit-2511 æ‰©æ•£ç¼–è¾‘æ¨¡å‹å®ç°ï¼ˆå¤šGPUå¹¶è¡Œ - å¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰
    
    å®˜æ–¹ä»“åº“: https://huggingface.co/Qwen/Qwen-Image-Edit-2511
    
    ä½¿ç”¨multiprocessingå®ç°æ•°æ®å¹¶è¡Œï¼š
    - æ¯ä¸ªGPUå¯¹åº”ä¸€ä¸ªç‹¬ç«‹è¿›ç¨‹
    - æ¯ä¸ªè¿›ç¨‹åŠ è½½ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹å‰¯æœ¬
    - ä»»åŠ¡æŒ‰è½®è¯¢æ–¹å¼åˆ†é…åˆ°å„ä¸ªGPUè¿›ç¨‹
    - æ‰€æœ‰GPUè¿›ç¨‹å¹¶è¡Œå¤„ç†ä¸åŒçš„å›¾åƒ
    
    ç‰¹ç‚¹ï¼š
    - æ”¯æŒæ‰¹æ¬¡åŒæ­¥ï¼Œç¡®ä¿GPUé—´è¿›åº¦ä¸€è‡´
    - è¿›ç¨‹é—´å®Œå…¨éš”ç¦»ï¼Œé¿å…GILå’Œèµ„æºç«äº‰
    - ä½¿ç”¨ QwenImageEditPlusPipelineï¼ˆ2511ç‰ˆæœ¬ï¼‰
    """
    
    def _initialize(self):
        """åˆå§‹åŒ–å¤šGPUæ¨¡å‹ï¼ˆå¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰"""
        # è·å–é…ç½®
        self.model_name = self.config.get("model_name", "Qwen/Qwen-Image-Edit-2511")
        device_ids = self.config.get("device_ids", None)
        
        # ç¡®å®šä½¿ç”¨å“ªäº›GPU
        if device_ids is None:
            # å¦‚æœæ²¡æœ‰æŒ‡å®šï¼Œä½¿ç”¨æ‰€æœ‰å¯ç”¨GPU
            self.num_gpus = torch.cuda.device_count()
            self.device_ids = list(range(self.num_gpus))
        else:
            self.device_ids = device_ids
            self.num_gpus = len(device_ids)
        
        print(f"[QwenImageEdit2511] æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU")
        print(f"[QwenImageEdit2511] å°†ä½¿ç”¨ {self.num_gpus} ä¸ªGPU: {self.device_ids}")
        print(f"[QwenImageEdit2511] ä½¿ç”¨å¤šè¿›ç¨‹æ¶æ„ï¼ˆæ¯ä¸ªGPUä¸€ä¸ªç‹¬ç«‹è¿›ç¨‹ï¼‰\n")
        
        # åˆ›å»ºè¿›ç¨‹é—´é€šä¿¡é˜Ÿåˆ—
        self.task_queues = [mp.Queue() for _ in range(self.num_gpus)]
        self.result_queue = mp.Queue()
        
        # å¯åŠ¨å·¥ä½œè¿›ç¨‹
        self.processes = []
        print("=" * 70)
        print("ğŸš€ Starting Worker Processes (Qwen-Image-Edit-2511)")
        print("=" * 70)
        print(f"Starting {self.num_gpus} worker processes...")
        print("(Each process will load model independently)")
        print()
        
        for idx, gpu_id in enumerate(self.device_ids):
            print(f"[{idx+1}/{self.num_gpus}] Starting process for GPU {gpu_id}...")
            
            p = mp.Process(
                target=_process_worker,
                args=(
                    gpu_id,
                    self.model_name,
                    self.config,
                    self.task_queues[idx],
                    self.result_queue
                ),
                name=f"GPU-{gpu_id}"
            )
            p.start()
            self.processes.append((gpu_id, p))
            print(f"  âœ… GPU {gpu_id}: Process started (PID: {p.pid})\n")
        
        print(f"âœ… Successfully started {len(self.processes)} worker processes")
        print(f"  âš¡ All processes are loading models independently")
        print("=" * 70)
        print()
    
    def edit_image(self, 
                   original_image: Image.Image,
                   edit_instruction: str,
                   **kwargs) -> Image.Image:
        """
        ç¼–è¾‘å•å¼ å›¾åƒï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªGPUè¿›ç¨‹ï¼‰
        
        Args:
            original_image: åŸå§‹PILå›¾åƒ
            edit_instruction: ç¼–è¾‘æŒ‡ä»¤æ–‡æœ¬
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç¼–è¾‘åçš„PILå›¾åƒ
        """
        # å•å¼ å›¾åƒä½¿ç”¨ç¬¬ä¸€ä¸ªGPUè¿›ç¨‹
        results = self.batch_edit([original_image], [edit_instruction], **kwargs)
        return results[0]
    
    def batch_edit(self,
                   images: List[Image.Image],
                   instructions: List[str],
                   **kwargs) -> List[Image.Image]:
        """
        å¤šGPUå¹¶è¡Œæ‰¹é‡ç¼–è¾‘å›¾åƒï¼ˆå¸¦æ‰¹æ¬¡åŒæ­¥ï¼‰
        
        å®ç°æ‰¹æ¬¡åŒæ­¥æœºåˆ¶ï¼š
        - å°†ä»»åŠ¡åˆ†æˆå¤šä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹å¤§å° = GPUæ•°é‡
        - æ¯æ‰¹ä»»åŠ¡æäº¤åï¼Œç­‰å¾…æ‰€æœ‰GPUå®Œæˆ
        - å†æäº¤ä¸‹ä¸€æ‰¹ï¼Œç¡®ä¿GPUä¹‹é—´ä¿æŒåŒæ­¥
        
        Args:
            images: åŸå§‹å›¾åƒåˆ—è¡¨
            instructions: ç¼–è¾‘æŒ‡ä»¤åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
                - enable_batch_sync: æ˜¯å¦å¯ç”¨æ‰¹æ¬¡åŒæ­¥ï¼ˆé»˜è®¤Trueï¼‰
                - num_inference_steps: æ¨ç†æ­¥æ•°
                - true_cfg_scale: CFG scale
                - guidance_scale: Guidance scaleï¼ˆé»˜è®¤1.0ï¼‰
                - num_images_per_prompt: æ¯ä¸ªpromptç”Ÿæˆçš„å›¾åƒæ•°ï¼ˆé»˜è®¤1ï¼‰
                - negative_prompt: è´Ÿé¢æç¤ºè¯
                - seed: éšæœºç§å­
                - show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡ï¼ˆé»˜è®¤Trueï¼‰
            
        Returns:
            ç¼–è¾‘åçš„å›¾åƒåˆ—è¡¨
        """
        if len(images) != len(instructions):
            raise ValueError("Number of images must match number of instructions")
        
        n = len(images)
        num_gpus = self.num_gpus
        enable_sync = kwargs.pop("enable_batch_sync", True)  # é»˜è®¤å¯ç”¨æ‰¹æ¬¡åŒæ­¥
        
        print(f"\n[QwenImageEdit2511] Starting batch edit: {n} images on {num_gpus} GPUs")
        print(f"  ğŸ”„ Batch synchronization: {'ENABLED âœ…' if enable_sync else 'DISABLED âš ï¸'}")
        
        # é¢„å…ˆåˆ†é…ä»»åŠ¡å¹¶æ˜¾ç¤º
        print("=" * 70)
        print("ğŸ“‹ Task Assignment:")
        print("=" * 70)
        from collections import defaultdict
        gpu_assignments = defaultdict(list)
        
        for idx in range(n):
            gpu_id = self.device_ids[idx % num_gpus]
            gpu_assignments[gpu_id].append(idx)
        
        for gpu_id in sorted(gpu_assignments.keys()):
            assigned = gpu_assignments[gpu_id]
            print(f"  GPU {gpu_id}: {len(assigned)} images")
            preview = ", ".join(map(str, assigned[:5]))
            if len(assigned) > 5:
                preview += f", ... +{len(assigned) - 5} more"
            print(f"           â†’ [{preview}]")
        
        print("=" * 70)
        print()
        
        # ç»“æœåˆ—è¡¨ï¼ˆä¿æŒåŸå§‹é¡ºåºï¼‰
        results = [None] * n
        
        # è·å–åŸºç¡€seed
        base_seed = kwargs.get("seed", self.config.get("seed", 0))
        
        if enable_sync:
            # æ‰¹æ¬¡åŒæ­¥æ¨¡å¼ï¼šæ¯æ‰¹num_gpusä¸ªä»»åŠ¡ï¼Œæ‰¹æ¬¡é—´åŒæ­¥
            results = self._batch_edit_with_sync(
                images, instructions, n, num_gpus, base_seed, **kwargs
            )
        else:
            # åŸå§‹æ¨¡å¼ï¼šä¸€æ¬¡æ€§æäº¤æ‰€æœ‰ä»»åŠ¡ï¼ˆå‘åå…¼å®¹ï¼‰
            results = self._batch_edit_no_sync(
                images, instructions, n, num_gpus, base_seed, **kwargs
            )
        
        print(f"âœ… Batch edit completed: {n} images\n")
        return results
    
    def _batch_edit_with_sync(self, images, instructions, n, num_gpus, base_seed, **kwargs):
        """
        æ‰¹æ¬¡åŒæ­¥æ¨¡å¼ï¼šç¡®ä¿æ¯æ‰¹æ‰€æœ‰GPUå®Œæˆåå†å¼€å§‹ä¸‹ä¸€æ‰¹ï¼ˆå¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰
        """
        results = [None] * n
        
        # è®¡ç®—æ‰¹æ¬¡æ•°
        num_batches = (n + num_gpus - 1) // num_gpus
        
        print(f"ğŸ”„ Batch synchronization mode (multiprocess):")
        print(f"   - Total batches: {num_batches}")
        print(f"   - Batch size: {num_gpus} (one task per GPU process)")
        print(f"   - All GPU processes will stay synchronized at batch boundaries\n")
        
        # æ€»è¿›åº¦æ¡
        with tqdm(total=n, desc="[SYNC] Editing images", unit="img") as pbar:
            # é€æ‰¹å¤„ç†
            for batch_idx in range(num_batches):
                batch_start = batch_idx * num_gpus
                batch_end = min(batch_start + num_gpus, n)
                batch_size = batch_end - batch_start
                
                # å‡†å¤‡å½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡
                task_indices = []
                for i in range(batch_start, batch_end):
                    # ä½¿ç”¨å…¨å±€è½®è¯¢åˆ†é…ï¼ši % num_gpus ç¡®ä¿æ‰€æœ‰ä»»åŠ¡æŒ‰é¡ºåºè½®è¯¢åˆ†é…åˆ°ä¸åŒGPU
                    gpu_idx = i % num_gpus
                    current_seed = base_seed + i
                    
                    # ç¼–ç å›¾åƒ
                    image_b64 = _image_to_base64(images[i])
                    
                    # å‘é€ä»»åŠ¡åˆ°å¯¹åº”çš„GPUè¿›ç¨‹
                    task = (i, image_b64, instructions[i], current_seed, kwargs)
                    self.task_queues[gpu_idx].put(task)
                    task_indices.append(i)
                
                # ç­‰å¾…å½“å‰æ‰¹æ¬¡æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆåŒæ­¥ç‚¹ï¼‰
                # ä½¿ç”¨å­—å…¸è·Ÿè¸ªå½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡ï¼Œç¡®ä¿åªæ”¶é›†å½“å‰æ‰¹æ¬¡çš„ç»“æœ
                batch_results = {}
                expected_task_ids = set(task_indices)
                
                # ç­‰å¾…ç›´åˆ°æ”¶é›†åˆ°å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰ç»“æœ
                # è¿™æ˜¯çœŸæ­£çš„åŒæ­¥ç‚¹ï¼šå¿…é¡»ç­‰å¾…å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰ä»»åŠ¡å®Œæˆæ‰èƒ½ç»§ç»­ä¸‹ä¸€æ‰¹
                while len(batch_results) < len(task_indices):
                    try:
                        task_id, success, result_b64, error = self.result_queue.get()
                        
                        # åªå¤„ç†å±äºå½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡ç»“æœ
                        if task_id in expected_task_ids:
                            if success:
                                batch_results[task_id] = _base64_to_image(result_b64)
                            else:
                                print(f"\nâŒ Error editing image {task_id}: {error}")
                                batch_results[task_id] = images[task_id]  # fallback
                            pbar.update(1)
                        else:
                            # å¦‚æœæ”¶åˆ°ä¸å±äºå½“å‰æ‰¹æ¬¡çš„ç»“æœï¼Œè¯´æ˜æœ‰è¿›ç¨‹æå‰å®Œæˆäº†ä»»åŠ¡
                            # è¿™ç§æƒ…å†µä¸åº”è¯¥åœ¨åŒæ­¥æ¨¡å¼ä¸‹å‘ç”Ÿï¼ˆå› ä¸ºä»»åŠ¡æ˜¯æŒ‰æ‰¹æ¬¡æäº¤çš„ï¼‰
                            # ä½†ä¸ºäº†å¥å£®æ€§ï¼Œæˆ‘ä»¬ä»ç„¶å¤„ç†å®ƒï¼Œä½†ä¼šè®°å½•è­¦å‘Š
                            print(f"\nâš ï¸  [SYNC] Received result for task {task_id} outside current batch {expected_task_ids}")
                            # ç›´æ¥å¤„ç†ï¼Œå› ä¸ºtask_idå¯ä»¥æ­£ç¡®åŒ¹é…åˆ°resultsæ•°ç»„
                            if success:
                                results[task_id] = _base64_to_image(result_b64)
                            else:
                                results[task_id] = images[task_id]
                            pbar.update(1)
                    except Exception as e:
                        print(f"\nâŒ Error receiving result: {e}")
                
                # å°†å½“å‰æ‰¹æ¬¡çš„ç»“æœå†™å…¥resultsæ•°ç»„ï¼ˆç¡®ä¿æ‰€æœ‰ç»“æœéƒ½å·²æ”¶é›†ï¼‰
                for task_id in task_indices:
                    if task_id in batch_results:
                        results[task_id] = batch_results[task_id]
                    else:
                        # å¦‚æœæŸä¸ªä»»åŠ¡æ²¡æœ‰ç»“æœï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œä½¿ç”¨åŸå›¾ä½œä¸ºfallback
                        print(f"\nâš ï¸  [SYNC] Missing result for task {task_id} in batch {batch_idx}")
                        results[task_id] = images[task_id]
                
                # å½“å‰æ‰¹æ¬¡å®Œæˆï¼Œæ‰€æœ‰GPUè¿›ç¨‹å·²åŒæ­¥ï¼Œå¯ä»¥å¼€å§‹ä¸‹ä¸€æ‰¹
                if batch_idx < num_batches - 1:
                    pbar.set_postfix_str(f"Batch {batch_idx+1}/{num_batches} done, GPUs synced âœ“")
        
        return results
    
    def _batch_edit_no_sync(self, images, instructions, n, num_gpus, base_seed, **kwargs):
        """
        æ— åŒæ­¥æ¨¡å¼ï¼šä¸€æ¬¡æ€§æäº¤æ‰€æœ‰ä»»åŠ¡ï¼ˆå¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰
        """
        results = [None] * n
        
        print(f"âš¡ No-sync mode (multiprocess): All {n} tasks submitted at once\n")
        
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        for idx in range(n):
            gpu_idx = idx % num_gpus
            current_seed = base_seed + idx
            
            # ç¼–ç å›¾åƒ
            image_b64 = _image_to_base64(images[idx])
            
            # å‘é€ä»»åŠ¡åˆ°å¯¹åº”çš„GPUè¿›ç¨‹
            task = (idx, image_b64, instructions[idx], current_seed, kwargs)
            self.task_queues[gpu_idx].put(task)
        
        # æ”¶é›†ç»“æœï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
        with tqdm(total=n, desc="[NO-SYNC] Editing images", unit="img") as pbar:
            for _ in range(n):
                try:
                    task_id, success, result_b64, error = self.result_queue.get()
                    
                    if success:
                        results[task_id] = _base64_to_image(result_b64)
                    else:
                        print(f"\nâŒ Error editing image {task_id}: {error}")
                        results[task_id] = images[task_id]
                except Exception as e:
                    print(f"\nâŒ Error receiving result: {e}")
                finally:
                    pbar.update(1)
        
        return results
    
    def _cleanup_processes(self):
        """
        æ¸…ç†æ‰€æœ‰workerè¿›ç¨‹å’Œé˜Ÿåˆ—
        """
        if not hasattr(self, 'processes'):
            return
        
        print(f"[QwenImageEdit2511] ğŸ§¹ Cleaning up {len(self.processes)} worker processes...")
        
        # å‘æ‰€æœ‰å­˜æ´»çš„è¿›ç¨‹å‘é€åœæ­¢ä¿¡å·
        if hasattr(self, 'task_queues'):
            for task_queue in self.task_queues:
                try:
                    task_queue.put(None)
                except:
                    pass  # é˜Ÿåˆ—å¯èƒ½å·²æŸå
        
        # ç»ˆæ­¢æ‰€æœ‰è¿›ç¨‹
        for gpu_id, p in self.processes:
            if p.is_alive():
                try:
                    p.terminate()
                    p.join(timeout=5)
                    print(f"[QwenImageEdit2511]   âœ… GPU {gpu_id} process terminated")
                except Exception as e:
                    print(f"[QwenImageEdit2511]   âš ï¸  Error terminating GPU {gpu_id} process: {e}")
            else:
                print(f"[QwenImageEdit2511]   âœ“ GPU {gpu_id} process already dead")
        
        # æ¸…ç†é˜Ÿåˆ—ï¼ˆé‡è¦ï¼šé˜²æ­¢é˜Ÿåˆ—å †ç§¯å¯¼è‡´å†…å­˜æ³„æ¼ï¼‰
        if hasattr(self, 'task_queues'):
            for task_queue in self.task_queues:
                # æ¸…ç©ºé˜Ÿåˆ—ä¸­çš„æ‰€æœ‰ä»»åŠ¡
                while not task_queue.empty():
                    try:
                        task_queue.get_nowait()
                    except:
                        break
        
        if hasattr(self, 'result_queue'):
            while not self.result_queue.empty():
                try:
                    self.result_queue.get_nowait()
                except:
                    break
        
        print(f"[QwenImageEdit2511] âœ… Cleanup complete")
    
    def unload_from_gpu(self):
        """
        åœæ­¢æ‰€æœ‰å·¥ä½œè¿›ç¨‹ï¼ˆæ¸…ç†èµ„æºï¼‰
        """
        if not hasattr(self, 'processes') or len(self.processes) == 0:
            print(f"[QwenImageEdit2511] No processes to unload")
            return
        
        print(f"[QwenImageEdit2511] Stopping {len(self.processes)} worker processes...")
        
        # å‘æ‰€æœ‰è¿›ç¨‹å‘é€åœæ­¢ä¿¡å·ï¼ˆä¼˜é›…é€€å‡ºï¼‰
        if hasattr(self, 'task_queues'):
            for task_queue in self.task_queues:
                try:
                    task_queue.put(None)
                except:
                    pass  # é˜Ÿåˆ—å¯èƒ½å·²æŸå
        
        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹ä¼˜é›…é€€å‡ºï¼ˆæœ€å¤š5ç§’æ¯ä¸ªè¿›ç¨‹ï¼‰
        import time
        start_time = time.time()
        for gpu_id, p in self.processes:
            if not p.is_alive():
                print(f"[QwenImageEdit2511] âœ“ GPU {gpu_id} process already stopped")
                continue
            
            try:
                remaining_time = max(1, 5 - (time.time() - start_time))
                p.join(timeout=remaining_time)
                
                if p.is_alive():
                    print(f"[QwenImageEdit2511] âš ï¸  GPU {gpu_id} process did not terminate gracefully, forcing...")
                    p.terminate()
                    p.join(timeout=3)
                    
                    # å¦‚æœterminateè¿˜ä¸è¡Œï¼Œä½¿ç”¨kill
                    if p.is_alive():
                        print(f"[QwenImageEdit2511] âš ï¸  GPU {gpu_id} process did not respond to SIGTERM, killing...")
                        p.kill()
                        p.join(timeout=2)
                        
                        if p.is_alive():
                            print(f"[QwenImageEdit2511] âŒ GPU {gpu_id} process is unresponsive (zombie)")
                        else:
                            print(f"[QwenImageEdit2511] âœ… GPU {gpu_id} process killed")
                    else:
                        print(f"[QwenImageEdit2511] âœ… GPU {gpu_id} process terminated")
                else:
                    print(f"[QwenImageEdit2511] âœ… GPU {gpu_id} process stopped gracefully")
                    
            except Exception as e:
                print(f"[QwenImageEdit2511] âš ï¸  Error stopping GPU {gpu_id} process: {e}")
        
        # æ¸…ç†é˜Ÿåˆ—ä¸­çš„æ®‹ç•™æ•°æ®ï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
        if hasattr(self, 'task_queues'):
            for i, task_queue in enumerate(self.task_queues):
                cleared = 0
                while not task_queue.empty():
                    try:
                        task_queue.get_nowait()
                        cleared += 1
                    except:
                        break
                if cleared > 0:
                    print(f"[QwenImageEdit2511] ğŸ§¹ Cleared {cleared} pending tasks from GPU {self.device_ids[i]} queue")
        
        if hasattr(self, 'result_queue'):
            cleared = 0
            while not self.result_queue.empty():
                try:
                    self.result_queue.get_nowait()
                    cleared += 1
                except:
                    break
            if cleared > 0:
                print(f"[QwenImageEdit2511] ğŸ§¹ Cleared {cleared} pending results from result queue")
        
        print(f"[QwenImageEdit2511] âœ… All worker processes stopped")
    
    def load_to_gpu(self, parallel: bool = True):
        """
        å°†æ¨¡å‹åŠ è½½åˆ°GPUï¼ˆé‡æ–°å¯åŠ¨workerè¿›ç¨‹ï¼‰
        
        å¤šè¿›ç¨‹ç‰ˆæœ¬éœ€è¦æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜æ´»ï¼Œå¦‚æœè¿›ç¨‹å·²æ­»åˆ™é‡æ–°å¯åŠ¨
        """
        # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜æ´»
        if hasattr(self, 'processes') and len(self.processes) > 0:
            alive_processes = [p for _, p in self.processes if p.is_alive()]
            dead_processes = [gpu_id for gpu_id, p in self.processes if not p.is_alive()]
            
            if len(alive_processes) == len(self.processes):
                print(f"[QwenImageEdit2511] âœ… All {len(self.processes)} worker processes are already running")
                return
            else:
                print(f"[QwenImageEdit2511] âš ï¸  Detected {len(dead_processes)} dead processes: {dead_processes}")
                print(f"[QwenImageEdit2511] ğŸ”„ Restarting all worker processes...")
                # æ¸…ç†æ‰€æœ‰è¿›ç¨‹å’Œé˜Ÿåˆ—
                self._cleanup_processes()
        
        # é‡æ–°åˆå§‹åŒ–ï¼ˆå¯åŠ¨æ–°è¿›ç¨‹ï¼‰
        print(f"[QwenImageEdit2511] ğŸš€ Initializing worker processes...")
        self._initialize()
    
    def __del__(self):
        """æ¸…ç†èµ„æºï¼ˆææ„å‡½æ•°ï¼‰"""
        if hasattr(self, 'processes') and len(self.processes) > 0:
            # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å·²åœæ­¢ï¼ˆä½¿ç”¨å¼ºåˆ¶æ–¹å¼ï¼‰
            for gpu_id, p in self.processes:
                if p.is_alive():
                    try:
                        # ç›´æ¥terminateï¼Œä¸ç­‰å¾…ä¼˜é›…é€€å‡º
                        p.terminate()
                        p.join(timeout=2)
                        
                        # å¦‚æœè¿˜æ´»ç€ï¼Œå¼ºåˆ¶kill
                        if p.is_alive():
                            p.kill()
                            p.join(timeout=1)
                    except:
                        pass  # ææ„å‡½æ•°ä¸åº”æŠ›å‡ºå¼‚å¸¸






