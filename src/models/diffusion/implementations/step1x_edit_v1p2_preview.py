"""
Step1X-Edit diffusion model implementation
Step1X-Edit å›¾åƒç¼–è¾‘æ¨¡å‹å®ç°ï¼ˆæ”¯æŒå¤šGPUå¹¶è¡Œ - å¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰

åŸºäº stepfun-ai/Step1X-Edit-v1p2
ä½¿ç”¨å¤šè¿›ç¨‹æ¶æ„å®ç°å¤šGPUå¹¶è¡Œ

å¤šè¿›ç¨‹æ¶æ„ï¼š
- æ¯ä¸ªGPUå¯¹åº”ä¸€ä¸ªç‹¬ç«‹è¿›ç¨‹
- è¿›ç¨‹é—´å®Œå…¨éš”ç¦»ï¼Œé¿å…GILå’Œèµ„æºç«äº‰
- ä½¿ç”¨Queueè¿›è¡Œè¿›ç¨‹é—´é€šä¿¡

Subprocessæ¨¡å¼ï¼ˆå½“é…ç½®conda_envæ—¶å¯ç”¨ï¼‰ï¼š
- ä½¿ç”¨subprocessè°ƒç”¨ç‹¬ç«‹è„šæœ¬
- æ”¯æŒåœ¨ç‰¹å®šcondaç¯å¢ƒä¸­è¿è¡Œ
- é€šè¿‡ä¸´æ—¶æ–‡ä»¶ä¼ é€’æ•°æ®
"""

import multiprocessing as mp
import subprocess
import tempfile
import json
import torch
from PIL import Image
from typing import List, Dict, Any
from pathlib import Path
from tqdm import tqdm
import base64
from io import BytesIO
import sys
from concurrent.futures import ThreadPoolExecutor

from ..base_diffusion import BaseDiffusionModel
from ....utils import setup_logger

# å¿…é¡»è®¾ç½®ï¼Œå¦åˆ™å¤šè¿›ç¨‹ä¼šå‡ºé”™ï¼ˆä»…åœ¨ésubprocessæ¨¡å¼ä¸‹éœ€è¦ï¼‰
try:
    mp.set_start_method('spawn', force=True)
except RuntimeError:
    pass  # å·²ç»è®¾ç½®è¿‡


def _load_model_in_process(gpu_id: int, model_name: str, config: Dict[str, Any]):
    """
    åœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­åŠ è½½æ¨¡å‹
    
    Args:
        gpu_id: GPU ID
        model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
        config: æ¨¡å‹é…ç½®å‚æ•°
    
    Returns:
        pipelineå¯¹è±¡
    """
    print(f"[GPU {gpu_id}] ğŸ”„ Loading Step1X-Edit model...")
    try:
        from diffusers import Step1XEditPipelineV1P2
        
        # è®¾ç½®å½“å‰è®¾å¤‡
        torch.cuda.set_device(gpu_id)
        
        # æ¸…ç©ºGPUç¼“å­˜
        print(f"[GPU {gpu_id}] ğŸ§¹ Clearing GPU cache...")
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        # è§£ædtype
        dtype = config.get("dtype", "bfloat16")
        if dtype == "bfloat16":
            torch_dtype = torch.bfloat16
        elif dtype == "float16":
            torch_dtype = torch.float16
        else:
            torch_dtype = torch.float32
        
        # åŠ è½½æ¨¡å‹
        print(f"[GPU {gpu_id}] ğŸ”¹ Loading Step1X-Edit pipeline...")
        pipeline = Step1XEditPipelineV1P2.from_pretrained(
            model_name,
            torch_dtype=torch_dtype,
        )
        
        # ç§»åŠ¨åˆ°ç›®æ ‡GPU
        print(f"[GPU {gpu_id}] ğŸ”¹ Moving model to cuda:{gpu_id}...")
        pipeline.to(f"cuda:{gpu_id}")
        
        # ç¦ç”¨è¿›åº¦æ¡
        if config.get("disable_progress_bar", True):
            pipeline.set_progress_bar_config(disable=True)
        
        print(f"[GPU {gpu_id}] âœ… Model loaded successfully")
        return pipeline
        
    except Exception as e:
        print(f"[GPU {gpu_id}] âŒ Error loading model: {e}")
        import traceback
        traceback.print_exc()
        return None


def _image_to_base64(image: Image.Image) -> str:
    """å°†PIL Imageè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²"""
    buffer = BytesIO()
    image.save(buffer, format='PNG')
    return base64.b64encode(buffer.getvalue()).decode('utf-8')


def _base64_to_image(b64_str: str) -> Image.Image:
    """å°†base64å­—ç¬¦ä¸²è½¬æ¢ä¸ºPIL Image"""
    image_data = base64.b64decode(b64_str)
    return Image.open(BytesIO(image_data))


def _process_worker(gpu_id: int, model_name: str, config: Dict[str, Any], 
                    task_queue: mp.Queue, result_queue: mp.Queue):
    """
    è¿›ç¨‹å·¥ä½œå‡½æ•°ï¼šåœ¨ç‹¬ç«‹è¿›ç¨‹ä¸­å¤„ç†å›¾åƒç¼–è¾‘ä»»åŠ¡
    
    Args:
        gpu_id: GPU ID
        model_name: æ¨¡å‹åç§°æˆ–è·¯å¾„
        config: æ¨¡å‹é…ç½®å‚æ•°
        task_queue: ä»»åŠ¡é˜Ÿåˆ—ï¼ˆæ¥æ”¶ä»»åŠ¡ï¼‰
        result_queue: ç»“æœé˜Ÿåˆ—ï¼ˆå‘é€ç»“æœï¼‰
    """
    try:
        # åŠ è½½æ¨¡å‹
        pipeline = _load_model_in_process(gpu_id, model_name, config)
        if pipeline is None:
            print(f"[GPU {gpu_id}] âŒ Failed to load model, exiting...")
            return
        
        # æå–é…ç½®å‚æ•°
        num_inference_steps = config.get("num_inference_steps", 50)
        true_cfg_scale = config.get("true_cfg_scale", 6.0)
        seed = config.get("seed", 0)
        enable_thinking_mode = config.get("enable_thinking_mode", False)
        enable_reflection_mode = config.get("enable_reflection_mode", False)
        
        print(f"[GPU {gpu_id}] âœ… Worker ready, waiting for tasks...")
        
        # å¤„ç†ä»»åŠ¡å¾ªç¯
        while True:
            # ä»ä»»åŠ¡é˜Ÿåˆ—è·å–ä»»åŠ¡
            task = task_queue.get()
            
            # æ£€æŸ¥ç»“æŸä¿¡å·
            if task is None:
                print(f"[GPU {gpu_id}] ğŸ›‘ Received stop signal, exiting...")
                break
            
            task_id, image_b64, instruction, current_seed, kwargs = task
            
            try:
                # è§£ç å›¾åƒ
                image = _base64_to_image(image_b64)
                
                # ç¡®ä¿å›¾åƒæ˜¯RGBæ ¼å¼
                if image.mode != 'RGB':
                    image = image.convert('RGB')
                
                # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è®¾å¤‡
                torch.cuda.set_device(gpu_id)
                
                # å‡†å¤‡å‚æ•°
                num_steps = kwargs.get("num_inference_steps", num_inference_steps)
                cfg_scale = kwargs.get("true_cfg_scale", true_cfg_scale)
                use_seed = current_seed if current_seed is not None else seed
                show_progress = kwargs.get("show_progress", True)  # é»˜è®¤æ˜¾ç¤ºè¿›åº¦æ¡
                thinking_mode = kwargs.get("enable_thinking_mode", enable_thinking_mode)
                reflection_mode = kwargs.get("enable_reflection_mode", enable_reflection_mode)
                
                # å‡†å¤‡pipelineè¾“å…¥ï¼ˆæ ¹æ®å®˜æ–¹APIï¼‰
                generator = torch.Generator(device=f"cuda:{gpu_id}").manual_seed(use_seed)
                
                # Step1X-Editçš„pipelineè¾“å…¥ï¼ˆæ ¹æ®å®˜æ–¹APIï¼Œä¸ä½¿ç”¨callbackï¼‰
                pipeline_inputs = {
                    "image": image,
                    "prompt": instruction,
                    "num_inference_steps": num_steps,
                    "true_cfg_scale": cfg_scale,
                    "generator": generator,
                    "enable_thinking_mode": thinking_mode,
                    "enable_reflection_mode": reflection_mode,
                }
                
                # æ‰§è¡Œæ¨ç†
                if show_progress:
                    print(f"[GPU {gpu_id}] Task {task_id}: Processing {num_steps} steps...")
                
                with torch.inference_mode():
                    pipe_output = pipeline(**pipeline_inputs)
                    # Step1X-Editè¿”å›çš„æ˜¯pipe_output.final_images[0]
                    edited_image = pipe_output.images[0]
                
                if show_progress:
                    print(f"[GPU {gpu_id}] Task {task_id}: Completed âœ“")
                
                # ç¼–ç ç»“æœå›¾åƒ
                result_b64 = _image_to_base64(edited_image)
                
                # å‘é€ç»“æœ
                result_queue.put((task_id, True, result_b64, None))
                
            except Exception as e:
                print(f"[GPU {gpu_id}] âŒ Error processing task {task_id}: {e}")
                import traceback
                traceback.print_exc()
                # å‘é€é”™è¯¯ç»“æœ
                result_queue.put((task_id, False, None, str(e)))
            
            # æ¸…ç†GPUç¼“å­˜
            torch.cuda.empty_cache()
        
        print(f"[GPU {gpu_id}] ğŸ‘‹ Worker exiting...")
        
    except Exception as e:
        print(f"[GPU {gpu_id}] âŒ Fatal error in worker: {e}")
        import traceback
        traceback.print_exc()
    


class Step1XEditModel(BaseDiffusionModel):
    """
    Step1X-Edit å›¾åƒç¼–è¾‘æ¨¡å‹ï¼ˆå¤šGPUå¹¶è¡Œ - å¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰
    
    ä½¿ç”¨multiprocessingå®ç°æ•°æ®å¹¶è¡Œï¼š
    - æ¯ä¸ªGPUå¯¹åº”ä¸€ä¸ªç‹¬ç«‹è¿›ç¨‹
    - æ¯ä¸ªè¿›ç¨‹åŠ è½½ä¸€ä¸ªå®Œæ•´çš„æ¨¡å‹å‰¯æœ¬
    - ä»»åŠ¡æŒ‰è½®è¯¢æ–¹å¼åˆ†é…åˆ°å„ä¸ªGPUè¿›ç¨‹
    - æ‰€æœ‰GPUè¿›ç¨‹å¹¶è¡Œå¤„ç†ä¸åŒçš„å›¾åƒ
    
    ç‰¹ç‚¹ï¼š
    - æ”¯æŒå¤šGPUå¹¶è¡Œå¤„ç†
    - æ”¯æŒæ‰¹æ¬¡åŒæ­¥ï¼Œç¡®ä¿GPUé—´è¿›åº¦ä¸€è‡´
    - è¿›ç¨‹é—´å®Œå…¨éš”ç¦»ï¼Œé¿å…GILå’Œèµ„æºç«äº‰
    - æ”¯æŒsubprocessæ¨¡å¼ï¼ˆä½¿ç”¨ç‰¹å®šcondaç¯å¢ƒï¼‰
    """
    
    def _initialize(self):
        """åˆå§‹åŒ–å¤šGPUæ¨¡å‹"""
        # è·å–é…ç½®
        self.model_name = self.config.get("model_name", "stepfun-ai/Step1X-Edit-v1p2")
        device_ids = self.config.get("device_ids", None)
        
        # æ£€æµ‹æ˜¯å¦ä½¿ç”¨subprocessæ¨¡å¼ï¼ˆå½“é…ç½®äº†conda_envæ—¶å¯ç”¨ï¼‰
        self.conda_env = self.config.get("conda_env", None)
        self.use_subprocess = self.conda_env is not None
        
        # ç¡®å®šä½¿ç”¨å“ªäº›GPU
        if device_ids is None:
            self.num_gpus = torch.cuda.device_count()
            self.device_ids = list(range(self.num_gpus))
        else:
            self.device_ids = device_ids
            self.num_gpus = len(device_ids)
        
        print(f"[Step1XEdit] æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU")
        print(f"[Step1XEdit] å°†ä½¿ç”¨ {self.num_gpus} ä¸ªGPU: {self.device_ids}")
        
        if self.use_subprocess:
            # Subprocessæ¨¡å¼ï¼šä½¿ç”¨ç‰¹å®šcondaç¯å¢ƒ
            self._initialize_subprocess_mode()
        else:
            # åŸæœ‰çš„multiprocessingæ¨¡å¼
            self._initialize_multiprocessing_mode()
    
    def _initialize_subprocess_mode(self):
        """åˆå§‹åŒ–subprocessæ¨¡å¼ï¼ˆä½¿ç”¨ç‰¹å®šcondaç¯å¢ƒï¼‰"""
        print(f"[Step1XEdit] ä½¿ç”¨Subprocessæ¨¡å¼ï¼ˆcondaç¯å¢ƒ: {self.conda_env}ï¼‰\n")
        
        # æŸ¥æ‰¾workerè„šæœ¬è·¯å¾„
        current_dir = Path(__file__).parent.parent
        self.worker_script = current_dir / "step1x_edit_subprocess_worker.py"
        
        if not self.worker_script.exists():
            raise FileNotFoundError(f"Worker script not found: {self.worker_script}")
        
        # subprocessæ¨¡å¼ä¸éœ€è¦é¢„å¯åŠ¨è¿›ç¨‹ï¼Œæ¯æ¬¡batch_editæ—¶åŠ¨æ€è°ƒç”¨
        self.processes = []  # ä¿æŒå…¼å®¹æ€§
        self.task_queues = []
        self.result_queue = None
        
        print("=" * 70)
        print("ğŸš€ Step1X-Edit Subprocess Mode Initialized")
        print("=" * 70)
        print(f"  Conda Environment: {self.conda_env}")
        print(f"  Worker Script: {self.worker_script}")
        print(f"  GPUs: {self.device_ids}")
        print("  âš¡ Models will be loaded on-demand in subprocess")
        print("=" * 70)
        print()
    
    def _initialize_multiprocessing_mode(self):
        """åˆå§‹åŒ–åŸæœ‰çš„multiprocessingæ¨¡å¼"""
        print(f"[Step1XEdit] ä½¿ç”¨å¤šè¿›ç¨‹æ¶æ„ï¼ˆæ¯ä¸ªGPUä¸€ä¸ªç‹¬ç«‹è¿›ç¨‹ï¼‰\n")
        
        # åˆ›å»ºè¿›ç¨‹é—´é€šä¿¡é˜Ÿåˆ—
        self.task_queues = [mp.Queue() for _ in range(self.num_gpus)]
        self.result_queue = mp.Queue()
        
        # å¯åŠ¨å·¥ä½œè¿›ç¨‹
        self.processes = []
        print("=" * 70)
        print("ğŸš€ Starting Worker Processes (Step1X-Edit)")
        print("=" * 70)
        print(f"Starting {self.num_gpus} worker processes...")
        print("(Each process will load model independently)")
        print()
        
        for idx, gpu_id in enumerate(self.device_ids):
            print(f"[{idx+1}/{self.num_gpus}] Starting process for GPU {gpu_id}...")
            
            p = mp.Process(
                target=_process_worker,
                args=(
                    gpu_id,
                    self.model_name,
                    self.config,
                    self.task_queues[idx],
                    self.result_queue
                ),
                name=f"GPU-{gpu_id}"
            )
            p.start()
            self.processes.append((gpu_id, p))
            print(f"  âœ… GPU {gpu_id}: Process started (PID: {p.pid})\n")
        
        print(f"âœ… Successfully started {len(self.processes)} worker processes")
        print(f"  âš¡ All processes are loading models independently")
        print("=" * 70)
        print()
    
    def edit_image(self, original_image: Image.Image, 
                   edit_instruction: str,
                   **kwargs) -> Image.Image:
        """
        ç¼–è¾‘å•å¼ å›¾åƒï¼ˆä½¿ç”¨ç¬¬ä¸€ä¸ªGPUè¿›ç¨‹ï¼‰
        
        Args:
            original_image: åŸå§‹PILå›¾åƒ
            edit_instruction: ç¼–è¾‘æŒ‡ä»¤
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç¼–è¾‘åçš„PILå›¾åƒ
        """
        # å•å¼ å›¾åƒä½¿ç”¨ç¬¬ä¸€ä¸ªGPUè¿›ç¨‹
        results = self.batch_edit([original_image], [edit_instruction], **kwargs)
        return results[0]
    
    def batch_edit(self, images: List[Image.Image],
                   instructions: List[str],
                   **kwargs) -> List[Image.Image]:
        """
        å¤šGPUå¹¶è¡Œæ‰¹é‡ç¼–è¾‘å›¾åƒï¼ˆå¸¦æ‰¹æ¬¡åŒæ­¥ï¼‰
        
        å®ç°æ‰¹æ¬¡åŒæ­¥æœºåˆ¶ï¼š
        - å°†ä»»åŠ¡åˆ†æˆå¤šä¸ªæ‰¹æ¬¡ï¼Œæ¯æ‰¹å¤§å° = GPUæ•°é‡
        - æ¯æ‰¹ä»»åŠ¡æäº¤åï¼Œç­‰å¾…æ‰€æœ‰GPUå®Œæˆ
        - å†æäº¤ä¸‹ä¸€æ‰¹ï¼Œç¡®ä¿GPUä¹‹é—´ä¿æŒåŒæ­¥
        
        Args:
            images: åŸå§‹å›¾åƒåˆ—è¡¨
            instructions: ç¼–è¾‘æŒ‡ä»¤åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
                - enable_batch_sync: æ˜¯å¦å¯ç”¨æ‰¹æ¬¡åŒæ­¥ï¼ˆé»˜è®¤Trueï¼‰
            
        Returns:
            ç¼–è¾‘åçš„å›¾åƒåˆ—è¡¨
        """
        if len(images) != len(instructions):
            raise ValueError("Number of images must match number of instructions")
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©å®ç°
        if self.use_subprocess:
            return self._batch_edit_subprocess(images, instructions, **kwargs)
        else:
            return self._batch_edit_multiprocessing(images, instructions, **kwargs)
    
    def _batch_edit_multiprocessing(self, images: List[Image.Image],
                                     instructions: List[str],
                                     **kwargs) -> List[Image.Image]:
        """åŸæœ‰çš„multiprocessingæ¨¡å¼æ‰¹é‡ç¼–è¾‘"""
        n = len(images)
        num_gpus = self.num_gpus
        enable_sync = kwargs.pop("enable_batch_sync", True)
        
        print(f"\n[Step1XEdit] Starting batch edit: {n} images on {num_gpus} GPUs")
        print(f"  ğŸ”„ Batch synchronization: {'ENABLED âœ…' if enable_sync else 'DISABLED âš ï¸'}")
        
        # é¢„å…ˆåˆ†é…ä»»åŠ¡å¹¶æ˜¾ç¤º
        print("=" * 70)
        print("ğŸ“‹ Task Assignment:")
        print("=" * 70)
        from collections import defaultdict
        gpu_assignments = defaultdict(list)
        
        for idx in range(n):
            gpu_id = self.device_ids[idx % num_gpus]
            gpu_assignments[gpu_id].append(idx)
        
        for gpu_id in sorted(gpu_assignments.keys()):
            assigned = gpu_assignments[gpu_id]
            print(f"  GPU {gpu_id}: {len(assigned)} images")
            preview = ", ".join(map(str, assigned[:5]))
            if len(assigned) > 5:
                preview += f", ... +{len(assigned) - 5} more"
            print(f"           â†’ [{preview}]")
        
        print("=" * 70)
        print()
        
        # è·å–åŸºç¡€seed
        base_seed = kwargs.get("seed", self.config.get("seed", 0))
        
        if enable_sync:
            results = self._batch_edit_with_sync(
                images, instructions, n, num_gpus, base_seed, **kwargs
            )
        else:
            results = self._batch_edit_no_sync(
                images, instructions, n, num_gpus, base_seed, **kwargs
            )
        
        print(f"âœ… Batch edit completed: {n} images\n")
        return results
    
    def _batch_edit_subprocess(self, images: List[Image.Image],
                                instructions: List[str],
                                **kwargs) -> List[Image.Image]:
        """
        Subprocessæ¨¡å¼æ‰¹é‡ç¼–è¾‘ï¼ˆä½¿ç”¨ç‰¹å®šcondaç¯å¢ƒï¼‰
        
        å°†ä»»åŠ¡åˆ†é…åˆ°å¤šä¸ªGPUï¼Œæ¯ä¸ªGPUå¯åŠ¨ä¸€ä¸ªsubprocessæ‰§è¡Œ
        """
        n = len(images)
        num_gpus = self.num_gpus
        base_seed = kwargs.get("seed", self.config.get("seed", 0))
        
        print(f"\n[Step1XEdit-Subprocess] Starting batch edit: {n} images on {num_gpus} GPUs")
        print(f"  ğŸ Conda Environment: {self.conda_env}")
        
        # ç¼–ç æ‰€æœ‰å›¾åƒ
        print(f"[Step1XEdit-Subprocess] Encoding {n} images to base64...")
        image_b64s = [_image_to_base64(img) for img in images]
        
        # æŒ‰GPUåˆ†é…ä»»åŠ¡
        gpu_tasks = [[] for _ in range(num_gpus)]
        for i in range(n):
            gpu_idx = i % num_gpus
            gpu_tasks[gpu_idx].append({
                'task_id': i,
                'image_b64': image_b64s[i],
                'instruction': instructions[i],
                'seed': base_seed + i
            })
        
        # æ˜¾ç¤ºä»»åŠ¡åˆ†é…
        print("=" * 70)
        print("ğŸ“‹ Task Assignment (Subprocess Mode):")
        print("=" * 70)
        for gpu_idx, gpu_id in enumerate(self.device_ids):
            num_tasks = len(gpu_tasks[gpu_idx])
            print(f"  GPU {gpu_id}: {num_tasks} tasks")
        print("=" * 70)
        print()
        
        # ç»“æœåˆ—è¡¨
        results = [None] * n
        
        # å¹¶è¡Œå¯åŠ¨subprocess
        with ThreadPoolExecutor(max_workers=num_gpus) as executor:
            futures = []
            for gpu_idx, gpu_id in enumerate(self.device_ids):
                if gpu_tasks[gpu_idx]:
                    future = executor.submit(
                        self._call_subprocess_single_gpu,
                        gpu_tasks[gpu_idx],
                        gpu_id
                    )
                    futures.append((future, gpu_idx, gpu_id))
            
            # æ”¶é›†ç»“æœ
            for future, gpu_idx, gpu_id in futures:
                try:
                    gpu_results = future.result()
                    for result in gpu_results:
                        task_id = result['task_id']
                        if result['success']:
                            results[task_id] = _base64_to_image(result['image_b64'])
                        else:
                            print(f"âŒ Task {task_id} failed on GPU {gpu_id}: {result['error']}")
                            results[task_id] = images[task_id]  # fallback to original
                except Exception as e:
                    print(f"âŒ Error in GPU {gpu_id} subprocess: {e}")
                    # å¯¹è¯¥GPUçš„æ‰€æœ‰ä»»åŠ¡ä½¿ç”¨åŸå›¾ä½œä¸ºfallback
                    for task in gpu_tasks[gpu_idx]:
                        task_id = task['task_id']
                        if results[task_id] is None:
                            results[task_id] = images[task_id]
        
        print(f"âœ… Batch edit (subprocess) completed: {n} images\n")
        return results
    
    def _call_subprocess_single_gpu(self, tasks: List[Dict], gpu_id: int) -> List[Dict]:
        """
        åœ¨æŒ‡å®šGPUä¸Šè°ƒç”¨subprocessæ‰§è¡Œç¼–è¾‘ä»»åŠ¡
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«task_id, image_b64, instruction, seed
            gpu_id: GPU ID
            
        Returns:
            ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«task_id, success, image_b64, error
        """
        if not tasks:
            return []
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        input_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)
        output_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)
        
        try:
            # å†™å…¥è¾“å…¥æ•°æ®
            input_data = {'tasks': tasks}
            json.dump(input_data, input_file)
            input_file.close()
            output_file.close()
            
            # æ„å»ºå‘½ä»¤
            cmd = [
                'conda', 'run', '-n', self.conda_env, '--no-capture-output',
                'python', str(self.worker_script)
            ]
            
            # æ·»åŠ å‚æ•°
            cmd.extend([
                '--input', input_file.name,
                '--output', output_file.name,
                '--model-name', self.model_name,
                '--device', f'cuda:{gpu_id}',
                '--dtype', self.config.get('dtype', 'bfloat16'),
                '--num-inference-steps', str(self.config.get('num_inference_steps', 50)),
                '--true-cfg-scale', str(self.config.get('true_cfg_scale', 6.0)),
                '--seed', str(self.config.get('seed', 0)),
            ])
            
            if self.config.get('enable_thinking_mode', False):
                cmd.append('--enable-thinking-mode')
            if self.config.get('enable_reflection_mode', False):
                cmd.append('--enable-reflection-mode')
            if self.config.get('disable_progress_bar', True):
                cmd.append('--disable-progress-bar')
            
            print(f"[GPU {gpu_id}] Starting subprocess with {len(tasks)} tasks...")
            
            # æ‰§è¡Œsubprocessï¼ˆå®æ—¶è¾“å‡ºï¼‰
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # å®æ—¶æ‰“å°stderr
            while True:
                stderr_line = process.stderr.readline()
                if stderr_line:
                    print(f"[GPU {gpu_id}] {stderr_line.rstrip()}")
                elif process.poll() is not None:
                    break
            
            # è·å–å‰©ä½™è¾“å‡º
            remaining_stderr = process.stderr.read()
            if remaining_stderr:
                for line in remaining_stderr.split('\n'):
                    if line.strip():
                        print(f"[GPU {gpu_id}] {line}")
            
            # ç­‰å¾…å®Œæˆ
            return_code = process.wait(timeout=3600)  # 1å°æ—¶è¶…æ—¶
            
            if return_code != 0:
                raise RuntimeError(f"Subprocess failed with return code {return_code}")
            
            # è¯»å–è¾“å‡º
            with open(output_file.name, 'r') as f:
                output_data = json.load(f)
            
            if output_data.get('status') != 'success':
                raise RuntimeError(f"Worker error: {output_data.get('error', 'Unknown')}")
            
            return output_data['results']
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            Path(input_file.name).unlink(missing_ok=True)
            Path(output_file.name).unlink(missing_ok=True)
    
    def _batch_edit_with_sync(self, images, instructions, n, num_gpus, base_seed, **kwargs):
        """
        æ‰¹æ¬¡åŒæ­¥æ¨¡å¼ï¼šç¡®ä¿æ¯æ‰¹æ‰€æœ‰GPUå®Œæˆåå†å¼€å§‹ä¸‹ä¸€æ‰¹ï¼ˆå¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰
        """
        results = [None] * n
        
        # è®¡ç®—æ‰¹æ¬¡æ•°
        num_batches = (n + num_gpus - 1) // num_gpus
        
        print(f"ğŸ”„ Batch synchronization mode (multiprocess):")
        print(f"   - Total batches: {num_batches}")
        print(f"   - Batch size: {num_gpus} (one task per GPU process)")
        print(f"   - All GPU processes will stay synchronized at batch boundaries\n")
        
        # æ€»è¿›åº¦æ¡
        with tqdm(total=n, desc="[SYNC] Editing images", unit="img") as pbar:
            # é€æ‰¹å¤„ç†
            for batch_idx in range(num_batches):
                batch_start = batch_idx * num_gpus
                batch_end = min(batch_start + num_gpus, n)
                batch_size = batch_end - batch_start
                
                # å‡†å¤‡å½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡
                task_indices = []
                for i in range(batch_start, batch_end):
                    # ä½¿ç”¨å…¨å±€è½®è¯¢åˆ†é…ï¼ši % num_gpus ç¡®ä¿æ‰€æœ‰ä»»åŠ¡æŒ‰é¡ºåºè½®è¯¢åˆ†é…åˆ°ä¸åŒGPU
                    gpu_idx = i % num_gpus
                    current_seed = base_seed + i
                    
                    # ç¼–ç å›¾åƒ
                    image_b64 = _image_to_base64(images[i])
                    
                    # å‘é€ä»»åŠ¡åˆ°å¯¹åº”çš„GPUè¿›ç¨‹
                    task = (i, image_b64, instructions[i], current_seed, kwargs)
                    self.task_queues[gpu_idx].put(task)
                    task_indices.append(i)
                
                # ç­‰å¾…å½“å‰æ‰¹æ¬¡æ‰€æœ‰ä»»åŠ¡å®Œæˆï¼ˆåŒæ­¥ç‚¹ï¼‰
                # ä½¿ç”¨å­—å…¸è·Ÿè¸ªå½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡ï¼Œç¡®ä¿åªæ”¶é›†å½“å‰æ‰¹æ¬¡çš„ç»“æœ
                batch_results = {}
                expected_task_ids = set(task_indices)
                
                # ç­‰å¾…ç›´åˆ°æ”¶é›†åˆ°å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰ç»“æœ
                # è¿™æ˜¯çœŸæ­£çš„åŒæ­¥ç‚¹ï¼šå¿…é¡»ç­‰å¾…å½“å‰æ‰¹æ¬¡çš„æ‰€æœ‰ä»»åŠ¡å®Œæˆæ‰èƒ½ç»§ç»­ä¸‹ä¸€æ‰¹
                while len(batch_results) < len(task_indices):
                    try:
                        task_id, success, result_b64, error = self.result_queue.get()
                        
                        # åªå¤„ç†å±äºå½“å‰æ‰¹æ¬¡çš„ä»»åŠ¡ç»“æœ
                        if task_id in expected_task_ids:
                            if success:
                                batch_results[task_id] = _base64_to_image(result_b64)
                            else:
                                print(f"\nâŒ Error editing image {task_id}: {error}")
                                batch_results[task_id] = images[task_id]  # fallback
                            pbar.update(1)
                        else:
                            # å¦‚æœæ”¶åˆ°ä¸å±äºå½“å‰æ‰¹æ¬¡çš„ç»“æœï¼Œè¯´æ˜æœ‰è¿›ç¨‹æå‰å®Œæˆäº†ä»»åŠ¡
                            # è¿™ç§æƒ…å†µä¸åº”è¯¥åœ¨åŒæ­¥æ¨¡å¼ä¸‹å‘ç”Ÿï¼ˆå› ä¸ºä»»åŠ¡æ˜¯æŒ‰æ‰¹æ¬¡æäº¤çš„ï¼‰
                            # ä½†ä¸ºäº†å¥å£®æ€§ï¼Œæˆ‘ä»¬ä»ç„¶å¤„ç†å®ƒï¼Œä½†ä¼šè®°å½•è­¦å‘Š
                            print(f"\nâš ï¸  [SYNC] Received result for task {task_id} outside current batch {expected_task_ids}")
                            # ç›´æ¥å¤„ç†ï¼Œå› ä¸ºtask_idå¯ä»¥æ­£ç¡®åŒ¹é…åˆ°resultsæ•°ç»„
                            if success:
                                results[task_id] = _base64_to_image(result_b64)
                            else:
                                results[task_id] = images[task_id]
                            pbar.update(1)
                    except Exception as e:
                        print(f"\nâŒ Error receiving result: {e}")
                
                # å°†å½“å‰æ‰¹æ¬¡çš„ç»“æœå†™å…¥resultsæ•°ç»„ï¼ˆç¡®ä¿æ‰€æœ‰ç»“æœéƒ½å·²æ”¶é›†ï¼‰
                for task_id in task_indices:
                    if task_id in batch_results:
                        results[task_id] = batch_results[task_id]
                    else:
                        # å¦‚æœæŸä¸ªä»»åŠ¡æ²¡æœ‰ç»“æœï¼ˆç†è®ºä¸Šä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œä½¿ç”¨åŸå›¾ä½œä¸ºfallback
                        print(f"\nâš ï¸  [SYNC] Missing result for task {task_id} in batch {batch_idx}")
                        results[task_id] = images[task_id]
                
                # å½“å‰æ‰¹æ¬¡å®Œæˆï¼Œæ‰€æœ‰GPUè¿›ç¨‹å·²åŒæ­¥ï¼Œå¯ä»¥å¼€å§‹ä¸‹ä¸€æ‰¹
                if batch_idx < num_batches - 1:
                    pbar.set_postfix_str(f"Batch {batch_idx+1}/{num_batches} done, GPUs synced âœ“")
        
        return results
    
    def _batch_edit_no_sync(self, images, instructions, n, num_gpus, base_seed, **kwargs):
        """
        æ— åŒæ­¥æ¨¡å¼ï¼šä¸€æ¬¡æ€§æäº¤æ‰€æœ‰ä»»åŠ¡ï¼ˆå¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰
        """
        results = [None] * n
        
        print(f"âš¡ No-sync mode (multiprocess): All {n} tasks submitted at once\n")
        
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        for idx in range(n):
            gpu_idx = idx % num_gpus
            current_seed = base_seed + idx
            
            # ç¼–ç å›¾åƒ
            image_b64 = _image_to_base64(images[idx])
            
            # å‘é€ä»»åŠ¡åˆ°å¯¹åº”çš„GPUè¿›ç¨‹
            task = (idx, image_b64, instructions[idx], current_seed, kwargs)
            self.task_queues[gpu_idx].put(task)
        
        # æ”¶é›†ç»“æœï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
        with tqdm(total=n, desc="[NO-SYNC] Editing images", unit="img") as pbar:
            for _ in range(n):
                try:
                    task_id, success, result_b64, error = self.result_queue.get()
                    
                    if success:
                        results[task_id] = _base64_to_image(result_b64)
                    else:
                        print(f"\nâŒ Error editing image {task_id}: {error}")
                        results[task_id] = images[task_id]
                except Exception as e:
                    print(f"\nâŒ Error receiving result: {e}")
                finally:
                    pbar.update(1)
        
        return results
    
    def _cleanup_processes(self):
        """
        æ¸…ç†æ‰€æœ‰workerè¿›ç¨‹å’Œé˜Ÿåˆ—
        """
        if not hasattr(self, 'processes'):
            return
        
        print(f"[Step1XEdit] ğŸ§¹ Cleaning up {len(self.processes)} worker processes...")
        
        # å‘æ‰€æœ‰å­˜æ´»çš„è¿›ç¨‹å‘é€åœæ­¢ä¿¡å·
        if hasattr(self, 'task_queues'):
            for task_queue in self.task_queues:
                try:
                    task_queue.put(None)
                except:
                    pass  # é˜Ÿåˆ—å¯èƒ½å·²æŸå
        
        # ç»ˆæ­¢æ‰€æœ‰è¿›ç¨‹
        for gpu_id, p in self.processes:
            if p.is_alive():
                try:
                    p.terminate()
                    p.join(timeout=5)
                    print(f"[Step1XEdit]   âœ… GPU {gpu_id} process terminated")
                except Exception as e:
                    print(f"[Step1XEdit]   âš ï¸  Error terminating GPU {gpu_id} process: {e}")
            else:
                print(f"[Step1XEdit]   âœ“ GPU {gpu_id} process already dead")
        
        # æ¸…ç†é˜Ÿåˆ—ï¼ˆé‡è¦ï¼šé˜²æ­¢é˜Ÿåˆ—å †ç§¯å¯¼è‡´å†…å­˜æ³„æ¼ï¼‰
        if hasattr(self, 'task_queues'):
            for task_queue in self.task_queues:
                # æ¸…ç©ºé˜Ÿåˆ—ä¸­çš„æ‰€æœ‰ä»»åŠ¡
                while not task_queue.empty():
                    try:
                        task_queue.get_nowait()
                    except:
                        break
        
        if hasattr(self, 'result_queue'):
            while not self.result_queue.empty():
                try:
                    self.result_queue.get_nowait()
                except:
                    break
        
        print(f"[Step1XEdit] âœ… Cleanup complete")
    
    def unload_from_gpu(self):
        """
        åœæ­¢æ‰€æœ‰å·¥ä½œè¿›ç¨‹ï¼ˆæ¸…ç†èµ„æºï¼‰
        """
        if self.use_subprocess:
            # Subprocessæ¨¡å¼ï¼šæ— éœ€æ¸…ç†ï¼Œæ¨¡å‹åœ¨subprocessç»“æŸæ—¶è‡ªåŠ¨é‡Šæ”¾
            print(f"[Step1XEdit] Subprocess mode: resources auto-released")
            return
        
        if not hasattr(self, 'processes') or len(self.processes) == 0:
            print(f"[Step1XEdit] No processes to unload")
            return
        
        # Multiprocessingæ¨¡å¼ï¼šåœæ­¢å·¥ä½œè¿›ç¨‹
        print(f"[Step1XEdit] Stopping {len(self.processes)} worker processes...")
        
        # å‘æ‰€æœ‰è¿›ç¨‹å‘é€åœæ­¢ä¿¡å·ï¼ˆä¼˜é›…é€€å‡ºï¼‰
        if hasattr(self, 'task_queues'):
            for task_queue in self.task_queues:
                try:
                    task_queue.put(None)
                except:
                    pass  # é˜Ÿåˆ—å¯èƒ½å·²æŸå
        
        # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹ä¼˜é›…é€€å‡ºï¼ˆæœ€å¤š5ç§’æ¯ä¸ªè¿›ç¨‹ï¼‰
        import time
        start_time = time.time()
        for gpu_id, p in self.processes:
            if not p.is_alive():
                print(f"[Step1XEdit] âœ“ GPU {gpu_id} process already stopped")
                continue
            
            try:
                remaining_time = max(1, 5 - (time.time() - start_time))
                p.join(timeout=remaining_time)
                
                if p.is_alive():
                    print(f"[Step1XEdit] âš ï¸  GPU {gpu_id} process did not terminate gracefully, forcing...")
                    p.terminate()
                    p.join(timeout=3)
                    
                    # å¦‚æœterminateè¿˜ä¸è¡Œï¼Œä½¿ç”¨kill
                    if p.is_alive():
                        print(f"[Step1XEdit] âš ï¸  GPU {gpu_id} process did not respond to SIGTERM, killing...")
                        p.kill()
                        p.join(timeout=2)
                        
                        if p.is_alive():
                            print(f"[Step1XEdit] âŒ GPU {gpu_id} process is unresponsive (zombie)")
                        else:
                            print(f"[Step1XEdit] âœ… GPU {gpu_id} process killed")
                    else:
                        print(f"[Step1XEdit] âœ… GPU {gpu_id} process terminated")
                else:
                    print(f"[Step1XEdit] âœ… GPU {gpu_id} process stopped gracefully")
                    
            except Exception as e:
                print(f"[Step1XEdit] âš ï¸  Error stopping GPU {gpu_id} process: {e}")
        
        # æ¸…ç†é˜Ÿåˆ—ä¸­çš„æ®‹ç•™æ•°æ®ï¼ˆé˜²æ­¢å†…å­˜æ³„æ¼ï¼‰
        if hasattr(self, 'task_queues'):
            for i, task_queue in enumerate(self.task_queues):
                cleared = 0
                while not task_queue.empty():
                    try:
                        task_queue.get_nowait()
                        cleared += 1
                    except:
                        break
                if cleared > 0:
                    print(f"[Step1XEdit] ğŸ§¹ Cleared {cleared} pending tasks from GPU {self.device_ids[i]} queue")
        
        if hasattr(self, 'result_queue'):
            cleared = 0
            while not self.result_queue.empty():
                try:
                    self.result_queue.get_nowait()
                    cleared += 1
                except:
                    break
            if cleared > 0:
                print(f"[Step1XEdit] ğŸ§¹ Cleared {cleared} pending results from result queue")
        
        print(f"[Step1XEdit] âœ… All worker processes stopped")
    
    def load_to_gpu(self, parallel: bool = True):
        """
        å°†æ¨¡å‹åŠ è½½åˆ°GPUï¼ˆé‡æ–°å¯åŠ¨workerè¿›ç¨‹/subprocessï¼‰
        
        å¤šè¿›ç¨‹ç‰ˆæœ¬éœ€è¦æ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜æ´»ï¼Œå¦‚æœè¿›ç¨‹å·²æ­»åˆ™é‡æ–°å¯åŠ¨
        """
        if self.use_subprocess:
            # Subprocessæ¨¡å¼ä¸éœ€è¦ç»´æŠ¤é•¿æœŸè¿è¡Œçš„è¿›ç¨‹ï¼Œæ¯æ¬¡è°ƒç”¨æ—¶å¯åŠ¨
            print(f"[Step1XEdit] Subprocess mode: models loaded on-demand")
            return
        
        # å¤šè¿›ç¨‹æ¨¡å¼ï¼šæ£€æŸ¥è¿›ç¨‹æ˜¯å¦å­˜æ´»
        if hasattr(self, 'processes') and len(self.processes) > 0:
            alive_processes = [p for _, p in self.processes if p.is_alive()]
            dead_processes = [gpu_id for gpu_id, p in self.processes if not p.is_alive()]
            
            if len(alive_processes) == len(self.processes):
                print(f"[Step1XEdit] âœ… All {len(self.processes)} worker processes are already running")
                return
            else:
                print(f"[Step1XEdit] âš ï¸  Detected {len(dead_processes)} dead processes: {dead_processes}")
                print(f"[Step1XEdit] ğŸ”„ Restarting all worker processes...")
                # æ¸…ç†æ‰€æœ‰è¿›ç¨‹å’Œé˜Ÿåˆ—
                self._cleanup_processes()
        
        # é‡æ–°åˆå§‹åŒ–ï¼ˆå¯åŠ¨æ–°è¿›ç¨‹ï¼‰
        print(f"[Step1XEdit] ğŸš€ Initializing worker processes...")
        self._initialize()
    
    def __del__(self):
        """æ¸…ç†èµ„æºï¼ˆææ„å‡½æ•°ï¼‰"""
        if hasattr(self, 'use_subprocess') and self.use_subprocess:
            # Subprocessæ¨¡å¼ï¼šæ— éœ€é¢å¤–æ¸…ç†
            return
        
        if hasattr(self, 'processes') and len(self.processes) > 0:
            # Multiprocessingæ¨¡å¼ï¼šç¡®ä¿æ‰€æœ‰è¿›ç¨‹éƒ½å·²åœæ­¢ï¼ˆä½¿ç”¨å¼ºåˆ¶æ–¹å¼ï¼‰
            for gpu_id, p in self.processes:
                if p.is_alive():
                    try:
                        # ç›´æ¥terminateï¼Œä¸ç­‰å¾…ä¼˜é›…é€€å‡º
                        p.terminate()
                        p.join(timeout=2)
                        
                        # å¦‚æœè¿˜æ´»ç€ï¼Œå¼ºåˆ¶kill
                        if p.is_alive():
                            p.kill()
                            p.join(timeout=1)
                    except:
                        pass  # ææ„å‡½æ•°ä¸åº”æŠ›å‡ºå¼‚å¸¸
