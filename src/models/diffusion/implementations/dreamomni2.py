"""
DreamOmni2 diffusion model implementation
DreamOmni2 å›¾åƒç¼–è¾‘æ¨¡å‹å®ç°ï¼ˆæ”¯æŒå¤šGPUå¹¶è¡Œ - Subprocessæ¨¡å¼ï¼‰

åŸºäº DreamOmni2 å®˜æ–¹ä»“åº“
ä½¿ç”¨Subprocessæ¨¡å¼å®ç°å¤šGPUå¹¶è¡Œ

DreamOmni2 ç‰¹ç‚¹ï¼š
- åŒæ¨¡å‹æ¶æ„ï¼šVLM (Qwen2.5-VL) + Diffusion (FLUX.1-Kontext + LoRA)
- ä¸¤é˜¶æ®µæ¨ç†ï¼šVLMç”Ÿæˆprompt â†’ Diffusionç”Ÿæˆå›¾åƒ
- éœ€è¦ç‰¹å®šcondaç¯å¢ƒè¿è¡Œ
"""

import subprocess
import tempfile
import json
import torch
from PIL import Image
from typing import List, Dict, Any
from pathlib import Path
from tqdm import tqdm
import base64
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor

from ..base_diffusion import BaseDiffusionModel
from ....utils import setup_logger


def _image_to_base64(image: Image.Image) -> str:
    """å°†PIL Imageè½¬æ¢ä¸ºbase64å­—ç¬¦ä¸²"""
    buffer = BytesIO()
    image.save(buffer, format='PNG')
    return base64.b64encode(buffer.getvalue()).decode('utf-8')


def _base64_to_image(b64_str: str) -> Image.Image:
    """å°†base64å­—ç¬¦ä¸²è½¬æ¢ä¸ºPIL Image"""
    image_data = base64.b64decode(b64_str)
    return Image.open(BytesIO(image_data))


class DreamOmni2Model(BaseDiffusionModel):
    """
    DreamOmni2 å›¾åƒç¼–è¾‘æ¨¡å‹ï¼ˆå¤šGPUå¹¶è¡Œ - Subprocessæ¨¡å¼ï¼‰
    
    ä½¿ç”¨subprocesså®ç°æ•°æ®å¹¶è¡Œï¼š
    - æ¯ä¸ªGPUå¯åŠ¨ç‹¬ç«‹subprocess
    - æ¯ä¸ªsubprocessåŠ è½½VLMå’ŒDiffusionåŒæ¨¡å‹
    - ä»»åŠ¡æŒ‰è½®è¯¢æ–¹å¼åˆ†é…åˆ°å„ä¸ªGPU
    
    ç‰¹ç‚¹ï¼š
    - åŒæ¨¡å‹æ¶æ„ï¼šVLM + Diffusion
    - æ”¯æŒå¤šGPUå¹¶è¡Œå¤„ç†
    - ç¯å¢ƒéš”ç¦»ï¼Œæ”¯æŒç‰¹å®šcondaç¯å¢ƒ
    """
    
    def _initialize(self):
        """åˆå§‹åŒ–DreamOmni2æ¨¡å‹"""
        # è·å–é…ç½®
        self.vlm_path = self.config.get("vlm_path")
        self.base_model_path = self.config.get("base_model_path")
        self.edit_lora_path = self.config.get("edit_lora_path")
        self.dreamomni2_repo = self.config.get("dreamomni2_repo", "/data2/yixuan/DreamOmni2")
        
        # éªŒè¯å¿…è¦é…ç½®
        if not self.vlm_path:
            raise ValueError("vlm_path is required for DreamOmni2")
        if not self.base_model_path:
            raise ValueError("base_model_path is required for DreamOmni2")
        if not self.edit_lora_path:
            raise ValueError("edit_lora_path is required for DreamOmni2")
        
        device_ids = self.config.get("device_ids", None)
        
        # DreamOmni2 åªæ”¯æŒ subprocess æ¨¡å¼
        self.conda_env = self.config.get("conda_env", None)
        if self.conda_env is None:
            raise ValueError("conda_env is required for DreamOmni2 (subprocess mode only)")
        
        # ç¡®å®šä½¿ç”¨å“ªäº›GPU
        if device_ids is None:
            self.num_gpus = torch.cuda.device_count()
            self.device_ids = list(range(self.num_gpus))
        else:
            self.device_ids = device_ids
            self.num_gpus = len(device_ids)
        
        print(f"[DreamOmni2] æ£€æµ‹åˆ° {torch.cuda.device_count()} ä¸ªGPU")
        print(f"[DreamOmni2] å°†ä½¿ç”¨ {self.num_gpus} ä¸ªGPU: {self.device_ids}")
        
        # åˆå§‹åŒ–subprocessæ¨¡å¼
        self._initialize_subprocess_mode()
    
    def _initialize_subprocess_mode(self):
        """åˆå§‹åŒ–subprocessæ¨¡å¼"""
        print(f"[DreamOmni2] ä½¿ç”¨Subprocessæ¨¡å¼ï¼ˆcondaç¯å¢ƒ: {self.conda_env}ï¼‰\n")
        
        # æŸ¥æ‰¾workerè„šæœ¬è·¯å¾„
        current_dir = Path(__file__).parent.parent
        self.worker_script = current_dir / "dreamomni2_subprocess_worker.py"
        
        if not self.worker_script.exists():
            raise FileNotFoundError(f"Worker script not found: {self.worker_script}")
        
        # éªŒè¯DreamOmni2ä»“åº“è·¯å¾„
        dreamomni2_path = Path(self.dreamomni2_repo)
        if not dreamomni2_path.exists():
            raise FileNotFoundError(f"DreamOmni2 repo not found: {self.dreamomni2_repo}")
        
        print("=" * 70)
        print("ğŸš€ DreamOmni2 Subprocess Mode Initialized")
        print("=" * 70)
        print(f"  Conda Environment: {self.conda_env}")
        print(f"  Worker Script: {self.worker_script}")
        print(f"  DreamOmni2 Repo: {self.dreamomni2_repo}")
        print(f"  VLM Path: {self.vlm_path}")
        print(f"  Base Model: {self.base_model_path}")
        print(f"  Edit LoRA: {self.edit_lora_path}")
        print(f"  GPUs: {self.device_ids}")
        print("  âš¡ Models will be loaded on-demand in subprocess")
        print("=" * 70)
        print()
    
    def edit_image(self, original_image: Image.Image, 
                   edit_instruction: str,
                   **kwargs) -> Image.Image:
        """
        ç¼–è¾‘å•å¼ å›¾åƒ
        
        Args:
            original_image: åŸå§‹PILå›¾åƒ
            edit_instruction: ç¼–è¾‘æŒ‡ä»¤
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç¼–è¾‘åçš„PILå›¾åƒ
        """
        results = self.batch_edit([original_image], [edit_instruction], **kwargs)
        return results[0]
    
    def batch_edit(self, images: List[Image.Image],
                   instructions: List[str],
                   **kwargs) -> List[Image.Image]:
        """
        å¤šGPUå¹¶è¡Œæ‰¹é‡ç¼–è¾‘å›¾åƒ
        
        Args:
            images: åŸå§‹å›¾åƒåˆ—è¡¨
            instructions: ç¼–è¾‘æŒ‡ä»¤åˆ—è¡¨
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            ç¼–è¾‘åçš„å›¾åƒåˆ—è¡¨
        """
        if len(images) != len(instructions):
            raise ValueError("Number of images must match number of instructions")
        
        return self._batch_edit_subprocess(images, instructions, **kwargs)
    
    def _batch_edit_subprocess(self, images: List[Image.Image],
                                instructions: List[str],
                                **kwargs) -> List[Image.Image]:
        """
        Subprocessæ¨¡å¼æ‰¹é‡ç¼–è¾‘
        
        å°†ä»»åŠ¡åˆ†é…åˆ°å¤šä¸ªGPUï¼Œæ¯ä¸ªGPUå¯åŠ¨ä¸€ä¸ªsubprocessæ‰§è¡Œ
        """
        n = len(images)
        num_gpus = self.num_gpus
        base_seed = kwargs.get("seed", self.config.get("seed", 0))
        
        print(f"\n[DreamOmni2-Subprocess] Starting batch edit: {n} images on {num_gpus} GPUs")
        print(f"  ğŸ Conda Environment: {self.conda_env}")
        
        # ç¼–ç æ‰€æœ‰å›¾åƒ
        print(f"[DreamOmni2-Subprocess] Encoding {n} images to base64...")
        image_b64s = [_image_to_base64(img) for img in images]
        
        # æŒ‰GPUåˆ†é…ä»»åŠ¡
        gpu_tasks = [[] for _ in range(num_gpus)]
        for i in range(n):
            gpu_idx = i % num_gpus
            gpu_tasks[gpu_idx].append({
                'task_id': i,
                'image_b64': image_b64s[i],
                'instruction': instructions[i],
                'seed': base_seed + i
            })
        
        # æ˜¾ç¤ºä»»åŠ¡åˆ†é…
        print("=" * 70)
        print("ğŸ“‹ Task Assignment (DreamOmni2 Subprocess Mode):")
        print("=" * 70)
        for gpu_idx, gpu_id in enumerate(self.device_ids):
            num_tasks = len(gpu_tasks[gpu_idx])
            print(f"  GPU {gpu_id}: {num_tasks} tasks")
        print("=" * 70)
        print()
        
        # ç»“æœåˆ—è¡¨
        results = [None] * n
        
        # å¹¶è¡Œå¯åŠ¨subprocess
        with ThreadPoolExecutor(max_workers=num_gpus) as executor:
            futures = []
            for gpu_idx, gpu_id in enumerate(self.device_ids):
                if gpu_tasks[gpu_idx]:
                    future = executor.submit(
                        self._call_subprocess_single_gpu,
                        gpu_tasks[gpu_idx],
                        gpu_id
                    )
                    futures.append((future, gpu_idx, gpu_id))
            
            # æ”¶é›†ç»“æœ
            for future, gpu_idx, gpu_id in futures:
                try:
                    gpu_results = future.result()
                    for result in gpu_results:
                        task_id = result['task_id']
                        if result['success']:
                            results[task_id] = _base64_to_image(result['image_b64'])
                        else:
                            print(f"âŒ Task {task_id} failed on GPU {gpu_id}: {result['error']}")
                            results[task_id] = images[task_id]  # fallback to original
                except Exception as e:
                    print(f"âŒ Error in GPU {gpu_id} subprocess: {e}")
                    import traceback
                    traceback.print_exc()
                    # å¯¹è¯¥GPUçš„æ‰€æœ‰ä»»åŠ¡ä½¿ç”¨åŸå›¾ä½œä¸ºfallback
                    for task in gpu_tasks[gpu_idx]:
                        task_id = task['task_id']
                        if results[task_id] is None:
                            results[task_id] = images[task_id]
        
        print(f"âœ… Batch edit (DreamOmni2 subprocess) completed: {n} images\n")
        return results
    
    def _call_subprocess_single_gpu(self, tasks: List[Dict], gpu_id: int) -> List[Dict]:
        """
        åœ¨æŒ‡å®šGPUä¸Šè°ƒç”¨subprocessæ‰§è¡Œç¼–è¾‘ä»»åŠ¡
        
        Args:
            tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªä»»åŠ¡åŒ…å«task_id, image_b64, instruction, seed
            gpu_id: GPU ID
            
        Returns:
            ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å«task_id, success, image_b64, error
        """
        if not tasks:
            return []
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        input_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)
        output_file = tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False)
        
        try:
            # å†™å…¥è¾“å…¥æ•°æ®
            input_data = {'tasks': tasks}
            json.dump(input_data, input_file)
            input_file.close()
            output_file.close()
            
            # æ„å»ºå‘½ä»¤
            cmd = [
                'conda', 'run', '-n', self.conda_env, '--no-capture-output',
                'python', str(self.worker_script)
            ]
            
            # æ·»åŠ å‚æ•°
            cmd.extend([
                '--input', input_file.name,
                '--output', output_file.name,
                '--vlm-path', self.vlm_path,
                '--base-model-path', self.base_model_path,
                '--edit-lora-path', self.edit_lora_path,
                '--dreamomni2-repo', self.dreamomni2_repo,
                '--device', f'cuda:{gpu_id}',
                '--dtype', self.config.get('dtype', 'bfloat16'),
                '--num-inference-steps', str(self.config.get('num_inference_steps', 30)),
                '--guidance-scale', str(self.config.get('guidance_scale', 3.5)),
                '--seed', str(self.config.get('seed', 0)),
            ])
            
            if self.config.get('disable_progress_bar', True):
                cmd.append('--disable-progress-bar')
            
            print(f"[GPU {gpu_id}] Starting DreamOmni2 subprocess with {len(tasks)} tasks...")
            
            # æ‰§è¡Œsubprocessï¼ˆå®æ—¶è¾“å‡ºï¼‰
            process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            
            # å®æ—¶æ‰“å°stderr
            while True:
                stderr_line = process.stderr.readline()
                if stderr_line:
                    print(f"[GPU {gpu_id}] {stderr_line.rstrip()}")
                elif process.poll() is not None:
                    break
            
            # è·å–å‰©ä½™è¾“å‡º
            remaining_stderr = process.stderr.read()
            if remaining_stderr:
                for line in remaining_stderr.split('\n'):
                    if line.strip():
                        print(f"[GPU {gpu_id}] {line}")
            
            # ç­‰å¾…å®Œæˆ
            return_code = process.wait(timeout=7200)  # 2å°æ—¶è¶…æ—¶ï¼ˆDreamOmni2è¾ƒæ…¢ï¼‰
            
            if return_code != 0:
                raise RuntimeError(f"Subprocess failed with return code {return_code}")
            
            # è¯»å–è¾“å‡º
            with open(output_file.name, 'r') as f:
                output_data = json.load(f)
            
            if output_data.get('status') != 'success':
                raise RuntimeError(f"Worker error: {output_data.get('error', 'Unknown')}")
            
            return output_data['results']
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            Path(input_file.name).unlink(missing_ok=True)
            Path(output_file.name).unlink(missing_ok=True)
    
    def unload_from_gpu(self):
        """
        Subprocessæ¨¡å¼ï¼šèµ„æºåœ¨subprocessç»“æŸæ—¶è‡ªåŠ¨é‡Šæ”¾
        """
        print(f"[DreamOmni2] Subprocess mode: resources auto-released")
    
    def load_to_gpu(self, parallel: bool = True):
        """
        Subprocessæ¨¡å¼ï¼šæ¨¡å‹åœ¨subprocesså¯åŠ¨æ—¶æŒ‰éœ€åŠ è½½
        """
        print(f"[DreamOmni2] Subprocess mode: models loaded on-demand")

