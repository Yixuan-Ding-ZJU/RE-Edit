"""
Iterative Refinement Pipeline
è¿­ä»£ä¼˜åŒ–Pipeline - é€šè¿‡MLLMåˆ†æå’ŒäºŒæ¬¡ç¼–è¾‘ä¼˜åŒ–ç»“æœ

æœŸæœ›å®ç°v3ä¸v4çš„åŒºåˆ«ï¼šreward æ‰“åˆ†å˜æˆäº†yes or no
"""

import os
import logging
import random
from typing import Dict, List, Optional, Any
from PIL import Image
from concurrent.futures import ThreadPoolExecutor, as_completed

from .data.benchmark_loader import BenchmarkLoader
from .data.iterative_data import IterativeDataPair, IterativeCategoryData, IterativeBenchmarkData
from .utils.logger import setup_logger
from .utils.image_utils import decode_base64_image
from .utils.prompt_manager import PromptManager


class IterativeRefinementPipeline:
    """
    è¿­ä»£ä¼˜åŒ–Pipeline
    
    æµç¨‹ï¼š
    1. åˆæ¬¡ç¼–è¾‘ï¼ˆPrimary Editingï¼‰
    2. MLLMåˆ†æï¼ˆMLLM Analysisï¼‰
    3. äºŒæ¬¡ç¼–è¾‘ï¼ˆRefinement Editingï¼‰
    4. å¯¹æ¯”è¯„åˆ†ï¼ˆComparative Scoringï¼‰
    5. ç”ŸæˆæŠ¥å‘Šï¼ˆReport Generationï¼‰
    """
    
    def __init__(self, config: dict):
        """
        åˆå§‹åŒ–è¿­ä»£ä¼˜åŒ–Pipeline
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.logger = setup_logger("iterative_pipeline", config.get("log_level", "INFO"))
        
        # æ•°æ®åŠ è½½
        self.data_loader = BenchmarkLoader()
        self.prompt_manager = PromptManager(config.get("prompts", {}))
        
        # æ¨¡å‹å®ä¾‹ï¼ˆå»¶è¿ŸåŠ è½½ï¼Œåˆå§‹åŒ–æ—¶ä¸ºNoneï¼‰
        self.primary_diffusion = None
        self.refinement_diffusion = None
        self.mllm = None
        self.reward_model = None
        
        # å½“å‰å·²åŠ è½½çš„æ¨¡å‹ï¼ˆç”¨äºè·Ÿè¸ªå’Œè‡ªåŠ¨å¸è½½ï¼‰
        self._current_loaded_model = None
        self._current_model_name = None
        
        # ä¿å­˜pipelineå¯åŠ¨æ—¶é—´æˆ³ï¼ˆç”¨äºæŠ¥å‘Šæ–‡ä»¶å‘½åï¼‰
        from datetime import datetime
        self.start_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜ä¸»æ¨¡å‹ç±»å‹ï¼ˆç”¨äºæŠ¥å‘Šæ–‡ä»¶å‘½åï¼‰
        self.primary_model_type = config.get("diffusion_model", {}).get("primary", {}).get("type", "unknown")
        
        # è¾“å‡ºé…ç½®
        self.output_dir = config["evaluation"].get("output_dir", "./results_iterative")
        self.save_images = config["evaluation"].get("save_images", True)
        self.enable_disk_cache = config["evaluation"].get("enable_disk_cache", False)
        self.cache_dir = config["evaluation"].get("cache_dir", "./cache")
        
        # è¯„åˆ†æŒ‡æ ‡é…ç½®ï¼ˆæ§åˆ¶æ˜¯å¦è¿›è¡Œ PQã€SC å’Œ Instruction Following è¯„ä»·ï¼‰
        self.enable_pq_metric = config["evaluation"].get("enable_pq_metric", False)  # é»˜è®¤å…³é—­
        self.enable_sc_metric = config["evaluation"].get("enable_sc_metric", False)  # é»˜è®¤å…³é—­
        self.enable_instruction_following_metric = config["evaluation"].get("enable_instruction_following_metric", False)  # é»˜è®¤å…³é—­
        
        # æ˜¯å¦è·³è¿‡ refinement æµç¨‹ï¼ˆPrimary Edit åç›´æ¥ Scoringï¼Œä»…å¯¹ primary è¯„åˆ†ï¼‰
        self.skip_refinement = config["evaluation"].get("skip_refinement", False)  # é»˜è®¤å…³é—­
        if self.skip_refinement:
            self.logger.info("âš ï¸  skip_refinement is ENABLED: Will skip Stage 2 (MLLM) and Stage 3 (Refinement)")
            self.logger.info("  Primary Edit -> Scoring -> Report. Only primary_* fields will be populated.")
        
        # Primaryè¯„åˆ†é…ç½®ï¼ˆæ§åˆ¶æ˜¯å¦å¯¹primary edited imagesè¿›è¡Œè¯„åˆ†ï¼‰
        # skip_refinement æ—¶å¼ºåˆ¶ enable_primary_scoring=True
        self.enable_primary_scoring = config["evaluation"].get("enable_primary_scoring", False)  # é»˜è®¤å…³é—­ä»¥ä¿æŒå‘åå…¼å®¹
        if self.skip_refinement:
            self.enable_primary_scoring = True  # skip_refinement æ—¶å¼ºåˆ¶ä»…å¯¹ primary è¯„åˆ†
        
        if not self.enable_pq_metric:
            self.logger.info("PQ metric evaluation is DISABLED (enable_pq_metric=False)")
        if not self.enable_sc_metric:
            self.logger.info("SC metric evaluation is DISABLED (enable_sc_metric=False)")
        if self.enable_instruction_following_metric:
            self.logger.info("Instruction Following metric evaluation is ENABLED")
        if self.enable_primary_scoring:
            self.logger.info("Primary image scoring is ENABLED (enable_primary_scoring=True)")
            self.logger.info("  Will evaluate both primary and refined edited images")
        else:
            self.logger.info("Primary image scoring is DISABLED (enable_primary_scoring=False)")
            self.logger.info("  Will only evaluate refined edited images")
        
        # è¿­ä»£refinementé…ç½®ï¼ˆæ§åˆ¶æ˜¯å¦å¯ç”¨è¿­ä»£refinementï¼‰
        self.enable_iterative_refinement = config["evaluation"].get("enable_iterative_refinement", False)  # é»˜è®¤å…³é—­ä»¥ä¿æŒå‘åå…¼å®¹
        
        if self.enable_iterative_refinement:
            self.logger.info("Iterative refinement is ENABLED (enable_iterative_refinement=True)")
            self.logger.info("  Multiple re-edit instructions will be processed iteratively")
        else:
            self.logger.info("Iterative refinement is DISABLED (enable_iterative_refinement=False)")
            self.logger.info("  Using single refinement with concatenated re-edit instructions")
        
        # Primary images ç›®å½•é…ç½®ï¼ˆç”¨äºä»å·²æœ‰ç»“æœåŠ è½½ primary edited imagesï¼‰
        self.primary_images_dir = config["evaluation"].get("primary_images_dir", None)
        # Primary image æ–‡ä»¶ååç¼€é…ç½®ï¼ˆä»…åœ¨å¯ç”¨ primary_images_dir æ—¶æœ‰æ•ˆï¼‰
        self.primary_image_suffix = config["evaluation"].get("primary_image_suffix", "_primary.png")
        # å›¾ç‰‡ä¿å­˜/åŠ è½½æ—¶çš„ category ç›®å½•æ˜ å°„ï¼ˆç”¨äºå…¼å®¹ä¸åŒçš„ç›®å½•å‘½åï¼‰
        # å½“ primary_images_dir ä½¿ç”¨ä¸­æ–‡å­ç›®å½•(ç‰©ç†/ç¯å¢ƒç­‰)æ—¶ï¼Œå¯é…ç½® category_to_dir å°†è‹±æ–‡ category æ˜ å°„åˆ°ç›®å½•å:
        #   {"physical": "ç‰©ç†", "environmental": "ç¯å¢ƒ", "cultural": "ç¤¾ä¼š", "causal": "å› æœ", "referential": "æŒ‡ä»£"}
        # ä¸é…ç½®æ—¶ç›´æ¥ä½¿ç”¨ category ä½œä¸ºç›®å½•åï¼ˆè‹±æ–‡ subset æ—¶å³ physical/environmental ç­‰ï¼‰
        self.category_to_dir = config["evaluation"].get("category_to_dir", None)
        if self.primary_images_dir:
            self.logger.info(f"Primary images directory: {self.primary_images_dir}")
            self.logger.info(f"Primary image suffix: {self.primary_image_suffix}")
            self.logger.info("Will skip Stage 1 (Primary Editing) and load images from directory")
        
        # ä½¿ç”¨åŸå›¾ä½œä¸º primary image çš„é…ç½®ï¼ˆtrickï¼šç›´æ¥ä½¿ç”¨ original_image ä½œä¸º primary_edited_imageï¼‰
        self.use_original_as_primary = config["evaluation"].get("use_original_as_primary", False)  # é»˜è®¤å…³é—­
        if self.use_original_as_primary:
            self.logger.info("âš ï¸  use_original_as_primary is ENABLED: Will use original images directly as primary images")
            self.logger.info("  This means refinement will start from original images instead of primary edited images")
        
        # è·³è¿‡ Stage4 (Comparative Scoring) çš„é…ç½®
        self.skip_stage4 = config["evaluation"].get("skip_stage4", False)  # é»˜è®¤ä¸è·³è¿‡
        if self.skip_stage4:
            self.logger.info("âš ï¸  skip_stage4 is ENABLED: Will skip Stage 4 (Comparative Scoring)")
            self.logger.info("  Pipeline will only perform MLLM analysis and refinement, without scoring")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        if self.enable_disk_cache:
            os.makedirs(self.cache_dir, exist_ok=True)
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶è·¯å¾„ï¼ˆå®æ—¶ä¿å­˜ï¼‰
        # å¦‚æœé…ç½®äº† use_original_as_primaryã€primary_images_dir æˆ– skip_refinementï¼Œåœ¨æ—¥å¿—æ–‡ä»¶åä¸­æ·»åŠ æ ‡è¯†ä»¥åŒºåˆ†
        if self.skip_refinement:
            log_filename = f"pipeline_{self.primary_model_type}_skip_refinement_{self.start_timestamp}.log"
        elif self.use_original_as_primary:
            log_filename = f"pipeline_{self.primary_model_type}_use_original_as_primary_{self.start_timestamp}.log"
        elif self.primary_images_dir:
            primary_dir_name = os.path.basename(os.path.normpath(self.primary_images_dir))
            log_filename = f"pipeline_{self.primary_model_type}_refined_only_{primary_dir_name}_{self.start_timestamp}.log"
        else:
            log_filename = f"pipeline_{self.primary_model_type}_{self.start_timestamp}.log"
        self.log_file_path = os.path.join(self.output_dir, log_filename)
        
        # é‡æ–°é…ç½®loggerï¼Œæ·»åŠ æ–‡ä»¶è¾“å‡ºï¼ˆå®æ—¶å†™å…¥ï¼‰
        self.logger = setup_logger(
            "iterative_pipeline", 
            config.get("log_level", "INFO"),
            log_file=self.log_file_path,
            console_output=True
        )
        
        # è¯„åˆ†å¥åº·åº¦è¿½è¸ªï¼ˆç”¨äºè®°å½•GPUå¤±è´¥ä¿¡æ¯ï¼‰
        self.scoring_health = {
            'primary_failures': {},      # Primaryè¯„åˆ†å¤±è´¥ä¿¡æ¯ï¼ˆä»…åœ¨enable_primary_scoringæ—¶ä½¿ç”¨ï¼‰
            'refined_failures': {},       # Refinedè¯„åˆ†å¤±è´¥ä¿¡æ¯
            'total_primary_samples': 0,   # æ€»primaryæ ·æœ¬æ•°
            'total_primary_failures': 0,  # æ€»primaryå¤±è´¥æ•°
            'total_refined_samples': 0,   # æ€»refinedæ ·æœ¬æ•°
            'total_refined_failures': 0   # æ€»refinedå¤±è´¥æ•°
        }
        
        self.logger.info("Iterative Refinement Pipeline initialized successfully (models will be loaded on demand)")
        self.logger.info(f"Log file: {self.log_file_path}")
    
    def _ensure_model_loaded(self, model_name: str):
        """
        ç¡®ä¿æŒ‡å®šæ¨¡å‹å·²åŠ è½½ï¼Œå¹¶è‡ªåŠ¨å¸è½½ä¹‹å‰çš„æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§° ("primary_diffusion", "refinement_diffusion", "mllm", "reward_model")
        """
        # å¦‚æœå½“å‰å·²ç»æ˜¯è¦åŠ è½½çš„æ¨¡å‹ï¼Œç›´æ¥è¿”å›
        if self._current_model_name == model_name and self._current_loaded_model is not None:
            self.logger.info(f"[Model Manager] {model_name} already loaded, skipping")
            return
        
        # å…ˆå¸è½½å½“å‰æ¨¡å‹
        if self._current_loaded_model is not None:
            self.logger.info(f"[Model Manager] Unloading {self._current_model_name}...")
            if hasattr(self._current_loaded_model, 'unload_from_gpu'):
                self._current_loaded_model.unload_from_gpu()
            # å®Œå…¨é‡Šæ”¾å®ä¾‹ï¼ˆå¯é€‰ï¼Œå–å†³äºæ˜¯å¦éœ€è¦é‡ç”¨ï¼‰
            # del self._current_loaded_model
            # self._current_loaded_model = None
            self._current_loaded_model = None
            self._current_model_name = None
        
        # åŠ è½½æ–°æ¨¡å‹
        self.logger.info(f"[Model Manager] Loading {model_name}...")
        
        if model_name == "primary_diffusion":
            if self.primary_diffusion is None:
                self.primary_diffusion = self._load_primary_diffusion()
            else:
                if hasattr(self.primary_diffusion, 'load_to_gpu'):
                    self.primary_diffusion.load_to_gpu()
            self._current_loaded_model = self.primary_diffusion
        
        elif model_name == "refinement_diffusion":
            if self.refinement_diffusion is None:
                self.refinement_diffusion = self._load_refinement_diffusion()
            else:
                if hasattr(self.refinement_diffusion, 'load_to_gpu'):
                    self.refinement_diffusion.load_to_gpu()
            self._current_loaded_model = self.refinement_diffusion
        
        elif model_name == "mllm":
            if self.mllm is None:
                self.mllm = self._load_mllm()
            else:
                if hasattr(self.mllm, 'load_to_gpu'):
                    self.mllm.load_to_gpu()
            self._current_loaded_model = self.mllm
        
        elif model_name == "reward_model":
            if self.reward_model is None:
                self.reward_model = self._load_reward_model()
            else:
                if hasattr(self.reward_model, 'load_to_gpu'):
                    self.reward_model.load_to_gpu()
            self._current_loaded_model = self.reward_model
        
        else:
            raise ValueError(f"Unknown model name: {model_name}")
        
        self._current_model_name = model_name
        self.logger.info(f"[Model Manager] {model_name} loaded and ready")
    
    def _unload_current_model(self):
        """å¸è½½å½“å‰åŠ è½½çš„æ¨¡å‹"""
        if self._current_loaded_model is not None:
            self.logger.info(f"[Model Manager] Unloading {self._current_model_name}...")
            if hasattr(self._current_loaded_model, 'unload_from_gpu'):
                self._current_loaded_model.unload_from_gpu()
            self._current_loaded_model = None
            self._current_model_name = None
            self.logger.info(f"[Model Manager] Model unloaded")
    
    def _load_primary_diffusion(self):
        """
        åŠ è½½å¾…è¯„æµ‹Diffusionæ¨¡å‹ï¼ˆåˆæ¬¡ç¼–è¾‘ï¼‰
        
        Returns:
            Diffusionæ¨¡å‹å®ä¾‹
        """
        diffusion_config = self.config["diffusion_model"]["primary"]
        model_type = diffusion_config["type"]
        model_params = diffusion_config.get("params", {})
        
        self.logger.info(f"Loading primary diffusion model: {model_type}")
        
        # åŠ¨æ€å¯¼å…¥
        if model_type == "multi_gpu_qwen_edit":
            from .models.diffusion.implementations.qwen_image_edit import QwenImageEditModel
            return QwenImageEditModel(model_params)
        elif model_type == "qwen_image_edit_2511":
            from .models.diffusion.implementations.qwen_image_edit_2511 import QwenImageEdit2511Model
            return QwenImageEdit2511Model(model_params)
        elif model_type == "flux_kontext":
            from .models.diffusion.implementations.flux_kontext import FluxKontextModel
            return FluxKontextModel(model_params)
        elif model_type == "step1x_edit_v1p2_preview":
            from .models.diffusion.implementations.step1x_edit_v1p2_preview import Step1XEditModel
            return Step1XEditModel(model_params)
        elif model_type == "step1x_edit_v1p1":
            from .models.diffusion.implementations.step1x_edit_v1p1 import Step1XEditPreviewModel
            return Step1XEditPreviewModel(model_params)
        elif model_type == "dreamomni2":
            from .models.diffusion.implementations.dreamomni2 import DreamOmni2Model
            return DreamOmni2Model(model_params)
        elif model_type == "omnigen2":
            from .models.diffusion.implementations.omnigen2 import OmniGen2Model
            return OmniGen2Model(model_params)
        elif model_type == "hidream_e1":
            from .models.diffusion.implementations.hidream_e1 import HiDreamE1Model
            return HiDreamE1Model(model_params)
        elif model_type == "ovis_u1":
            from .models.diffusion.implementations.ovis_u1 import OvisU1Model
            return OvisU1Model(model_params)
        elif model_type == "janus":
            from .models.diffusion.implementations.janus import JanusModel
            return JanusModel(model_params)
        elif model_type == "flux2_dev":
            from .models.diffusion.implementations.flux2_dev import Flux2DevModel
            return Flux2DevModel(model_params)
        else:
            raise ValueError(f"Unknown primary diffusion model type: {model_type}")
    
    def _load_refinement_diffusion(self):
        """
        åŠ è½½äºŒæ¬¡ç¼–è¾‘Diffusionæ¨¡å‹ï¼ˆå›ºå®šæ¨¡ç»„ï¼‰
        
        è¿™ä¸ªæ¨¡å‹ä¸MLLMæ„æˆå›ºå®šçš„ä¼˜åŒ–æ¨¡ç»„ï¼Œç”¨äºåŸºäºMLLMç”Ÿæˆçš„Re-editæŒ‡ä»¤
        å¯¹åˆæ¬¡ç¼–è¾‘ç»“æœè¿›è¡ŒäºŒæ¬¡ä¼˜åŒ–ã€‚é€šå¸¸ä½¿ç”¨æ ‡å‡†çš„Qwen-Image-Editæ¨¡å‹ã€‚
        
        Returns:
            Diffusionæ¨¡å‹å®ä¾‹
        """
        refinement_config = self.config["diffusion_model"].get("refinement")
        
        if not refinement_config:
            self.logger.warning("No refinement diffusion model configured, using primary model as fallback")
            return None
        
        model_type = refinement_config["type"]
        model_params = refinement_config.get("params", {})
        
        self.logger.info(f"Loading refinement diffusion model: {model_type}")
        self.logger.info(f"  Model: {model_params.get('model_name', 'N/A')}")
        self.logger.info(f"  GPUs: {model_params.get('device_ids', 'auto')}")
        
        # åŠ¨æ€å¯¼å…¥
        if model_type == "multi_gpu_qwen_edit":
            from .models.diffusion.implementations.qwen_image_edit import QwenImageEditModel
            return QwenImageEditModel(model_params)
        elif model_type == "qwen_image_edit_2511":
            from .models.diffusion.implementations.qwen_image_edit_2511 import QwenImageEdit2511Model
            return QwenImageEdit2511Model(model_params)
        elif model_type == "flux_kontext":
            from .models.diffusion.implementations.flux_kontext import FluxKontextModel
            return FluxKontextModel(model_params)
        elif model_type == "step1x_edit_v1p2_preview":
            from .models.diffusion.implementations.step1x_edit_v1p2_preview import Step1XEditModel
            return Step1XEditModel(model_params)
        elif model_type == "step1x_edit_v1p1":
            from .models.diffusion.implementations.step1x_edit_v1p1 import Step1XEditPreviewModel
            return Step1XEditPreviewModel(model_params)
        elif model_type == "dreamomni2":
            from .models.diffusion.implementations.dreamomni2 import DreamOmni2Model
            return DreamOmni2Model(model_params)
        elif model_type == "omnigen2":
            from .models.diffusion.implementations.omnigen2 import OmniGen2Model
            return OmniGen2Model(model_params)
        elif model_type == "hidream_e1":
            from .models.diffusion.implementations.hidream_e1 import HiDreamE1Model
            return HiDreamE1Model(model_params)
        elif model_type == "ovis_u1":
            from .models.diffusion.implementations.ovis_u1 import OvisU1Model
            return OvisU1Model(model_params)
        elif model_type == "janus":
            from .models.diffusion.implementations.janus import JanusModel
            return JanusModel(model_params)
        elif model_type == "flux2_dev":
            from .models.diffusion.implementations.flux2_dev import Flux2DevModel
            return Flux2DevModel(model_params)
        else:
            raise ValueError(f"Unknown refinement diffusion model type: {model_type}")
    
    def _load_mllm(self):
        """
        åŠ è½½MLLMæ¨¡å‹
        
        Returns:
            MLLMæ¨¡å‹å®ä¾‹
        """
        mllm_config = self.config["mllm"]
        model_type = mllm_config["type"]
        model_params = mllm_config.get("params", {})
        
        self.logger.info(f"Loading MLLM model: {model_type}")
        
        # åŠ¨æ€å¯¼å…¥
        if model_type == "qwen25_vl":
            from .models.mllm.implementations.qwen25_vl_mllm import Qwen25VLMllm
            return Qwen25VLMllm(model_params)
        else:
            raise ValueError(f"Unknown MLLM type: {model_type}")
    
    def _load_reward_model(self):
        """
        åŠ è½½Rewardæ¨¡å‹
        
        Returns:
            Rewardæ¨¡å‹å®ä¾‹
        """
        reward_config = self.config["reward_model"]
        model_type = reward_config["type"]
        model_params = reward_config.get("params", {})
        
        self.logger.info(f"Loading reward model: {model_type}")
        
        # ä¼ é€’æ—¥å¿—æ–‡ä»¶è·¯å¾„ç»™reward modelï¼ˆç”¨äºå­è¿›ç¨‹è¾“å‡ºï¼‰
        if hasattr(self, 'log_file_path'):
            model_params = model_params.copy()  # é¿å…ä¿®æ”¹åŸå§‹é…ç½®
            model_params['log_file_path'] = self.log_file_path
        
        # åŠ¨æ€å¯¼å…¥
        if model_type == "qwen3_vl_multi_gpu_subprocess":
            from .models.reward.implementations.qwen3_vl_multi_gpu_subprocess import Qwen3VLMultiGPUSubprocessRewardModel
            return Qwen3VLMultiGPUSubprocessRewardModel(model_params)
        elif model_type == "qwen3_vl_vllm_subprocess":
            from .models.reward.implementations.qwen3_vl_vllm_subprocess import Qwen3VLvLLMSubprocessRewardModel
            return Qwen3VLvLLMSubprocessRewardModel(model_params)
        elif model_type == "qwen3_vl_subprocess":
            from .models.reward.implementations.qwen3_vl_subprocess import Qwen3VLSubprocessRewardModel
            return Qwen3VLSubprocessRewardModel(model_params)
        elif model_type == "qwen3_vl":
            from .models.reward.implementations.qwen3_vl_reward import Qwen3VLRewardModel
            return Qwen3VLRewardModel(model_params)
        else:
            raise ValueError(f"Unknown reward model type: {model_type}")
    
    def _load_benchmark_data(self):
        """
        åŠ è½½benchmarkæ•°æ®
        
        ä¸standard pipelineä¿æŒä¸€è‡´çš„æ•°æ®åŠ è½½é€»è¾‘
        
        Returns:
            BenchmarkDataå¯¹è±¡
        """
        # ä»é…ç½®ä¸­è¯»å–æ•°æ®è·¯å¾„å’Œç±»åˆ«
        data_config = self.config.get("data", {})
        data_path = data_config.get("path")
        categories = data_config.get("categories", [])
        
        if not data_path:
            raise ValueError("data.path not specified in config")
        
        if not categories:
            raise ValueError("data.categories not specified in config")
        
        self.logger.info(f"Loading benchmark data from: {data_path}")
        self.logger.info(f"Categories: {categories}")
        
        # è°ƒç”¨data_loader.load()
        benchmark_data = self.data_loader.load(
            data_path=data_path,
            categories=categories,
            decode_images=False  # å»¶è¿Ÿè§£ç ä»¥èŠ‚çœå†…å­˜
        )
        
        self.logger.info(f"Loaded {benchmark_data.total_pairs} data pairs across {len(categories)} categories")
        
        return benchmark_data
    
    def run(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„è¿­ä»£ä¼˜åŒ–è¯„æµ‹
        
        Returns:
            è¯„æµ‹æŠ¥å‘Šå­—å…¸ï¼ˆä¸standard pipelineå…¼å®¹ï¼‰
        """
        self.logger.info("=" * 80)
        self.logger.info("Starting Iterative Refinement Benchmark")
        self.logger.info("=" * 80)
        
        # åŠ è½½æ•°æ®
        benchmark_data = self._load_benchmark_data()
        
        # è½¬æ¢ä¸ºIterativeDataPair
        iterative_benchmark = self._convert_to_iterative_data(benchmark_data)
        
        # æŒ‰ç±»åˆ«å¤„ç†
        for category_name in iterative_benchmark.category_names:
            category_data = iterative_benchmark.get_category(category_name)
            self.logger.info(f"\n{'='*80}")
            self.logger.info(f"Processing category: {category_name}")
            self.logger.info(f"{'='*80}")
            
            self._process_category_iterative(category_name, category_data)
        
        # ç”ŸæˆæŠ¥å‘Š
        self.logger.info("\n" + "=" * 80)
        self.logger.info("Generating comparison report...")
        self.logger.info("=" * 80)
        report = self._generate_report(iterative_benchmark)
        
        self.logger.info("\n" + "=" * 80)
        self.logger.info("âœ“ Iterative Refinement Benchmark completed successfully!")
        self.logger.info("=" * 80)
        
        return report
    
    def _convert_to_iterative_data(self, benchmark_data) -> IterativeBenchmarkData:
        """
        å°†æ ‡å‡†BenchmarkDataè½¬æ¢ä¸ºIterativeBenchmarkData
        
        Args:
            benchmark_data: æ ‡å‡†BenchmarkData
        
        Returns:
            IterativeBenchmarkData
        """
        iterative_categories = {}
        
        for category_name, category_data in benchmark_data.categories.items():
            # è½¬æ¢æ•°æ®å¯¹
            iterative_pairs = []
            for pair in category_data.data_pairs:
                iterative_pair = IterativeDataPair(
                    pair_id=pair.pair_id,
                    category=pair.category,
                    original_image_b64=pair.original_image_b64,
                    edit_instruction=pair.edit_instruction,
                    original_description=pair.original_description,
                    original_image=pair.original_image,
                    rationale=pair.rationale,  # ä¿®å¤ï¼šä¼ é€’rationaleå­—æ®µ
                    metadata=pair.metadata
                )
                iterative_pairs.append(iterative_pair)
            
            # åˆ›å»ºç±»åˆ«æ•°æ®
            iterative_categories[category_name] = IterativeCategoryData(
                category_name=category_name,
                data_pairs=iterative_pairs
            )
        
        return IterativeBenchmarkData(
            categories=iterative_categories,
            total_pairs=benchmark_data.total_pairs,
            category_names=benchmark_data.category_names,
            metadata=benchmark_data.metadata
        )
    
    def _process_category_iterative(self, category: str, category_data: IterativeCategoryData):
        """
        è¿­ä»£ä¼˜åŒ–å¤„ç†å•ä¸ªç±»åˆ«
        
        Args:
            category: ç±»åˆ«åç§°
            category_data: ç±»åˆ«æ•°æ®
        """
        # è®¡ç®—æ€»é˜¶æ®µæ•°ï¼ˆåŠ¨æ€è°ƒæ•´ï¼‰
        # skip_refinement: 2æˆ–3é˜¶æ®µï¼ˆPrimary/åŠ è½½, Scoring, Statisticsï¼‰
        # å¦‚æœè·³è¿‡ Stage1ï¼šåŸºç¡€3ä¸ªé˜¶æ®µï¼ˆMLLM, Refinement, Statisticsï¼‰ï¼Œå¦‚æœæ‰§è¡Œ Stage4 åˆ™+1
        # å¦‚æœæ‰§è¡Œ Stage1ï¼šåŸºç¡€5ä¸ªé˜¶æ®µï¼ˆPrimary, MLLM, Refinement, Scoring, Statisticsï¼‰ï¼Œå¦‚æœè·³è¿‡ Stage4 åˆ™-1
        if self.skip_refinement:
            total_stages = 2 if self.skip_stage4 else 3  # Primary/åŠ è½½, (Scoring), Statistics
        elif self.use_original_as_primary or self.primary_images_dir:
            total_stages = 3 if self.skip_stage4 else 4
        else:
            total_stages = 4 if self.skip_stage4 else 5
        
        # ===== skip_refinement æ¨¡å¼ï¼šPrimary -> Scoring -> Statistics =====
        if self.skip_refinement:
            if self.use_original_as_primary:
                self.logger.info(f"\n[STAGE 1/{total_stages}] Using Original Images as Primary Images ({category})")
                self._use_original_as_primary(category, category_data)
            elif self.primary_images_dir:
                self.logger.info(f"\n[STAGE 1/{total_stages}] Loading Primary Images from Directory ({category})")
                self._load_primary_images_from_dir(category, category_data)
            else:
                self.logger.info(f"\n[STAGE 1/{total_stages}] Primary Editing ({category})")
                self._stage1_primary_editing(category, category_data)
            
            if not self.skip_stage4:
                self.logger.info(f"\n[STAGE 2/{total_stages}] Primary-Only Scoring ({category})")
                self._stage4_comparative_scoring(category, category_data)
                self.logger.info(f"\n[STAGE 3/{total_stages}] Statistics ({category})")
                self._stage5_statistics(category, category_data)
            else:
                self.logger.info(f"\n[SKIPPED] Stage 2 (Scoring) and Stage 3 (Statistics) are skipped")
            return
        
        # ===== é˜¶æ®µ1: åŠ è½½å·²æœ‰ Primary Images =====
        if self.use_original_as_primary:
            # Trick: ç›´æ¥ä½¿ç”¨åŸå›¾ä½œä¸º primary image
            self.logger.info(f"\n[STAGE 1/{total_stages}] Using Original Images as Primary Images ({category})")
            self._use_original_as_primary(category, category_data)
        elif self.primary_images_dir:
            self.logger.info(f"\n[STAGE 1/{total_stages}] Loading Primary Images from Directory ({category})")
            self._load_primary_images_from_dir(category, category_data)
        else:
            # å¦‚æœæ²¡æœ‰é…ç½® primary_images_dirï¼Œä»ç„¶æ‰§è¡Œ stage1ï¼ˆå‘åå…¼å®¹ï¼‰
            self.logger.info(f"\n[STAGE 1/{total_stages}] Primary Editing ({category})")
            self._stage1_primary_editing(category, category_data)
            # å¦‚æœæ‰§è¡Œäº† stage1ï¼Œç»§ç»­åç»­é˜¶æ®µ
            self.logger.info(f"\n[STAGE 2/{total_stages}] MLLM Analysis ({category})")
            self._stage2_mllm_analysis(category, category_data)
            self.logger.info(f"\n[STAGE 3/{total_stages}] Refinement Editing ({category})")
            self._stage3_refinement_editing(category, category_data)
            
            # æ ¹æ® skip_stage4 å†³å®šæ˜¯å¦æ‰§è¡Œ Stage4
            if not self.skip_stage4:
                self.logger.info(f"\n[STAGE 4/{total_stages}] Comparative Scoring ({category})")
                self._stage4_comparative_scoring(category, category_data)
                # å¦‚æœæ‰§è¡Œäº† Stage4ï¼Œæ‰§è¡Œ Stage5 ç»Ÿè®¡
                self.logger.info(f"\n[STAGE 5/{total_stages}] Statistics ({category})")
                self._stage5_statistics(category, category_data)
            else:
                # å¦‚æœè·³è¿‡ Stage4ï¼Œè·³è¿‡ Stage5 ç»Ÿè®¡ï¼ˆæˆ–åªåšåŸºç¡€ç»Ÿè®¡ï¼‰
                self.logger.info(f"\n[SKIPPED] Stage 4 (Comparative Scoring) and Stage 5 (Statistics) are skipped")
            
            return
        
        # ===== é˜¶æ®µ2: MLLMåˆ†æ (MLLM Analysis) =====
        self.logger.info(f"\n[STAGE 2/{total_stages}] MLLM Analysis ({category})")
        self._stage2_mllm_analysis(category, category_data)
        
        # ===== é˜¶æ®µ3: äºŒæ¬¡ç¼–è¾‘ (Refinement Editing) =====
        self.logger.info(f"\n[STAGE 3/{total_stages}] Refinement Editing ({category})")
        self._stage3_refinement_editing(category, category_data)
        
        # ===== é˜¶æ®µ4: è¯„åˆ† (Scoring) =====
        if not self.skip_stage4:
            self.logger.info(f"\n[STAGE 4/{total_stages}] Scoring ({category})")
            self._stage4_comparative_scoring(category, category_data)
            
            # ===== ç»Ÿè®¡è®°å½• (Statistics) =====
            self.logger.info(f"\n[STATISTICS] Statistics ({category})")
            self._stage5_statistics(category, category_data)
        else:
            # å¦‚æœè·³è¿‡ Stage4ï¼Œè·³è¿‡ Stage5 ç»Ÿè®¡
            self.logger.info(f"\n[SKIPPED] Stage 4 (Comparative Scoring) and Stage 5 (Statistics) are skipped")
    
    def _stage1_primary_editing(self, category: str, category_data: IterativeCategoryData):
        """
        é˜¶æ®µ1: åˆæ¬¡ç¼–è¾‘
        
        ä½¿ç”¨å¾…è¯„æµ‹Diffusionæ¨¡å‹è¿›è¡Œåˆæ¬¡ç¼–è¾‘
        """
        # åŠ è½½Primary Diffusionæ¨¡å‹
        self._ensure_model_loaded("primary_diffusion")
        
        # å‡†å¤‡æ•°æ®
        original_images = []
        edit_instructions = []
        
        for pair in category_data.data_pairs:
            if pair.original_image is None:
                pair.original_image = decode_base64_image(pair.original_image_b64)
            original_images.append(pair.original_image)
            edit_instructions.append(pair.edit_instruction)
        
        # æ‰¹é‡ç¼–è¾‘
        self.logger.info(f"Editing {len(original_images)} images with primary model...")
        
        edited_images = []
        if hasattr(self.primary_diffusion, 'batch_edit'):
            edited_images = self.primary_diffusion.batch_edit(
                images=original_images,
                instructions=edit_instructions
            )
        else:
            # Fallback to sequential
            self.logger.warning("Primary model does not support batch_edit, using sequential processing")
            for i, (img, instr) in enumerate(zip(original_images, edit_instructions)):
                edited = self.primary_diffusion.edit_image(img, instr)
                edited_images.append(edited)
        
        # ä¿å­˜ç»“æœ
        for pair, edited_image in zip(category_data.data_pairs, edited_images):
            pair.primary_edited_image = edited_image
            
            # ä¿å­˜å›¾åƒï¼ˆå¯é€‰ï¼‰
            if self.save_images:
                self._save_image(edited_image, category, pair.pair_id, "primary")
        
        self.logger.info(f"Primary editing completed for {len(edited_images)} images")
        
        # å¸è½½æ¨¡å‹
        self._unload_current_model()
    
    def _stage2_mllm_analysis(self, category: str, category_data: IterativeCategoryData):
        """
        é˜¶æ®µ2: MLLMåˆ†æ
        
        ä½¿ç”¨MLLMåˆ†æåˆæ¬¡ç¼–è¾‘ç»“æœï¼Œç”Ÿæˆæ”¹è¿›æŒ‡ä»¤
        """
        # åŠ è½½MLLMæ¨¡å‹
        self._ensure_model_loaded("mllm")
        
        # å‡†å¤‡æ•°æ®
        # ç¡®ä¿ primary_edited_image å’Œ original_image éƒ½å·²åŠ è½½
        edited_images = []
        original_images = []
        original_descriptions = []
        edit_instructions = []
        
        for pair in category_data.data_pairs:
            # æ£€æŸ¥ primary_edited_image
            if pair.primary_edited_image is None:
                raise RuntimeError(
                    f"primary_edited_image is None for pair {pair.pair_id} in category {category}. "
                    f"Cannot proceed with MLLM analysis."
                )
            
            # ç¡®ä¿ original_image å·²è§£ç ï¼ˆå¦‚æœä½¿ç”¨ primary_images_dirï¼Œå¯èƒ½è¿˜æœªè§£ç ï¼‰
            if pair.original_image is None:
                if pair.original_image_b64:
                    pair.original_image = decode_base64_image(pair.original_image_b64)
                else:
                    raise RuntimeError(
                        f"original_image and original_image_b64 are both None for pair {pair.pair_id} "
                        f"in category {category}. Cannot proceed with MLLM analysis."
                    )
            
            edited_images.append(pair.primary_edited_image)
            original_images.append(pair.original_image)
            original_descriptions.append(pair.original_description if pair.original_description else "")
            edit_instructions.append(pair.edit_instruction if pair.edit_instruction else "")
        
        categories = [category] * len(category_data.data_pairs)
        
        # å®šä¹‰å®æ—¶è¾“å‡ºå›è°ƒå‡½æ•°
        def on_batch_complete(batch_idx, batch_results, batch_indices):
            """åœ¨æ¯ä¸ªbatchå®Œæˆåç«‹å³è¾“å‡ºç»“æœ"""
            self.logger.info(f"\n{'='*60}")
            self.logger.info(f"MLLM Batch {batch_idx + 1} Results:")
            self.logger.info(f"{'='*60}")
            
            for idx, output in zip(batch_indices, batch_results):
                pair = category_data.data_pairs[idx]
                
                # ç«‹å³ä¿å­˜ç»“æœåˆ°æ•°æ®å¯¹è±¡
                pair.cot_reasoning = output.get("cot", "")
                pair.re_edit_instruction = output.get("re_edit_instruction", "")  # æ‹¼æ¥åçš„å­—ç¬¦ä¸²ï¼ˆå‘åå…¼å®¹ï¼‰
                pair.re_edit_instructions = output.get("re_edit_instructions", [])  # ç‹¬ç«‹çš„æŒ‡ä»¤åˆ—è¡¨ï¼ˆç”¨äºè¿­ä»£refinementï¼‰
                pair.mllm_raw_output = output.get("raw_output", "")
                
                # å®æ—¶è¾“å‡ºæ¯ä¸ªæ ·æœ¬çš„MLLMåˆ†æç»“æœ
                self.logger.info(f"\n[Sample {pair.pair_id}]")
                self.logger.info(f"  Original Instruction: {pair.edit_instruction[:80]}...")
                if pair.cot_reasoning:
                    self.logger.info(f"  CoT Reasoning: {pair.cot_reasoning[:150]}...")
                else:
                    self.logger.info(f"  CoT Reasoning: (empty)")
                self.logger.info(f"  Re-edit Instruction: {pair.re_edit_instruction[:100]}...")
        
        # æ‰¹é‡åˆ†æï¼ˆå¸¦å®æ—¶è¾“å‡ºï¼‰
        self.logger.info(f"Analyzing {len(edited_images)} images with MLLM...")
        
        mllm_outputs = self.mllm.batch_analyze(
            edited_images=edited_images,
            original_images=original_images,
            original_descriptions=original_descriptions,
            edit_instructions=edit_instructions,
            categories=categories,
            on_batch_complete=on_batch_complete  # ä¼ å…¥å›è°ƒå‡½æ•°
        )
        
        # ç¡®ä¿æ‰€æœ‰ç»“æœéƒ½å·²ä¿å­˜ï¼ˆé˜²æ­¢å›è°ƒå¤±è´¥çš„æƒ…å†µï¼‰
        for pair, output in zip(category_data.data_pairs, mllm_outputs):
            if not hasattr(pair, 'cot_reasoning') or pair.cot_reasoning is None:
                pair.cot_reasoning = output.get("cot", "")
            if not hasattr(pair, 're_edit_instruction') or pair.re_edit_instruction is None:
                pair.re_edit_instruction = output.get("re_edit_instruction", "")  # æ‹¼æ¥åçš„å­—ç¬¦ä¸²ï¼ˆå‘åå…¼å®¹ï¼‰
            if not hasattr(pair, 're_edit_instructions') or pair.re_edit_instructions is None:
                pair.re_edit_instructions = output.get("re_edit_instructions", [])  # ç‹¬ç«‹çš„æŒ‡ä»¤åˆ—è¡¨ï¼ˆç”¨äºè¿­ä»£refinementï¼‰
            if not hasattr(pair, 'mllm_raw_output') or pair.mllm_raw_output is None:
                pair.mllm_raw_output = output.get("raw_output", "")
        
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"MLLM analysis completed for {len(mllm_outputs)} images")
        self.logger.info(f"{'='*60}")
        
        # å¸è½½æ¨¡å‹
        self._unload_current_model()
    
    def _stage3_refinement_editing(self, category: str, category_data: IterativeCategoryData):
        """
        é˜¶æ®µ3: äºŒæ¬¡ç¼–è¾‘
        
        ä½¿ç”¨Re-editæŒ‡ä»¤è¿›è¡ŒäºŒæ¬¡ç¼–è¾‘
        æ”¯æŒè¿­ä»£refinementï¼šå¦‚æœå¯ç”¨ä¸”MLLMè¾“å‡ºå¤šä¸ªæŒ‡ä»¤ï¼Œåˆ™è¿›è¡Œå¤šæ¬¡è¿­ä»£refinement
        """
        # ===== æ•°æ®å®Œæ•´æ€§éªŒè¯ =====
        # æ£€æŸ¥Stage 1çš„è¾“å‡ºæ˜¯å¦å®Œæ•´
        for pair in category_data.data_pairs:
            if pair.primary_edited_image is None:
                raise RuntimeError(
                    f"Pipeline implementation error: primary_edited_image is None for pair {pair.pair_id} "
                    f"in category {category}. This indicates Stage 1 (Primary Editing) failed to produce output."
                )
            if not pair.re_edit_instruction or len(pair.re_edit_instruction.strip()) == 0:
                raise RuntimeError(
                    f"Pipeline implementation error: re_edit_instruction is empty for pair {pair.pair_id} "
                    f"in category {category}. This indicates Stage 2 (MLLM Analysis) failed to produce valid output."
                )
        
        self.logger.info(f"Data integrity check passed: all {len(category_data.data_pairs)} pairs have valid primary_edited_image and re_edit_instruction")
        
        # åŠ è½½Refinement Diffusionæ¨¡å‹ï¼ˆå¦‚æœé…ç½®äº†ç‹¬ç«‹æ¨¡å‹ï¼‰æˆ–é‡ç”¨Primaryæ¨¡å‹
        if self.refinement_diffusion is not None or self.config["diffusion_model"].get("refinement"):
            self._ensure_model_loaded("refinement_diffusion")
            refinement_model = self.refinement_diffusion
        else:
            # å¦‚æœæ²¡æœ‰ç‹¬ç«‹çš„refinementæ¨¡å‹ï¼Œé‡ç”¨primaryæ¨¡å‹
            self._ensure_model_loaded("primary_diffusion")
            refinement_model = self.primary_diffusion
            self.logger.info("Using primary_diffusion for refinement (no separate refinement model configured)")
        
        # ===== åˆ¤æ–­æ˜¯å¦ä½¿ç”¨è¿­ä»£refinement =====
        use_iterative = self.enable_iterative_refinement
        
        if use_iterative:
            self.logger.info(f"\n{'='*80}")
            self.logger.info(f"[Iterative Refinement Mode] ENABLED")
            self.logger.info(f"{'='*80}")
        else:
            self.logger.info(f"\n{'='*80}")
            self.logger.info(f"[Single Refinement Mode] Using concatenated re-edit instructions")
            self.logger.info(f"{'='*80}")
        
        # ===== è¿­ä»£refinementé€»è¾‘ =====
        if use_iterative:
            # è¿­ä»£refinementæ¨¡å¼ï¼šå¤šGPUå¹¶è¡Œå¤„ç†ï¼Œæ¯ä¸ªGPUå†…éƒ¨ä¸²è¡Œè¿­ä»£
            self.logger.info(f"Processing {len(category_data.data_pairs)} pairs with iterative refinement (multi-GPU parallel)...")
            print(f"ğŸ”„ Processing {len(category_data.data_pairs)} pairs with iterative refinement (multi-GPU parallel)...")
            
            # ç»Ÿè®¡æ¯ä¸ªpairçš„è¿­ä»£æ¬¡æ•°å¹¶æ‰“å°
            total_iterations = 0
            for pair in category_data.data_pairs:
                if pair.re_edit_instructions:
                    iterations = len(pair.re_edit_instructions)
                else:
                    iterations = 1 if pair.re_edit_instruction else 0
                total_iterations += iterations
                pair_info = f"  ğŸ“Š Pair {pair.pair_id}: {iterations} iteration(s)"
                self.logger.info(pair_info)
                print(pair_info)
            
            summary_info = f"  ğŸ“ˆ Total: {len(category_data.data_pairs)} pairs, {total_iterations} total iterations"
            self.logger.info(summary_info)
            print(summary_info)
            
            # è·å–GPUè®¾å¤‡ä¿¡æ¯
            device_ids, num_gpus = self._get_gpu_info(refinement_model)
            gpu_info = f"Using {num_gpus} GPUs: {device_ids}"
            self.logger.info(gpu_info)
            print(f"ğŸ–¥ï¸  {gpu_info}")
            
            # ä»»åŠ¡åˆ†é…ï¼šå°†pairséšæœºåˆ†é…åˆ°ä¸åŒGPU
            gpu_tasks = self._assign_pairs_to_gpus(category_data.data_pairs, device_ids, num_gpus)
            
            # å¹¶è¡Œæ‰§è¡Œï¼šæ¯ä¸ªGPUå¤„ç†åˆ†é…ç»™å®ƒçš„pairs
            refined_images = self._parallel_iterative_refinement(
                refinement_model, gpu_tasks, device_ids, category, category_data
            )
            
            # ä¿å­˜å›¾åƒï¼ˆå¯é€‰ï¼‰
            if self.save_images:
                for pair, refined_image in zip(category_data.data_pairs, refined_images):
                    self._save_image(refined_image, category, pair.pair_id, "refined")
            
            # ç»Ÿè®¡ä¿¡æ¯
            total_iterations = sum(pair.refinement_iterations or 0 for pair in category_data.data_pairs)
            avg_iterations = total_iterations / len(category_data.data_pairs) if category_data.data_pairs else 0
            max_iterations = max((pair.refinement_iterations or 0 for pair in category_data.data_pairs), default=0)
            
            self.logger.info(f"\n{'='*80}")
            self.logger.info(f"[Iterative Refinement Summary]")
            self.logger.info(f"  Total pairs: {len(category_data.data_pairs)}")
            self.logger.info(f"  Total iterations: {total_iterations}")
            self.logger.info(f"  Average iterations per pair: {avg_iterations:.2f}")
            self.logger.info(f"  Max iterations: {max_iterations}")
            self.logger.info(f"{'='*80}")
        
        else:
            # ===== åŸæœ‰çš„ä¸€æ¬¡refinementé€»è¾‘ï¼ˆå‘åå…¼å®¹ï¼‰=====
            # å‡†å¤‡æ•°æ®ï¼ˆäºŒæ¬¡ç¼–è¾‘ä»¥åˆæ¬¡ç¼–è¾‘ç»“æœä¸ºåŸºç¡€ï¼‰
            base_images = [pair.primary_edited_image for pair in category_data.data_pairs]
            
            # ä»…ä½¿ç”¨ re_edit_instructionï¼ˆä¸æ‹¼æ¥CoTï¼‰
            re_edit_instructions = [pair.re_edit_instruction for pair in category_data.data_pairs]
            
            # æ‰¹é‡ç¼–è¾‘ï¼ˆä½¿ç”¨ä¸åˆæ¬¡ç¼–è¾‘ç›¸åŒçš„å¤šGPUå¹¶è¡Œæœºåˆ¶ï¼‰
            self.logger.info(f"Re-editing {len(base_images)} images with refinement model...")
            self.logger.info(f"  Using re-edit instructions only (CoT not included)")
            
            refined_images = []
            if hasattr(refinement_model, 'batch_edit'):
                # ä¼˜å…ˆä½¿ç”¨batch_editè¿›è¡Œå¤šGPUå¹¶è¡Œç¼–è¾‘
                refined_images = refinement_model.batch_edit(
                    images=base_images,
                    instructions=re_edit_instructions
                )
            else:
                # Fallback: ä¸²è¡Œç¼–è¾‘
                self.logger.warning("Refinement model does not support batch_edit, falling back to sequential processing")
                for i, (img, instr) in enumerate(zip(base_images, re_edit_instructions)):
                    self.logger.info(f"  Re-editing image {i+1}/{len(base_images)}")
                    refined = refinement_model.edit_image(img, instr)
                    refined_images.append(refined)
            
            # ä¿å­˜ç»“æœ
            for pair, refined_image in zip(category_data.data_pairs, refined_images):
                pair.refined_edited_image = refined_image
                pair.refinement_iterations = 1  # å•æ¬¡refinementè®°å½•ä¸º1æ¬¡è¿­ä»£
            
            # ä¿å­˜å›¾åƒï¼ˆå¯é€‰ï¼‰
            if self.save_images:
                for pair, refined_image in zip(category_data.data_pairs, refined_images):
                    self._save_image(refined_image, category, pair.pair_id, "refined")
        
        self.logger.info(f"Refinement editing completed for {len(refined_images)} images")
        
        # å¸è½½æ¨¡å‹
        self._unload_current_model()
    
    def _stage4_comparative_scoring(self, category: str, category_data: IterativeCategoryData):
        """
        é˜¶æ®µ4: è¯„åˆ†
        
        skip_refinement æ—¶ï¼šä»…å¯¹ primary è¯„åˆ†ï¼Œrefined_* ä¿æŒ None
        å¦åˆ™ï¼šæ ¹æ® enable_primary_scoring å†³å®šæ˜¯å¦è¯„åˆ† primaryï¼Œæ€»æ˜¯è¯„åˆ† refined
        """
        # ===== æ•°æ®å®Œæ•´æ€§éªŒè¯ =====
        if self.skip_refinement:
            # skip_refinement æ—¶åªæ£€æŸ¥ primary_edited_image
            for pair in category_data.data_pairs:
                if pair.primary_edited_image is None:
                    raise RuntimeError(
                        f"Pipeline implementation error: primary_edited_image is None for pair {pair.pair_id} "
                        f"in category {category}. skip_refinement mode requires valid primary images."
                    )
            self.logger.info(f"Data integrity check passed (skip_refinement): all {len(category_data.data_pairs)} pairs have valid primary images")
        else:
            # æ­£å¸¸æ¨¡å¼ï¼šæ£€æŸ¥ refinedï¼Œå¯é€‰æ£€æŸ¥ primary
            for pair in category_data.data_pairs:
                if pair.refined_edited_image is None:
                    raise RuntimeError(
                        f"Pipeline implementation error: refined_edited_image is None for pair {pair.pair_id} "
                        f"in category {category}. This indicates Stage 3 (Refinement Editing) failed to produce output."
                    )
                if self.enable_primary_scoring and pair.primary_edited_image is None:
                    raise RuntimeError(
                        f"Pipeline implementation error: primary_edited_image is None for pair {pair.pair_id} "
                        f"in category {category}. This indicates Stage 1 (Primary Editing) failed to produce output."
                    )
            if self.enable_primary_scoring:
                self.logger.info(f"Data integrity check passed: all {len(category_data.data_pairs)} pairs have valid primary and refined images")
            else:
                self.logger.info(f"Data integrity check passed: all {len(category_data.data_pairs)} pairs have valid refined images")
        
        # åŠ è½½Rewardæ¨¡å‹
        self._ensure_model_loaded("reward_model")
        
        # å‡†å¤‡é€šç”¨æ•°æ®
        original_images = [pair.original_image for pair in category_data.data_pairs]
        original_descriptions = [pair.original_description for pair in category_data.data_pairs]
        edit_instructions = [pair.edit_instruction for pair in category_data.data_pairs]
        rationales = [pair.rationale for pair in category_data.data_pairs]  # æ–°å¢ï¼šæå–rationale
        
        # æ‰“å°rationalesä¼ é€’æƒ…å†µï¼ˆç”¨äºè°ƒè¯•å’Œè®°å½•ï¼‰
        rationales_with_value = sum(1 for r in rationales if r and len(str(r).strip()) > 0)
        rationales_none_or_empty = len(rationales) - rationales_with_value
        self.logger.info(f"[Rationale Check] Total pairs: {len(rationales)}, "
                        f"With rationale: {rationales_with_value}, "
                        f"None/Empty: {rationales_none_or_empty}")
        if rationales_with_value > 0:
            # æ˜¾ç¤ºå‰3ä¸ªæœ‰å€¼çš„rationaleç¤ºä¾‹
            sample_rationales = [r for r in rationales if r and len(str(r).strip()) > 0][:3]
            for i, r in enumerate(sample_rationales, 1):
                self.logger.info(f"  Sample rationale {i}: {str(r)[:100]}...")
        
        # è·å–prompts
        system_prompts = [self.prompt_manager.get_system_prompt(category)] * len(category_data.data_pairs)
        user_prompts = [
            self.prompt_manager.get_user_prompt(category, pair.original_description, pair.edit_instruction)
            for pair in category_data.data_pairs
        ]
        
        # ===== ç±»åˆ«ç‰¹å®šè¯„ä»·ï¼šPrimaryç¼–è¾‘ç»“æœ =====
        # skip_refinement æ—¶ enable_primary_scoring æ’ä¸º Trueï¼Œåªè¯„ primary
        primary_images = [pair.primary_edited_image for pair in category_data.data_pairs]
        if self.enable_primary_scoring:
            self.logger.info(f"Scoring primary edited images...")
            
            primary_scores, primary_reasonings, primary_failures = self.reward_model.batch_score(
                edited_images=primary_images,
                original_descriptions=original_descriptions,
                edit_instructions=edit_instructions,
                system_prompts=system_prompts,
                user_prompts=user_prompts,
                original_images=original_images,
                rationales=rationales,
                return_failures=True
            )
            
            # è®°å½•primaryè¯„åˆ†å¤±è´¥ä¿¡æ¯
            self.scoring_health['primary_failures'][category] = primary_failures
            self.scoring_health['total_primary_samples'] += primary_failures['total_samples']
            self.scoring_health['total_primary_failures'] += primary_failures['failed_sample_count']
            
            # ä¿å­˜primaryåˆ†æ•°å’Œreasoning
            for pair, primary_score, primary_reasoning in zip(
                category_data.data_pairs, primary_scores, primary_reasonings
            ):
                pair.primary_score = primary_score  # "yes" or "no"
                pair.primary_score_reasoning = primary_reasoning  # reasoningæ–‡æœ¬
            
            # ç»Ÿè®¡primary yes/noç»“æœ
            primary_yes_count = sum(1 for s in primary_scores if s and isinstance(s, str) and s.lower() == "yes")
            primary_no_count = len(primary_scores) - primary_yes_count
            
            self.logger.info(f"Primary scoring completed for {len(primary_scores)} images")
            self.logger.info(f"  Primary: yes={primary_yes_count}, no={primary_no_count}")
        
        # ===== ç±»åˆ«ç‰¹å®šè¯„ä»·ï¼šRefinedç¼–è¾‘ç»“æœï¼ˆskip_refinement æ—¶è·³è¿‡ï¼‰=====
        if not self.skip_refinement:
            self.logger.info(f"Scoring refined edited images...")
            refined_images = [pair.refined_edited_image for pair in category_data.data_pairs]
            
            refined_scores, refined_reasonings, refined_failures = self.reward_model.batch_score(
                edited_images=refined_images,
                original_descriptions=original_descriptions,
                edit_instructions=edit_instructions,
                system_prompts=system_prompts,
                user_prompts=user_prompts,
                original_images=original_images,
                rationales=rationales,
                return_failures=True
            )
            
            self.scoring_health['refined_failures'][category] = refined_failures
            self.scoring_health['total_refined_samples'] += refined_failures['total_samples']
            self.scoring_health['total_refined_failures'] += refined_failures['failed_sample_count']
            
            for pair, refined_score, refined_reasoning in zip(
                category_data.data_pairs, refined_scores, refined_reasonings
            ):
                pair.refined_score = refined_score
                pair.refined_score_reasoning = refined_reasoning
            
            refined_yes_count = sum(1 for s in refined_scores if s and isinstance(s, str) and s.lower() == "yes")
            refined_no_count = len(refined_scores) - refined_yes_count
            self.logger.info(f"Refined scoring completed for {len(refined_scores)} images")
            self.logger.info(f"  Refined: yes={refined_yes_count}, no={refined_no_count}")
            
            # è®¡ç®—improvement rateï¼ˆä»…å½“æœ‰refinedæ—¶ï¼‰
            for pair in category_data.data_pairs:
                pair.calculate_improvement_rate()
        
        # ===== PQæŒ‡æ ‡è¯„ä»· =====
        if self.enable_pq_metric:
            self.logger.info(f"\n{'='*80}")
            self.logger.info(f"[PQ Metric Evaluation] Starting PQ metric evaluation for {category}")
            self.logger.info(f"{'='*80}")
            
            # ç¡®ä¿reward modelå·²åŠ è½½ï¼ˆå¦‚æœä¹‹å‰å¸è½½äº†ï¼‰
            self._ensure_model_loaded("reward_model")
            
            # PQæŒ‡æ ‡è¯„ä»· - Primary
            if self.enable_primary_scoring:
                self.logger.info(f"\n[PQ Metric] Scoring primary edited images...")
                primary_pq_scores, primary_pq_reasonings, primary_pq_failures = self._score_pq_metric(
                    edited_images=primary_images,
                    edit_instructions=edit_instructions,
                    original_images=original_images,
                    return_failures=True
                )
                for pair, primary_pq_score, primary_pq_reasoning in zip(
                    category_data.data_pairs, primary_pq_scores, primary_pq_reasonings
                ):
                    pair.primary_pq_score = primary_pq_score
                    pair.primary_pq_reasoning = primary_pq_reasoning
            
            # PQæŒ‡æ ‡è¯„ä»· - Refinedï¼ˆskip_refinement æ—¶è·³è¿‡ï¼‰
            if not self.skip_refinement:
                self.logger.info(f"\n[PQ Metric] Scoring refined edited images...")
                refined_pq_scores, refined_pq_reasonings, refined_pq_failures = self._score_pq_metric(
                    edited_images=[p.refined_edited_image for p in category_data.data_pairs],
                    edit_instructions=edit_instructions,
                    original_images=original_images,
                    return_failures=True
                )
                for pair, refined_pq_score, refined_pq_reasoning in zip(
                    category_data.data_pairs, refined_pq_scores, refined_pq_reasonings
                ):
                    pair.refined_pq_score = refined_pq_score
                    pair.refined_pq_reasoning = refined_pq_reasoning
            
            # è®¡ç®—PQæŒ‡æ ‡ç»Ÿè®¡
            if self.skip_refinement and primary_pq_scores:
                avg_primary_naturalness = sum(s[0] for s in primary_pq_scores) / len(primary_pq_scores)
                avg_primary_artifacts = sum(s[1] for s in primary_pq_scores) / len(primary_pq_scores)
                self.logger.info(f"\n[PQ Metric] Evaluation completed (skip_refinement)! Primary - Naturalness: {avg_primary_naturalness:.2f}, Artifacts: {avg_primary_artifacts:.2f}")
            elif self.enable_primary_scoring and primary_pq_scores and not self.skip_refinement:
                refined_pq_scores = [p.refined_pq_score for p in category_data.data_pairs if p.refined_pq_score is not None]
                if refined_pq_scores:
                    avg_primary_naturalness = sum(s[0] for s in primary_pq_scores) / len(primary_pq_scores)
                    avg_primary_artifacts = sum(s[1] for s in primary_pq_scores) / len(primary_pq_scores)
                    avg_refined_naturalness = sum(s[0] for s in refined_pq_scores) / len(refined_pq_scores)
                    avg_refined_artifacts = sum(s[1] for s in refined_pq_scores) / len(refined_pq_scores)
                    self.logger.info(f"\n[PQ Metric] Evaluation completed!")
                    self.logger.info(f"  Primary - Naturalness: {avg_primary_naturalness:.2f}, Artifacts: {avg_primary_artifacts:.2f}")
                    self.logger.info(f"  Refined - Naturalness: {avg_refined_naturalness:.2f}, Artifacts: {avg_refined_artifacts:.2f}")
                    self.logger.info(f"  Improvement - Naturalness: {avg_refined_naturalness - avg_primary_naturalness:+.2f}, Artifacts: {avg_refined_artifacts - avg_primary_artifacts:+.2f}")
            elif not self.skip_refinement:
                refined_pq_scores = [p.refined_pq_score for p in category_data.data_pairs if p.refined_pq_score is not None]
                if refined_pq_scores:
                    avg_refined_naturalness = sum(s[0] for s in refined_pq_scores) / len(refined_pq_scores)
                    avg_refined_artifacts = sum(s[1] for s in refined_pq_scores) / len(refined_pq_scores)
                    self.logger.info(f"\n[PQ Metric] Evaluation completed!")
                    self.logger.info(f"  Refined - Naturalness: {avg_refined_naturalness:.2f}, Artifacts: {avg_refined_artifacts:.2f}")
        else:
            self.logger.info(f"\n[PQ Metric] Skipped (enable_pq_metric=False)")
            # å°† PQ ç›¸å…³å­—æ®µè®¾ç½®ä¸º None
            for pair in category_data.data_pairs:
                if self.enable_primary_scoring:
                    pair.primary_pq_score = None
                    pair.primary_pq_reasoning = None
                pair.refined_pq_score = None
                pair.refined_pq_reasoning = None
        
        # ===== SCæŒ‡æ ‡è¯„ä»· =====
        if self.enable_sc_metric:
            self.logger.info(f"\n{'='*80}")
            self.logger.info(f"[SC Metric Evaluation] Starting SC metric evaluation for {category}")
            self.logger.info(f"{'='*80}")
            
            # ç¡®ä¿reward modelå·²åŠ è½½ï¼ˆå¦‚æœä¹‹å‰å¸è½½äº†ï¼‰
            self._ensure_model_loaded("reward_model")
            
            # SCæŒ‡æ ‡è¯„ä»· - Primary
            if self.enable_primary_scoring:
                self.logger.info(f"\n[SC Metric] Scoring primary edited images...")
                primary_sc_scores, primary_sc_reasonings, primary_sc_failures = self._score_sc_metric(
                    edited_images=primary_images,
                    edit_instructions=edit_instructions,
                    original_images=original_images,
                    return_failures=True
                )
                for pair, primary_sc_score, primary_sc_reasoning in zip(
                    category_data.data_pairs, primary_sc_scores, primary_sc_reasonings
                ):
                    pair.primary_sc_score = primary_sc_score
                    pair.primary_sc_reasoning = primary_sc_reasoning
            
            # SCæŒ‡æ ‡è¯„ä»· - Refinedï¼ˆskip_refinement æ—¶è·³è¿‡ï¼‰
            if not self.skip_refinement:
                self.logger.info(f"\n[SC Metric] Scoring refined edited images...")
                refined_sc_scores, refined_sc_reasonings, refined_sc_failures = self._score_sc_metric(
                    edited_images=[p.refined_edited_image for p in category_data.data_pairs],
                    edit_instructions=edit_instructions,
                    original_images=original_images,
                    return_failures=True
                )
                for pair, refined_sc_score, refined_sc_reasoning in zip(
                    category_data.data_pairs, refined_sc_scores, refined_sc_reasonings
                ):
                    pair.refined_sc_score = refined_sc_score
                    pair.refined_sc_reasoning = refined_sc_reasoning
            
            # è®¡ç®—SCæŒ‡æ ‡ç»Ÿè®¡
            if self.skip_refinement and primary_sc_scores:
                avg_primary_editing_success = sum(s[0] for s in primary_sc_scores) / len(primary_sc_scores)
                avg_primary_overediting = sum(s[1] for s in primary_sc_scores) / len(primary_sc_scores)
                self.logger.info(f"\n[SC Metric] Evaluation completed (skip_refinement)! Primary - Editing Success: {avg_primary_editing_success:.2f}, Degree of Overediting: {avg_primary_overediting:.2f}")
            elif self.enable_primary_scoring and primary_sc_scores and refined_sc_scores:
                avg_primary_editing_success = sum(s[0] for s in primary_sc_scores) / len(primary_sc_scores)
                avg_primary_overediting = sum(s[1] for s in primary_sc_scores) / len(primary_sc_scores)
                avg_refined_editing_success = sum(s[0] for s in refined_sc_scores) / len(refined_sc_scores)
                avg_refined_overediting = sum(s[1] for s in refined_sc_scores) / len(refined_sc_scores)
                
                self.logger.info(f"\n[SC Metric] Evaluation completed!")
                self.logger.info(f"  Primary - Editing Success: {avg_primary_editing_success:.2f}, Degree of Overediting: {avg_primary_overediting:.2f}")
                self.logger.info(f"  Refined - Editing Success: {avg_refined_editing_success:.2f}, Degree of Overediting: {avg_refined_overediting:.2f}")
                self.logger.info(f"  Improvement - Editing Success: {avg_refined_editing_success - avg_primary_editing_success:+.2f}, Degree of Overediting: {avg_refined_overediting - avg_primary_overediting:+.2f}")
            elif refined_sc_scores:
                avg_refined_editing_success = sum(s[0] for s in refined_sc_scores) / len(refined_sc_scores)
                avg_refined_overediting = sum(s[1] for s in refined_sc_scores) / len(refined_sc_scores)
                
                self.logger.info(f"\n[SC Metric] Evaluation completed!")
                self.logger.info(f"  Refined - Editing Success: {avg_refined_editing_success:.2f}, Degree of Overediting: {avg_refined_overediting:.2f}")
        else:
            self.logger.info(f"\n[SC Metric] Skipped (enable_sc_metric=False)")
            # å°† SC ç›¸å…³å­—æ®µè®¾ç½®ä¸º None
            for pair in category_data.data_pairs:
                if self.enable_primary_scoring:
                    pair.primary_sc_score = None
                    pair.primary_sc_reasoning = None
                pair.refined_sc_score = None
                pair.refined_sc_reasoning = None
        
        # ===== Instruction FollowingæŒ‡æ ‡è¯„ä»· =====
        if self.enable_instruction_following_metric:
            self.logger.info(f"\n{'='*80}")
            self.logger.info(f"[Instruction Following Metric] Starting IF metric evaluation for {category}")
            self.logger.info(f"{'='*80}")
            
            self._ensure_model_loaded("reward_model")
            
            # IFæŒ‡æ ‡è¯„ä»· - Primaryï¼ˆå¯é€‰ï¼‰
            if self.enable_primary_scoring:
                self.logger.info(f"\n[IF Metric] Scoring primary edited images...")
                primary_if_scores, primary_if_reasonings, _ = self._score_instruction_following_metric(
                    edited_images=primary_images,
                    edit_instructions=edit_instructions,
                    original_images=original_images,
                    return_failures=True
                )
                for pair, primary_if_score, primary_if_reasoning in zip(
                    category_data.data_pairs, primary_if_scores, primary_if_reasonings
                ):
                    pair.primary_if_score = primary_if_score
                    pair.primary_if_reasoning = primary_if_reasoning
            
            # IFæŒ‡æ ‡è¯„ä»· - Refinedï¼ˆskip_refinement æ—¶è·³è¿‡ï¼‰
            if not self.skip_refinement:
                self.logger.info(f"\n[IF Metric] Scoring refined edited images...")
                refined_if_scores, refined_if_reasonings, _ = self._score_instruction_following_metric(
                    edited_images=[p.refined_edited_image for p in category_data.data_pairs],
                    edit_instructions=edit_instructions,
                    original_images=original_images,
                    return_failures=True
                )
                for pair, refined_if_score, refined_if_reasoning in zip(
                    category_data.data_pairs, refined_if_scores, refined_if_reasonings
                ):
                    pair.refined_if_score = refined_if_score
                    pair.refined_if_reasoning = refined_if_reasoning
            
            if self.skip_refinement and primary_if_scores:
                avg_primary = sum(primary_if_scores) / len(primary_if_scores)
                self.logger.info(f"\n[IF Metric] Evaluation completed (skip_refinement)! Primary Avg: {avg_primary:.2f}")
            elif self.enable_primary_scoring and primary_if_scores and not self.skip_refinement:
                refined_if_scores = [p.refined_if_score for p in category_data.data_pairs if p.refined_if_score is not None]
                if refined_if_scores:
                    avg_primary = sum(primary_if_scores) / len(primary_if_scores)
                    avg_refined = sum(refined_if_scores) / len(refined_if_scores)
                    self.logger.info(f"\n[IF Metric] Evaluation completed!")
                    self.logger.info(f"  Primary Avg: {avg_primary:.2f}, Refined Avg: {avg_refined:.2f}, Improvement: {avg_refined - avg_primary:+.2f}")
            elif not self.skip_refinement:
                refined_if_scores = [p.refined_if_score for p in category_data.data_pairs if p.refined_if_score is not None]
                if refined_if_scores:
                    avg_refined = sum(refined_if_scores) / len(refined_if_scores)
                    self.logger.info(f"\n[IF Metric] Evaluation completed! Refined Avg: {avg_refined:.2f}")
        else:
            self.logger.info(f"\n[IF Metric] Skipped (enable_instruction_following_metric=False)")
            for pair in category_data.data_pairs:
                if self.enable_primary_scoring:
                    pair.primary_if_score = None
                    pair.primary_if_reasoning = None
                pair.refined_if_score = None
                pair.refined_if_reasoning = None
        
        # å¸è½½æ¨¡å‹
        self._unload_current_model()
    
    def _score_pq_metric(self, 
                         edited_images: List[Image.Image],
                         edit_instructions: List[str],
                         original_images: List[Image.Image],
                         return_failures: bool = False) -> tuple:
        """
        PQæŒ‡æ ‡è¯„ä»·è¾…åŠ©æ–¹æ³•
        
        ä½¿ç”¨PQç‰¹å®šçš„promptå¯¹æ‰€æœ‰å›¾åƒè¿›è¡Œè¯„ä»·ï¼Œä¸ä¾èµ–ç±»åˆ«ã€‚
        
        Args:
            edited_images: ç¼–è¾‘åçš„å›¾åƒåˆ—è¡¨
            edit_instructions: ç¼–è¾‘æŒ‡ä»¤åˆ—è¡¨
            original_images: åŸå§‹å›¾åƒåˆ—è¡¨
            return_failures: æ˜¯å¦è¿”å›å¤±è´¥ä¿¡æ¯
            
        Returns:
            å¦‚æœreturn_failures=False: Tuple[List[List[float]], List[str]]
                (PQåˆ†æ•°åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º[naturalness, artifacts], reasoningåˆ—è¡¨)
            å¦‚æœreturn_failures=True: Tuple[List[List[float]], List[str], Dict]
                (PQåˆ†æ•°åˆ—è¡¨, reasoningåˆ—è¡¨, å¤±è´¥ä¿¡æ¯)
        """
        n = len(edited_images)
        self.logger.info(f"PQ metric scoring {n} images...")
        
        # è·å–PQç‰¹å®šçš„prompts
        pq_system_prompt = self.prompt_manager.get_pq_system_prompt()
        pq_system_prompts = [pq_system_prompt] * n
        
        pq_user_prompts = [
            self.prompt_manager.get_pq_user_prompt(edit_instruction)
            for edit_instruction in edit_instructions
        ]
        
        # è°ƒç”¨reward modelè¿›è¡Œè¯„ä»·
        # æ³¨æ„ï¼šPQè¯„ä»·ä¸éœ€è¦original_descriptionså’Œrationales
        # batch_scoreè¿”å›çš„æ˜¯(scores, reasonings, failures)ï¼Œå…¶ä¸­scoreså¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨
        result = self.reward_model.batch_score(
            edited_images=edited_images,
            original_descriptions=[""] * n,  # å ä½ç¬¦ï¼ŒPQä¸éœ€è¦
            edit_instructions=edit_instructions,
            system_prompts=pq_system_prompts,
            user_prompts=pq_user_prompts,
            original_images=original_images,
            rationales=None,  # PQä¸éœ€è¦rationale
            return_failures=True
        )
        
        # å¤„ç†è¿”å›ç»“æœï¼šæ ¹æ®è¿”å›ç±»å‹åˆ¤æ–­
        if len(result) == 3:
            pq_scores, pq_reasonings, pq_failures = result
        else:
            # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰failuresï¼Œåˆ›å»ºç©ºçš„failureså­—å…¸
            pq_scores, pq_reasonings = result
            pq_failures = {
                'failed_gpus': [],
                'failed_sample_indices': [],
                'failed_sample_count': 0,
                'total_samples': n,
                'error_messages': {}
            }
        
        # å¤„ç†è¿”å›ç»“æœï¼špq_scoreså¯èƒ½æ˜¯åˆ—è¡¨æ ¼å¼[naturalness, artifacts]æˆ–å­—ç¬¦ä¸²"yes"/"no"
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºé»˜è®¤å€¼
        processed_scores = []
        for i, score in enumerate(pq_scores):
            if isinstance(score, list) and len(score) >= 2:
                # å·²ç»æ˜¯åˆ—è¡¨æ ¼å¼
                try:
                    processed_scores.append([float(score[0]), float(score[1])])
                except (ValueError, TypeError, IndexError) as e:
                    self.logger.warning(f"PQ score format error at index {i}: {score}, error: {e}, using default [5.0, 5.0]")
                    processed_scores.append([5.0, 5.0])
            else:
                # å¦‚æœä¸æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆå¯èƒ½æ˜¯è§£æå¤±è´¥ï¼‰
                self.logger.warning(f"PQ score is not in list format at index {i}: {score} (type: {type(score)}), using default [5.0, 5.0]")
                processed_scores.append([5.0, 5.0])
        
        self.logger.info(f"PQ metric scoring completed for {n} images")
        
        if return_failures:
            return processed_scores, pq_reasonings, pq_failures
        else:
            return processed_scores, pq_reasonings
    
    def _score_sc_metric(self, 
                         edited_images: List[Image.Image],
                         edit_instructions: List[str],
                         original_images: List[Image.Image],
                         return_failures: bool = False) -> tuple:
        """
        SCæŒ‡æ ‡è¯„ä»·è¾…åŠ©æ–¹æ³•
        
        ä½¿ç”¨SCç‰¹å®šçš„promptå¯¹æ‰€æœ‰å›¾åƒè¿›è¡Œè¯„ä»·ï¼Œä¸ä¾èµ–ç±»åˆ«ã€‚
        
        Args:
            edited_images: ç¼–è¾‘åçš„å›¾åƒåˆ—è¡¨
            edit_instructions: ç¼–è¾‘æŒ‡ä»¤åˆ—è¡¨
            original_images: åŸå§‹å›¾åƒåˆ—è¡¨
            return_failures: æ˜¯å¦è¿”å›å¤±è´¥ä¿¡æ¯
            
        Returns:
            å¦‚æœreturn_failures=False: Tuple[List[List[float]], List[str]]
                (SCåˆ†æ•°åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º[editing_success, degree_of_overediting], reasoningåˆ—è¡¨)
            å¦‚æœreturn_failures=True: Tuple[List[List[float]], List[str], Dict]
                (SCåˆ†æ•°åˆ—è¡¨, reasoningåˆ—è¡¨, å¤±è´¥ä¿¡æ¯)
        """
        n = len(edited_images)
        self.logger.info(f"SC metric scoring {n} images...")
        
        # è·å–SCç‰¹å®šçš„prompts
        sc_system_prompt = self.prompt_manager.get_sc_system_prompt()
        sc_system_prompts = [sc_system_prompt] * n
        
        sc_user_prompts = [
            self.prompt_manager.get_sc_user_prompt(edit_instruction)
            for edit_instruction in edit_instructions
        ]
        
        # è°ƒç”¨reward modelè¿›è¡Œè¯„ä»·
        # æ³¨æ„ï¼šSCè¯„ä»·ä¸éœ€è¦original_descriptionså’Œrationales
        result = self.reward_model.batch_score(
            edited_images=edited_images,
            original_descriptions=[""] * n,  # å ä½ç¬¦ï¼ŒSCä¸éœ€è¦
            edit_instructions=edit_instructions,
            system_prompts=sc_system_prompts,
            user_prompts=sc_user_prompts,
            original_images=original_images,
            rationales=None,  # SCä¸éœ€è¦rationale
            return_failures=True
        )
        
        # å¤„ç†è¿”å›ç»“æœï¼šæ ¹æ®è¿”å›ç±»å‹åˆ¤æ–­
        if len(result) == 3:
            sc_scores, sc_reasonings, sc_failures = result
        else:
            # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰failuresï¼Œåˆ›å»ºç©ºçš„failureså­—å…¸
            sc_scores, sc_reasonings = result
            sc_failures = {
                'failed_gpus': [],
                'failed_sample_indices': [],
                'failed_sample_count': 0,
                'total_samples': n,
                'error_messages': {}
            }
        
        # å¤„ç†è¿”å›ç»“æœï¼šsc_scoreså¯èƒ½æ˜¯åˆ—è¡¨æ ¼å¼[editing_success, degree_of_overediting]æˆ–å­—ç¬¦ä¸²"yes"/"no"
        # å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œè½¬æ¢ä¸ºé»˜è®¤å€¼
        processed_scores = []
        for i, score in enumerate(sc_scores):
            if isinstance(score, list) and len(score) >= 2:
                # å·²ç»æ˜¯åˆ—è¡¨æ ¼å¼
                try:
                    processed_scores.append([float(score[0]), float(score[1])])
                except (ValueError, TypeError, IndexError) as e:
                    self.logger.warning(f"SC score format error at index {i}: {score}, error: {e}, using default [5.0, 5.0]")
                    processed_scores.append([5.0, 5.0])
            else:
                # å¦‚æœä¸æ˜¯åˆ—è¡¨æ ¼å¼ï¼Œä½¿ç”¨é»˜è®¤å€¼ï¼ˆå¯èƒ½æ˜¯è§£æå¤±è´¥ï¼‰
                self.logger.warning(f"SC score is not in list format at index {i}: {score} (type: {type(score)}), using default [5.0, 5.0]")
                processed_scores.append([5.0, 5.0])
        
        self.logger.info(f"SC metric scoring completed for {n} images")
        
        if return_failures:
            return processed_scores, sc_reasonings, sc_failures
        else:
            return processed_scores, sc_reasonings
    
    def _score_instruction_following_metric(self, 
                                            edited_images: List[Image.Image],
                                            edit_instructions: List[str],
                                            original_images: List[Image.Image],
                                            return_failures: bool = False) -> tuple:
        """
        Instruction FollowingæŒ‡æ ‡è¯„ä»·è¾…åŠ©æ–¹æ³•
        
        è¯„ä¼°ç¼–è¾‘ç»“æœå¯¹æŒ‡ä»¤çš„éµå¾ªç¨‹åº¦ï¼Œè¾“å‡ºå•åˆ†0-10ã€‚
        
        Returns:
            å¦‚æœreturn_failures=False: Tuple[List[float], List[str]]
            å¦‚æœreturn_failures=True: Tuple[List[float], List[str], Dict]
        """
        n = len(edited_images)
        self.logger.info(f"Instruction Following metric scoring {n} images...")
        
        if_system_prompt = self.prompt_manager.get_instruction_following_system_prompt()
        if_system_prompts = [if_system_prompt] * n
        if_user_prompts = [
            self.prompt_manager.get_instruction_following_user_prompt(edit_instruction)
            for edit_instruction in edit_instructions
        ]
        
        result = self.reward_model.batch_score(
            edited_images=edited_images,
            original_descriptions=[""] * n,
            edit_instructions=edit_instructions,
            system_prompts=if_system_prompts,
            user_prompts=if_user_prompts,
            original_images=original_images,
            rationales=None,
            return_failures=True
        )
        
        if len(result) == 3:
            if_scores, if_reasonings, if_failures = result
        else:
            if_scores, if_reasonings = result
            if_failures = {
                'failed_gpus': [], 'failed_sample_indices': [], 'failed_sample_count': 0,
                'total_samples': n, 'error_messages': {}
            }
        
        # å¤„ç†è¿”å›ï¼šIFä¸ºå•åˆ†0-10ï¼Œå¯èƒ½ä¸ºint/floatæˆ–é”™è¯¯è§£æä¸ºlist
        processed_scores = []
        for i, score in enumerate(if_scores):
            if isinstance(score, (int, float)):
                processed_scores.append(float(score))
            elif isinstance(score, list) and len(score) >= 1:
                try:
                    processed_scores.append(float(score[0]))
                except (ValueError, TypeError):
                    self.logger.warning(f"IF score format error at index {i}: {score}, using default 5.0")
                    processed_scores.append(5.0)
            else:
                self.logger.warning(f"IF score invalid at index {i}: {score} (type: {type(score)}), using default 5.0")
                processed_scores.append(5.0)
        
        self.logger.info(f"Instruction Following metric scoring completed for {n} images")
        
        if return_failures:
            return processed_scores, if_reasonings, if_failures
        return processed_scores, if_reasonings
    
    def _stage5_statistics(self, category: str, category_data: IterativeCategoryData):
        """
        ç»Ÿè®¡è®°å½•
        
        è®¡ç®—ç±»åˆ«ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ ¹æ®enable_primary_scoringå†³å®šæ˜¯å¦åŒ…å«primaryç»Ÿè®¡ï¼‰
        """
        category_data.calculate_statistics()
        
        stats = category_data.statistics
        self.logger.info(f"\nCategory: {category}")
        
        if self.skip_refinement:
            # skip_refinement æ¨¡å¼ï¼šä»…è¾“å‡º primary ç»Ÿè®¡
            self.logger.info(f"  Primary Yes Rate: {stats.get('primary_yes_rate', 0.0):.2f}% ({stats.get('primary_yes_count', 0)}/{stats.get('total_count', 0)})")
        elif self.enable_primary_scoring:
            # Primary + Refined ç»Ÿè®¡
            self.logger.info(f"  Primary Yes Rate: {stats.get('primary_yes_rate', 0.0):.2f}% ({stats.get('primary_yes_count', 0)}/{stats.get('total_count', 0)})")
            self.logger.info(f"  Refined Yes Rate: {stats.get('refined_yes_rate', 0.0):.2f}% ({stats.get('refined_yes_count', 0)}/{stats.get('total_count', 0)})")
            self.logger.info(f"  Improvement Rate: {stats.get('improvement_rate', 0.0):.2f}% ({stats.get('improved_count', 0)} improved)")
            self.logger.info(f"  Maintained Rate: {stats.get('maintained_rate', 0.0):.2f}% ({stats.get('maintained_count', 0)} maintained)")
            self.logger.info(f"  Regression Rate: {stats.get('regression_rate', 0.0):.2f}% ({stats.get('regression_count', 0)} regressed)")
            self.logger.info(f"  Unchanged Rate: {stats.get('unchanged_rate', 0.0):.2f}% ({stats.get('unchanged_count', 0)} unchanged)")
        else:
            self.logger.info(f"  Refined Yes Rate: {stats.get('refined_yes_rate', 0.0):.2f}% ({stats.get('refined_yes_count', 0)}/{stats.get('total_count', 0)})")
            self.logger.info(f"  Refined Avg (Yes Rate): {stats.get('refined_avg', 0.0):.2f}%")
    
    def _get_gpu_info(self, refinement_model) -> tuple:
        """
        è·å–refinement_modelçš„GPUè®¾å¤‡ä¿¡æ¯
        
        Args:
            refinement_model: refinement diffusionæ¨¡å‹å®ä¾‹
            
        Returns:
            (device_ids, num_gpus) å…ƒç»„
        """
        # å°è¯•ä»æ¨¡å‹è·å–device_ids
        if hasattr(refinement_model, 'device_ids'):
            device_ids = refinement_model.device_ids
            num_gpus = len(device_ids)
        elif hasattr(refinement_model, 'num_gpus'):
            num_gpus = refinement_model.num_gpus
            if hasattr(refinement_model, 'config') and refinement_model.config:
                device_ids = refinement_model.config.get("device_ids", list(range(num_gpus)))
            else:
                device_ids = list(range(num_gpus))
        else:
            # ä»é…ç½®ä¸­è¯»å–
            refinement_config = self.config["diffusion_model"].get("refinement") or self.config["diffusion_model"].get("primary")
            if refinement_config and "params" in refinement_config:
                device_ids = refinement_config["params"].get("device_ids", [0])
                num_gpus = len(device_ids) if isinstance(device_ids, list) else 1
                if not isinstance(device_ids, list):
                    device_ids = [device_ids]
            else:
                # é»˜è®¤ä½¿ç”¨å•ä¸ªGPU
                device_ids = [0]
                num_gpus = 1
        
        return device_ids, num_gpus
    
    def _assign_pairs_to_gpus(self, pairs: List[IterativeDataPair], device_ids: List[int], num_gpus: int) -> Dict[int, List[IterativeDataPair]]:
        """
        å°†pairséšæœºåˆ†é…åˆ°ä¸åŒçš„GPU
        
        Args:
            pairs: æ•°æ®å¯¹åˆ—è¡¨
            device_ids: GPUè®¾å¤‡IDåˆ—è¡¨
            num_gpus: GPUæ•°é‡
            
        Returns:
            {gpu_id: [list of pairs]} å­—å…¸
        """
        # åˆ›å»ºpairsçš„å‰¯æœ¬å¹¶éšæœºæ‰“ä¹±
        pairs_copy = list(pairs)
        random.shuffle(pairs_copy)
        
        # æŒ‰round-robinæ–¹å¼åˆ†é…åˆ°å„GPU
        gpu_tasks = {gpu_id: [] for gpu_id in device_ids}
        for idx, pair in enumerate(pairs_copy):
            gpu_idx = idx % num_gpus
            gpu_id = device_ids[gpu_idx]
            gpu_tasks[gpu_id].append(pair)
        
        # è®°å½•åˆ†é…æƒ…å†µ
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"[Task Assignment] Randomly assigned {len(pairs)} pairs to {num_gpus} GPUs:")
        self.logger.info(f"{'='*80}")
        for gpu_id in device_ids:
            num_tasks = len(gpu_tasks[gpu_id])
            self.logger.info(f"  GPU {gpu_id}: {num_tasks} pairs")
        self.logger.info(f"{'='*80}\n")
        
        return gpu_tasks
    
    def _process_pairs_on_gpu(self, refinement_model, gpu_id: int, pairs: List[IterativeDataPair], category: str) -> List[Dict]:
        """
        åœ¨æŒ‡å®šGPUä¸Šå¤„ç†åˆ†é…ç»™å®ƒçš„pairsï¼Œè¿›è¡Œä¸²è¡Œè¿­ä»£refinement
        
        Args:
            refinement_model: refinement diffusionæ¨¡å‹å®ä¾‹
            gpu_id: GPUè®¾å¤‡ID
            pairs: åˆ†é…ç»™è¯¥GPUçš„pairsåˆ—è¡¨
            category: ç±»åˆ«åç§°
            
        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º {"pair": pair, "refined_image": Image, "success": bool}
        """
        results = []
        
        self.logger.info(f"[GPU {gpu_id}] Starting processing {len(pairs)} pairs...")
        
        for pair_idx, pair in enumerate(pairs, 1):
            # è·å–è¯¥pairçš„æŒ‡ä»¤åˆ—è¡¨
            if pair.re_edit_instructions and len(pair.re_edit_instructions) > 0:
                instructions_list = pair.re_edit_instructions
            else:
                # å¦‚æœæ²¡æœ‰ç‹¬ç«‹çš„æŒ‡ä»¤åˆ—è¡¨ï¼Œfallbackåˆ°ä½¿ç”¨æ‹¼æ¥åçš„æŒ‡ä»¤ï¼ˆåŒ…è£…ä¸ºåˆ—è¡¨ï¼‰
                instructions_list = [pair.re_edit_instruction] if pair.re_edit_instruction else []
            
            num_iterations = len(instructions_list)
            pair.refinement_iterations = num_iterations
            
            if num_iterations == 0:
                self.logger.warning(f"[GPU {gpu_id}] Pair {pair.pair_id}: No re-edit instructions found, using primary image as refined image")
                results.append({
                    "pair": pair,
                    "refined_image": pair.primary_edited_image,
                    "success": True
                })
                continue
            
            # è®°å½•è¿­ä»£ä¿¡æ¯ï¼ˆåŒæ—¶è¾“å‡ºåˆ°æ—¥å¿—å’Œæ§åˆ¶å°ï¼‰
            iteration_info = f"[GPU {gpu_id}] [Pair {pair.pair_id} ({pair_idx}/{len(pairs)})] Starting iterative refinement with {num_iterations} iteration(s)"
            self.logger.info(iteration_info)
            print(f"ğŸ”„ {iteration_info}")
            
            # æ‰“å°è¿­ä»£æŒ‡ä»¤è¯¦æƒ…
            if instructions_list:
                self.logger.info(f"[GPU {gpu_id}]   Iteration instructions for {pair.pair_id}:")
                print(f"ğŸ“‹ [GPU {gpu_id}] Iteration instructions for {pair.pair_id}:")
                for idx, instr in enumerate(instructions_list, 1):
                    instr_msg = f"[GPU {gpu_id}]     [{idx}/{num_iterations}] {instr}"
                    self.logger.info(instr_msg)
                    print(f"   {instr_msg}")
            
            # åˆå§‹åŒ–ï¼šç¬¬ä¸€è½®ä½¿ç”¨ primary_edited_image ä½œä¸ºè¾“å…¥
            current_image = pair.primary_edited_image
            success = True
            
            # è¿­ä»£refinementï¼šæ¯æ¬¡ä½¿ç”¨ä¸€ä¸ªæŒ‡ä»¤ï¼Œç»“æœä½œä¸ºä¸‹ä¸€è½®çš„è¾“å…¥
            for iter_idx, instruction in enumerate(instructions_list, 1):
                iteration_msg = f"[GPU {gpu_id}]   [Iteration {iter_idx}/{num_iterations}] Pair {pair.pair_id}: {instruction}"
                self.logger.info(iteration_msg)
                print(f"âš™ï¸  {iteration_msg}")
                
                try:
                    # æ‰§è¡Œå•æ¬¡refinementï¼ˆä¸ç°æœ‰diffusionæ–¹å¼é€‚é…ï¼‰
                    # åœ¨è¿­ä»£refinementæ¨¡å¼ä¸‹ï¼Œç›´æ¥ä½¿ç”¨æŒ‡å®šGPUï¼Œé¿å…batch_syncå¯¼è‡´çš„åŒæ­¥ç­‰å¾…
                    if hasattr(refinement_model, 'edit_image'):
                        # å¦‚æœæ¨¡å‹æ”¯æŒtarget_gpu_idå‚æ•°ï¼Œç›´æ¥æŒ‡å®šGPUï¼ˆé¿å…batch_syncï¼‰
                        # å¦åˆ™ä½¿ç”¨é»˜è®¤è¡Œä¸ºï¼ˆå¯èƒ½è§¦å‘batch_syncï¼Œä½†è‡³å°‘èƒ½å·¥ä½œï¼‰
                        if hasattr(refinement_model, 'device_ids') and gpu_id in refinement_model.device_ids:
                            # ç›´æ¥æŒ‡å®šç›®æ ‡GPUï¼Œç»•è¿‡batch_syncæœºåˆ¶
                            refined_image = refinement_model.edit_image(
                                current_image, 
                                instruction,
                                target_gpu_id=gpu_id  # æŒ‡å®šGPUï¼Œé¿å…è½®è¯¢åˆ†é…å’ŒåŒæ­¥ç­‰å¾…
                            )
                        else:
                            # Fallbackï¼šä½¿ç”¨é»˜è®¤è¡Œä¸ºï¼ˆå¯èƒ½ä¸æ˜¯æœ€ä¼˜ï¼Œä½†è‡³å°‘èƒ½å·¥ä½œï¼‰
                            refined_image = refinement_model.edit_image(
                                current_image, 
                                instruction,
                                enable_batch_sync=False  # ç¦ç”¨åŒæ­¥ï¼Œè®©å„GPUç‹¬ç«‹å¤„ç†
                            )
                    elif hasattr(refinement_model, 'batch_edit'):
                        # å¦‚æœæ¨¡å‹ä¸æ”¯æŒå•å›¾ç¼–è¾‘ï¼Œä½¿ç”¨batch_editï¼ˆä¼ å…¥å•ä¸ªå…ƒç´ ï¼‰
                        refined_results = refinement_model.batch_edit(
                            images=[current_image],
                            instructions=[instruction],
                            enable_batch_sync=False  # ç¦ç”¨åŒæ­¥ï¼Œè®©å„GPUç‹¬ç«‹å¤„ç†
                        )
                        refined_image = refined_results[0] if refined_results else current_image
                    else:
                        raise RuntimeError(f"Refinement model does not support edit_image or batch_edit")
                    
                    # å…³é”®ï¼šå°†æœ¬æ¬¡refinementçš„ç»“æœä½œä¸ºä¸‹ä¸€è½®çš„è¾“å…¥
                    current_image = refined_image
                    
                    # ä¿å­˜è¿­ä»£ä¸­é—´ç»“æœï¼ˆè¿‡ç¨‹ç»“æœï¼‰
                    if self.save_images:
                        self._save_iteration_image(
                            refined_image, 
                            category, 
                            pair.pair_id, 
                            iter_idx, 
                            num_iterations
                        )
                    
                except Exception as e:
                    self.logger.error(f"[GPU {gpu_id}]   âœ— Iteration {iter_idx} failed for pair {pair.pair_id}: {e}")
                    self.logger.warning(f"[GPU {gpu_id}]   Using previous iteration result as fallback")
                    success = False
                    # å¦‚æœæŸæ¬¡è¿­ä»£å¤±è´¥ï¼Œä½¿ç”¨ä¸Šä¸€è½®çš„ç»“æœï¼ˆæˆ–primary imageï¼‰
                    if iter_idx == 1:
                        current_image = pair.primary_edited_image
                    # å¦åˆ™ current_image ä¿æŒä¸ºä¸Šä¸€è½®çš„ç»“æœ
            
            # ä¿å­˜æœ€ç»ˆç»“æœï¼ˆæœ€åä¸€è½®refinementçš„ç»“æœï¼‰
            pair.refined_edited_image = current_image
            results.append({
                "pair": pair,
                "refined_image": current_image,
                "success": success
            })
            
            completion_msg = f"[GPU {gpu_id}] [Pair {pair.pair_id}] Iterative refinement completed: {num_iterations} iteration(s)"
            self.logger.info(completion_msg)
            print(f"âœ… {completion_msg}")
        
        summary_msg = f"[GPU {gpu_id}] âœ… Completed processing {len(pairs)} pairs"
        self.logger.info(summary_msg)
        print(f"ğŸ‰ {summary_msg}")
        return results
    
    def _parallel_iterative_refinement(self, refinement_model, gpu_tasks: Dict[int, List[IterativeDataPair]], 
                                       device_ids: List[int], category: str, category_data: IterativeCategoryData) -> List[Image.Image]:
        """
        å¹¶è¡Œæ‰§è¡Œè¿­ä»£refinementï¼Œç­‰å¾…æ‰€æœ‰GPUå®Œæˆ
        
        Args:
            refinement_model: refinement diffusionæ¨¡å‹å®ä¾‹
            gpu_tasks: {gpu_id: [list of pairs]} ä»»åŠ¡åˆ†é…å­—å…¸
            device_ids: GPUè®¾å¤‡IDåˆ—è¡¨
            category: ç±»åˆ«åç§°
            category_data: ç±»åˆ«æ•°æ®ï¼ˆç”¨äºä¿æŒåŸå§‹é¡ºåºï¼‰
            
        Returns:
            æŒ‰åŸå§‹é¡ºåºæ’åˆ—çš„refined imagesåˆ—è¡¨
        """
        num_gpus = len(device_ids)
        refined_images = [None] * len(category_data.data_pairs)
        
        # åˆ›å»ºpair_idåˆ°ç´¢å¼•çš„æ˜ å°„ï¼ˆç”¨äºä¿æŒåŸå§‹é¡ºåºï¼‰
        pair_id_to_index = {pair.pair_id: idx for idx, pair in enumerate(category_data.data_pairs)}
        
        # ä½¿ç”¨ThreadPoolExecutorå¹¶è¡Œæ‰§è¡Œ
        with ThreadPoolExecutor(max_workers=num_gpus) as executor:
            futures = {}
            for gpu_id in device_ids:
                if gpu_tasks[gpu_id]:  # åªæäº¤æœ‰ä»»åŠ¡çš„GPU
                    future = executor.submit(
                        self._process_pairs_on_gpu,
                        refinement_model,
                        gpu_id,
                        gpu_tasks[gpu_id],
                        category
                    )
                    futures[future] = gpu_id
            
            # ç­‰å¾…æ‰€æœ‰GPUå®Œæˆå¹¶æ”¶é›†ç»“æœ
            self.logger.info(f"\n{'='*80}")
            self.logger.info(f"[Synchronization] Waiting for all {len(futures)} GPUs to complete...")
            self.logger.info(f"{'='*80}")
            
            completed_count = 0
            for future in as_completed(futures):
                gpu_id = futures[future]
                completed_count += 1
                try:
                    gpu_results = future.result()
                    self.logger.info(f"[GPU {gpu_id}] âœ… Completed ({completed_count}/{len(futures)} GPUs done)")
                    
                    # å°†ç»“æœæŒ‰åŸå§‹é¡ºåºç»„è£…
                    for result in gpu_results:
                        pair = result["pair"]
                        refined_image = result["refined_image"]
                        pair_idx = pair_id_to_index[pair.pair_id]
                        refined_images[pair_idx] = refined_image
                        
                        if not result["success"]:
                            self.logger.warning(f"[GPU {gpu_id}] Pair {pair.pair_id} had some failed iterations, using fallback result")
                    
                except Exception as e:
                    self.logger.error(f"[GPU {gpu_id}] âŒ Failed: {e}")
                    import traceback
                    traceback.print_exc()
                    # å¯¹äºå¤±è´¥çš„GPUï¼Œä½¿ç”¨primary imageä½œä¸ºfallback
                    for pair in gpu_tasks[gpu_id]:
                        pair_idx = pair_id_to_index[pair.pair_id]
                        if refined_images[pair_idx] is None:
                            refined_images[pair_idx] = pair.primary_edited_image
                            pair.refined_edited_image = pair.primary_edited_image
                            self.logger.warning(f"[GPU {gpu_id}] Using primary image as fallback for pair {pair.pair_id}")
            
            self.logger.info(f"{'='*80}")
            self.logger.info(f"[Synchronization] All {len(futures)} GPUs completed!")
            self.logger.info(f"{'='*80}\n")
        
        # éªŒè¯æ‰€æœ‰ç»“æœéƒ½å·²æ”¶é›†
        missing_count = sum(1 for img in refined_images if img is None)
        if missing_count > 0:
            self.logger.warning(f"Warning: {missing_count} pairs have None refined images, using primary images as fallback")
            for idx, img in enumerate(refined_images):
                if img is None:
                    pair = category_data.data_pairs[idx]
                    refined_images[idx] = pair.primary_edited_image
                    pair.refined_edited_image = pair.primary_edited_image
        
        return refined_images
    
    def _get_category_dir(self, category: str) -> str:
        """
        è·å– category å¯¹åº”çš„ç›®å½•åï¼ˆç”¨äºå›¾ç‰‡ä¿å­˜/åŠ è½½ï¼‰
        æ”¯æŒ category_to_dir æ˜ å°„ä»¥å…¼å®¹ä¸­æ–‡ç›®å½•åï¼ˆå¦‚ä» primary_images_dir åŠ è½½æ—¶ï¼‰
        """
        if self.category_to_dir and category in self.category_to_dir:
            return self.category_to_dir[category]
        return category
    
    # å°† pair_id/æ–‡ä»¶åä¸­å¯èƒ½åŒ…å«çš„ä¸­æ–‡ subset æ›¿æ¢ä¸ºè‹±æ–‡ï¼ˆç”¨äºä¿å­˜æ—¶ç”Ÿæˆç»Ÿä¸€è‹±æ–‡å­¦åï¼‰
    _SUBSET_ZH_TO_EN = {"ç‰©ç†": "physical", "ç¯å¢ƒ": "environmental", "ç¤¾ä¼š": "cultural", "å› æœ": "causal", "æŒ‡ä»£": "referential"}
    
    def _sanitize_filename_part(self, s: str) -> str:
        """å°† pair_id ä¸­çš„ä¸­æ–‡ subset æ›¿æ¢ä¸ºè‹±æ–‡ï¼Œé¿å…ä¿å­˜çš„æ–‡ä»¶ååŒ…å«ä¸­æ–‡"""
        if not s:
            return s
        result = s
        for zh, en in self._SUBSET_ZH_TO_EN.items():
            result = result.replace(zh, en)
        return result
    
    def _save_image(self, image: Image.Image, category: str, pair_id: str, stage: str):
        """
        ä¿å­˜å›¾åƒ
        
        Args:
            image: å›¾åƒå¯¹è±¡
            category: ç±»åˆ«åç§°ï¼ˆæ¥è‡ª config.data.categoriesï¼Œè‹±æ–‡ subset æ—¶ä¸º physical/environmental ç­‰ï¼‰
            pair_id: æ•°æ®å¯¹ID
            stage: é˜¶æ®µæ ‡è¯†ï¼ˆprimary/refinedï¼‰
        """
        dir_name = self._get_category_dir(category)
        category_dir = os.path.join(self.output_dir, dir_name)
        os.makedirs(category_dir, exist_ok=True)
        
        # å°† pair_id ä¸­çš„ä¸­æ–‡ subset æ›¿æ¢ä¸ºè‹±æ–‡ï¼Œé¿å…ä¿å­˜çš„æ–‡ä»¶ååŒ…å«ä¸­æ–‡ï¼ˆå¦‚ 00008_ç‰©ç†_medium -> 00008_physical_mediumï¼‰
        safe_pair_id = self._sanitize_filename_part(pair_id)
        
        # å¦‚æœé…ç½®äº† primary_images_dir ä¸”ä¿å­˜çš„æ˜¯ refined å›¾ç‰‡ï¼Œåœ¨æ–‡ä»¶åä¸­æ·»åŠ æ ‡è¯†ä»¥é¿å…è¦†ç›–
        # å¦‚æœä½¿ç”¨äº† use_original_as_primaryï¼Œä¹Ÿè¦åœ¨æ–‡ä»¶åä¸­æ·»åŠ æ ‡è¯†
        # æ³¨æ„ï¼šå¦‚æœ use_original_as_primary=Trueï¼Œä¸åº”è¯¥åŒ…å« primary_images_dir çš„æ ‡è¯†ï¼ˆå› ä¸ºå®é™…æœªä½¿ç”¨ï¼‰
        if stage == "refined":
            parts = []
            if self.use_original_as_primary:
                parts.append("use_original_as_primary")
                # å¦‚æœä½¿ç”¨åŸå›¾ä½œä¸º primaryï¼Œä¸åº”è¯¥åŒ…å« primary_images_dir çš„æ ‡è¯†
            elif self.primary_images_dir:
                # åªæœ‰åœ¨ä¸ä½¿ç”¨åŸå›¾ä½œä¸º primary æ—¶ï¼Œæ‰æ·»åŠ  primary_images_dir æ ‡è¯†
                primary_dir_name = os.path.basename(os.path.normpath(self.primary_images_dir))
                parts.append(f"refined_only_{primary_dir_name}_under_qwen_image_edit_small_data_sft_mllm_ablation")
            if self.enable_iterative_refinement:
                parts.append("iterative_refinement")
            
            if parts:
                filename = f"{safe_pair_id}_{'_'.join(parts)}.png"
            else:
                filename = f"{safe_pair_id}_{stage}.png"
        else:
            filename = f"{safe_pair_id}_{stage}.png"
        
        filepath = os.path.join(category_dir, filename)
        image.save(filepath)
    
    def _save_iteration_image(self, image: Image.Image, category: str, pair_id: str, 
                              iteration_idx: int, total_iterations: int):
        """
        ä¿å­˜è¿­ä»£refinementçš„ä¸­é—´ç»“æœï¼ˆè¿‡ç¨‹ç»“æœï¼‰
        
        Args:
            image: å›¾åƒå¯¹è±¡
            category: ç±»åˆ«åç§°ï¼ˆæ¥è‡ª config.data.categoriesï¼Œè‹±æ–‡ subset æ—¶ä¸º physical/environmental ç­‰ï¼‰
            pair_id: æ•°æ®å¯¹ID
            iteration_idx: å½“å‰è¿­ä»£ç¼–å·ï¼ˆä»1å¼€å§‹ï¼‰
            total_iterations: æ€»è¿­ä»£æ¬¡æ•°
        """
        dir_name = self._get_category_dir(category)
        category_dir = os.path.join(self.output_dir, dir_name)
        os.makedirs(category_dir, exist_ok=True)
        
        # åˆ›å»ºè¿­ä»£ç»“æœå­ç›®å½•
        if self.use_original_as_primary:
            iterations_dir = os.path.join(category_dir, "iterations_w_original_as_primary")
        else:
            iterations_dir = os.path.join(category_dir, "iterations_wo_original_as_primary")
        os.makedirs(iterations_dir, exist_ok=True)
        
        # å°† pair_id ä¸­çš„ä¸­æ–‡ subset æ›¿æ¢ä¸ºè‹±æ–‡
        safe_pair_id = self._sanitize_filename_part(pair_id)
        
        # å‘½åè§„åˆ™ï¼šæ˜ç¡®æ ‡è¯†ä¸ºè¿‡ç¨‹ç»“æœ
        # æ ¼å¼ï¼š{pair_id}_refined_iteration_{iter_idx}_of_{total_iterations}.png
        filename = f"{safe_pair_id}_refined_iteration_{iteration_idx}_of_{total_iterations}.png"
        filepath = os.path.join(iterations_dir, filename)
        image.save(filepath)
        
        # è®°å½•ä¿å­˜ä¿¡æ¯ï¼ˆè¿‡ç¨‹ç»“æœï¼‰
        save_msg = f"ğŸ’¾ Saved iteration {iteration_idx}/{total_iterations} intermediate result for pair {pair_id}: {filename}"
        self.logger.info(save_msg)
        print(save_msg)
    
    def _use_original_as_primary(self, category: str, category_data: IterativeCategoryData):
        """
        ç›´æ¥ä½¿ç”¨åŸå›¾ä½œä¸º primary edited imagesï¼ˆtrickï¼‰
        
        Args:
            category: ç±»åˆ«åç§°
            category_data: ç±»åˆ«æ•°æ®
        """
        self.logger.info(f"Using original images as primary images for category: {category}")
        
        for pair in category_data.data_pairs:
            # ç¡®ä¿ original_image å·²è§£ç 
            if pair.original_image is None:
                if pair.original_image_b64:
                    pair.original_image = decode_base64_image(pair.original_image_b64)
                else:
                    raise RuntimeError(
                        f"original_image and original_image_b64 are both None for pair {pair.pair_id} "
                        f"in category {category}. Cannot use original as primary."
                    )
            
            # ç›´æ¥ä½¿ç”¨åŸå›¾ä½œä¸º primary_edited_image
            pair.primary_edited_image = pair.original_image
        
        self.logger.info(f"Set {len(category_data.data_pairs)} pairs to use original images as primary images")
    
    def _load_primary_images_from_dir(self, category: str, category_data: IterativeCategoryData):
        """
        ä»æŒ‡å®šç›®å½•åŠ è½½ primary edited images
        
        æ–‡ä»¶å‘½åæ ¼å¼ï¼š{pair_id}{primary_image_suffix}
        ç›®å½•ç»“æ„ï¼š{primary_images_dir}/{category}/{pair_id}{primary_image_suffix}
        
        Args:
            category: ç±»åˆ«åç§°
            category_data: ç±»åˆ«æ•°æ®
        """
        if not self.primary_images_dir:
            raise ValueError("primary_images_dir not configured. Cannot load primary images from directory.")
        
        dir_name = self._get_category_dir(category)
        category_dir = os.path.join(self.primary_images_dir, dir_name)
        
        if not os.path.exists(category_dir):
            raise FileNotFoundError(f"Primary images directory not found: {category_dir}")
        
        self.logger.info(f"Loading primary edited images from: {category_dir}")
        self.logger.info(f"Using primary image suffix: {self.primary_image_suffix}")
        
        loaded_count = 0
        missing_count = 0
        
        for pair in category_data.data_pairs:
            # æ„å»ºæ–‡ä»¶è·¯å¾„ï¼š{pair_id}{primary_image_suffix}
            # å°è¯•ä¸¤ç§æ–‡ä»¶åï¼šåŸå§‹ pair_idï¼ˆå¯èƒ½å«ä¸­æ–‡ï¼‰å’Œ sanitized pair_idï¼ˆè‹±æ–‡ï¼‰ï¼Œä»¥å…¼å®¹æ–°æ—§æ–‡ä»¶
            filename_orig = f"{pair.pair_id}{self.primary_image_suffix}"
            filename_safe = f"{self._sanitize_filename_part(pair.pair_id)}{self.primary_image_suffix}"
            filepath = os.path.join(category_dir, filename_orig)
            if not os.path.exists(filepath):
                filepath = os.path.join(category_dir, filename_safe)
            
            if os.path.exists(filepath):
                try:
                    # åŠ è½½å›¾åƒ
                    pair.primary_edited_image = Image.open(filepath).convert('RGB')
                    loaded_count += 1
                except Exception as e:
                    self.logger.warning(f"Failed to load image {filepath}: {e}. Using original image as fallback.")
                    # ä½¿ç”¨åŸå›¾ä½œä¸º fallback
                    if pair.original_image is None:
                        pair.original_image = decode_base64_image(pair.original_image_b64)
                    pair.primary_edited_image = pair.original_image
                    missing_count += 1
            else:
                self.logger.warning(f"Primary image not found: {filepath}. Using original image as fallback.")
                # ä½¿ç”¨åŸå›¾ä½œä¸º fallback
                if pair.original_image is None:
                    pair.original_image = decode_base64_image(pair.original_image_b64)
                pair.primary_edited_image = pair.original_image
                missing_count += 1
        
        self.logger.info(f"Loaded {loaded_count}/{len(category_data.data_pairs)} primary images from directory")
        if missing_count > 0:
            self.logger.warning(f"Missing {missing_count} primary images, used original images as fallback")
    
    def _generate_report(self, benchmark_data: IterativeBenchmarkData) -> Dict[str, Any]:
        """
        ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        
        Args:
            benchmark_data: Benchmarkæ•°æ®
        
        Returns:
            æŠ¥å‘Šå­—å…¸ï¼ˆä¸standard pipelineå…¼å®¹ï¼‰
        """
        global_stats = benchmark_data.calculate_global_statistics()
        
        # å¦‚æœè·³è¿‡äº† Stage4ï¼Œä¸è¾“å‡ºè¯„ä»·ç›¸å…³çš„ç»Ÿè®¡ä¿¡æ¯
        if not self.skip_stage4:
            self.logger.info("\n" + "=" * 80)
            self.logger.info("GLOBAL STATISTICS")
            self.logger.info("=" * 80)
            self.logger.info(f"Total Pairs: {global_stats['total_pairs']}")
            if self.skip_refinement:
                self.logger.info(f"Primary Avg Score: {global_stats['global_primary_avg']:.3f}")
            else:
                self.logger.info(f"Refined Avg Score: {global_stats['global_refined_avg']:.3f}")
            
            if self.enable_pq_metric:
                self.logger.info(f"\n{'='*80}")
                self.logger.info("PQ METRIC STATISTICS (using min score)")
                self.logger.info(f"{'='*80}")
                if self.skip_refinement:
                    self.logger.info(f"Primary PQ Avg: {global_stats.get('global_primary_pq_avg', 0.0):.2f}")
                else:
                    self.logger.info(f"Refined PQ Avg: {global_stats.get('global_refined_pq_avg', 0.0):.2f}")
            
            if self.enable_sc_metric:
                self.logger.info(f"\n{'='*80}")
                self.logger.info("SC METRIC STATISTICS (using min score)")
                self.logger.info(f"{'='*80}")
                if self.skip_refinement:
                    self.logger.info(f"Primary SC Avg: {global_stats.get('global_primary_sc_avg', 0.0):.2f}")
                else:
                    self.logger.info(f"Refined SC Avg: {global_stats.get('global_refined_sc_avg', 0.0):.2f}")
            
            if self.enable_instruction_following_metric:
                self.logger.info(f"\n{'='*80}")
                self.logger.info("INSTRUCTION FOLLOWING METRIC STATISTICS")
                self.logger.info(f"{'='*80}")
                if self.skip_refinement:
                    self.logger.info(f"Primary IF Avg: {global_stats.get('global_primary_if_avg', 0.0):.2f}")
                else:
                    self.logger.info(f"Refined IF Avg: {global_stats.get('global_refined_if_avg', 0.0):.2f}")
        else:
            self.logger.info("\n" + "=" * 80)
            self.logger.info("GLOBAL SUMMARY (Stage 4 Scoring Skipped)")
            self.logger.info("=" * 80)
            self.logger.info(f"Total Pairs: {global_stats['total_pairs']}")
            self.logger.info("Note: Scoring statistics are not available (Stage 4 was skipped)")
        
        # æ„å»ºæŠ¥å‘Šæ•°æ®ç»“æ„
        category_statistics = {}
        mllm_analysis_details = {}  # æ–°å¢ï¼šå­˜å‚¨MLLMåˆ†æè¯¦æƒ…
        
        for category_name, category_data in benchmark_data.categories.items():
            stats = category_data.statistics
            
            # å¦‚æœè·³è¿‡äº† Stage4ï¼Œä¸åŒ…å«è¯„ä»·ç›¸å…³çš„ç»Ÿè®¡ä¿¡æ¯
            if not self.skip_stage4:
                if self.skip_refinement:
                    # skip_refinement æ¨¡å¼ï¼šä»…ä½¿ç”¨ primary_* ä½œä¸ºä¸»ç»Ÿè®¡
                    category_statistics[category_name] = {
                        "primary_yes_count": stats.get("primary_yes_count", 0),
                        "primary_no_count": stats.get("primary_no_count", 0),
                        "primary_yes_rate": stats.get("primary_yes_rate", 0.0),
                        "primary_avg": stats.get("primary_avg", 0.0),
                        "total_count": stats.get("total_count", 0),
                        "average": stats.get("primary_avg", 0.0),
                        "std": 0.0,
                        "min": 0.0,
                        "max": 100.0,
                        "count": stats.get("total_count", 0)
                    }
                else:
                    category_statistics[category_name] = {
                        "refined_yes_count": stats.get("refined_yes_count", 0),
                        "refined_no_count": stats.get("refined_no_count", 0),
                        "refined_yes_rate": stats.get("refined_yes_rate", 0.0),
                        "refined_avg": stats.get("refined_avg", 0.0),
                        "total_count": stats.get("total_count", 0),
                        "average": stats.get("refined_avg", 0.0),
                        "std": 0.0,
                        "min": 0.0,
                        "max": 100.0,
                        "count": stats.get("total_count", 0)
                    }
                
                # Primaryç»Ÿè®¡ï¼ˆé skip_refinement æ—¶ï¼Œenable_primary_scoring æ—¶åŒ…å«ï¼‰
                if self.enable_primary_scoring and not self.skip_refinement:
                    category_statistics[category_name].update({
                        "primary_yes_count": stats.get("primary_yes_count", 0),
                        "primary_no_count": stats.get("primary_no_count", 0),
                        "primary_yes_rate": stats.get("primary_yes_rate", 0.0),
                        "primary_avg": stats.get("primary_avg", 0.0),
                        # æ”¹è¿›ç»Ÿè®¡
                        "improved_count": stats.get("improved_count", 0),
                        "improvement_rate": stats.get("improvement_rate", 0.0),
                        "maintained_count": stats.get("maintained_count", 0),
                        "maintained_rate": stats.get("maintained_rate", 0.0),
                        "regression_count": stats.get("regression_count", 0),
                        "regression_rate": stats.get("regression_rate", 0.0),
                        "unchanged_count": stats.get("unchanged_count", 0),
                        "unchanged_rate": stats.get("unchanged_rate", 0.0)
                    })
                
                # PQæŒ‡æ ‡ç»Ÿè®¡ï¼ˆåªåœ¨å¯ç”¨æ—¶åŒ…å«ï¼‰
                if self.enable_pq_metric:
                    if self.skip_refinement:
                        pq_stats = {
                            "primary_pq_avg": stats.get("primary_pq_avg", 0.0),
                            "primary_pq_avg_naturalness": stats.get("primary_pq_avg_naturalness", 0.0),
                            "primary_pq_avg_artifacts": stats.get("primary_pq_avg_artifacts", 0.0)
                        }
                    else:
                        pq_stats = {
                            "refined_pq_avg": stats.get("refined_pq_avg", 0.0),
                            "refined_pq_avg_naturalness": stats.get("refined_pq_avg_naturalness", 0.0),
                            "refined_pq_avg_artifacts": stats.get("refined_pq_avg_artifacts", 0.0)
                        }
                    if self.enable_primary_scoring and not self.skip_refinement:
                        pq_stats.update({
                            "primary_pq_avg": stats.get("primary_pq_avg", 0.0),
                            "primary_pq_avg_naturalness": stats.get("primary_pq_avg_naturalness", 0.0),
                            "primary_pq_avg_artifacts": stats.get("primary_pq_avg_artifacts", 0.0),
                            "pq_improvement": stats.get("pq_improvement", 0.0),
                            "pq_improvement_naturalness": stats.get("pq_improvement_naturalness", 0.0),
                            "pq_improvement_artifacts": stats.get("pq_improvement_artifacts", 0.0)
                        })
                    category_statistics[category_name].update(pq_stats)
                
                # SCæŒ‡æ ‡ç»Ÿè®¡ï¼ˆåªåœ¨å¯ç”¨æ—¶åŒ…å«ï¼‰
                if self.enable_sc_metric:
                    if self.skip_refinement:
                        sc_stats = {
                            "primary_sc_avg": stats.get("primary_sc_avg", 0.0),
                            "primary_sc_avg_editing_success": stats.get("primary_sc_avg_editing_success", 0.0),
                            "primary_sc_avg_overediting": stats.get("primary_sc_avg_overediting", 0.0)
                        }
                    else:
                        sc_stats = {
                            "refined_sc_avg": stats.get("refined_sc_avg", 0.0),
                            "refined_sc_avg_editing_success": stats.get("refined_sc_avg_editing_success", 0.0),
                            "refined_sc_avg_overediting": stats.get("refined_sc_avg_overediting", 0.0)
                        }
                    if self.enable_primary_scoring and not self.skip_refinement:
                        sc_stats.update({
                            "primary_sc_avg": stats.get("primary_sc_avg", 0.0),
                            "primary_sc_avg_editing_success": stats.get("primary_sc_avg_editing_success", 0.0),
                            "primary_sc_avg_overediting": stats.get("primary_sc_avg_overediting", 0.0),
                            "sc_improvement": stats.get("sc_improvement", 0.0),
                            "sc_improvement_editing_success": stats.get("sc_improvement_editing_success", 0.0),
                            "sc_improvement_overediting": stats.get("sc_improvement_overediting", 0.0)
                        })
                    category_statistics[category_name].update(sc_stats)
                
                # IFæŒ‡æ ‡ç»Ÿè®¡ï¼ˆåªåœ¨å¯ç”¨æ—¶åŒ…å«ï¼‰
                if self.enable_instruction_following_metric:
                    if self.skip_refinement:
                        if_stats = {"primary_if_avg": stats.get("primary_if_avg", 0.0)}
                    else:
                        if_stats = {"refined_if_avg": stats.get("refined_if_avg", 0.0)}
                    if self.enable_primary_scoring and not self.skip_refinement:
                        if_stats.update({
                            "primary_if_avg": stats.get("primary_if_avg", 0.0),
                            "if_improvement": stats.get("if_improvement", 0.0)
                        })
                    category_statistics[category_name].update(if_stats)
            else:
                # å¦‚æœè·³è¿‡ Stage4ï¼ŒåªåŒ…å«åŸºç¡€ç»Ÿè®¡ä¿¡æ¯ï¼ˆä¸åŒ…å«è¯„ä»·ç›¸å…³ï¼‰
                category_statistics[category_name] = {
                    "total_count": stats.get("total_count", 0),
                    "note": "Scoring statistics not available (Stage 4 was skipped)"
                }
            
            # æ”¶é›†è¯¥ç±»åˆ«çš„MLLMåˆ†æè¯¦æƒ…
            mllm_analysis_details[category_name] = []
            for pair in category_data.data_pairs:
                sample_detail = {
                    "pair_id": pair.pair_id,
                    "original_instruction": pair.edit_instruction,
                    "cot_reasoning": pair.cot_reasoning,
                    "re_edit_instruction": pair.re_edit_instruction,  # æ‹¼æ¥åçš„å­—ç¬¦ä¸²ï¼ˆå‘åå…¼å®¹ï¼‰
                    "re_edit_instructions": pair.re_edit_instructions,  # ç‹¬ç«‹çš„æŒ‡ä»¤åˆ—è¡¨ï¼ˆç”¨äºè¿­ä»£refinementï¼‰
                    "mllm_raw_output": pair.mllm_raw_output,
                    "refinement_iterations": pair.refinement_iterations,  # è®°å½•refinementè¿­ä»£æ¬¡æ•°
                }
                
                # å¦‚æœæœªè·³è¿‡ Stage4ï¼ŒåŒ…å«è¯„ä»·ç»“æœ
                if not self.skip_stage4:
                    if self.skip_refinement:
                        # skip_refinement æ¨¡å¼ï¼šä»…åŒ…å« primary_* è¯„ä»·ç»“æœ
                        sample_detail.update({
                            "primary_score": pair.primary_score,
                            "primary_score_reasoning": pair.primary_score_reasoning
                        })
                        if self.enable_pq_metric:
                            sample_detail.update({
                                "primary_pq_score": pair.primary_pq_score,
                                "primary_pq_reasoning": pair.primary_pq_reasoning
                            })
                        if self.enable_sc_metric:
                            sample_detail.update({
                                "primary_sc_score": pair.primary_sc_score,
                                "primary_sc_reasoning": pair.primary_sc_reasoning
                            })
                        if self.enable_instruction_following_metric:
                            sample_detail.update({
                                "primary_if_score": pair.primary_if_score,
                                "primary_if_reasoning": pair.primary_if_reasoning
                            })
                    else:
                        # æ­£å¸¸æ¨¡å¼ï¼šåŒ…å« refined å’Œ primary
                        sample_detail.update({
                            "refined_score": pair.refined_score,
                            "refined_score_reasoning": pair.refined_score_reasoning
                        })
                        if self.enable_primary_scoring:
                            sample_detail.update({
                                "primary_score": pair.primary_score,
                                "primary_score_reasoning": pair.primary_score_reasoning,
                                "improvement_rate": pair.improvement_rate
                            })
                        if self.enable_pq_metric:
                            pq_detail = {
                                "refined_pq_score": pair.refined_pq_score,
                                "refined_pq_reasoning": pair.refined_pq_reasoning
                            }
                            if self.enable_primary_scoring:
                                pq_detail.update({
                                    "primary_pq_score": pair.primary_pq_score,
                                    "primary_pq_reasoning": pair.primary_pq_reasoning
                                })
                            sample_detail.update(pq_detail)
                        if self.enable_sc_metric:
                            sc_detail = {
                                "refined_sc_score": pair.refined_sc_score,
                                "refined_sc_reasoning": pair.refined_sc_reasoning
                            }
                            if self.enable_primary_scoring:
                                sc_detail.update({
                                    "primary_sc_score": pair.primary_sc_score,
                                    "primary_sc_reasoning": pair.primary_sc_reasoning
                                })
                            sample_detail.update(sc_detail)
                        if self.enable_instruction_following_metric:
                            if_detail = {
                                "refined_if_score": pair.refined_if_score,
                                "refined_if_reasoning": pair.refined_if_reasoning
                            }
                            if self.enable_primary_scoring:
                                if_detail.update({
                                    "primary_if_score": pair.primary_if_score,
                                    "primary_if_reasoning": pair.primary_if_reasoning
                                })
                            sample_detail.update(if_detail)
                
                mllm_analysis_details[category_name].append(sample_detail)
        
        # æ„å»ºoverall statisticsï¼ˆå…¼å®¹standard pipelineï¼‰
        if not self.skip_stage4:
            if self.skip_refinement:
                overall_statistics = {
                    "global_primary_yes_count": global_stats.get('global_primary_yes_count', 0),
                    "global_primary_yes_rate": global_stats.get('global_primary_yes_rate', 0.0),
                    "average": global_stats.get('global_primary_avg', 0.0),
                    "std": 0.0,
                    "total_samples": global_stats['total_pairs'],
                    "primary_average": global_stats.get('global_primary_avg', 0.0)
                }
            else:
                overall_statistics = {
                    "global_refined_yes_count": global_stats.get('global_refined_yes_count', 0),
                    "global_refined_yes_rate": global_stats.get('global_refined_yes_rate', 0.0),
                    "average": global_stats.get('global_refined_avg', 0.0),
                    "std": 0.0,
                    "total_samples": global_stats['total_pairs'],
                    "refined_average": global_stats.get('global_refined_avg', 0.0)
                }
            
            # Primaryç»Ÿè®¡ï¼ˆé skip_refinement æ—¶ï¼Œenable_primary_scoring æ—¶åŒ…å«ï¼‰
            if self.enable_primary_scoring and not self.skip_refinement:
                overall_statistics.update({
                    "global_primary_yes_count": global_stats.get('global_primary_yes_count', 0),
                    "global_primary_yes_rate": global_stats.get('global_primary_yes_rate', 0.0),
                    "primary_average": global_stats.get('global_primary_avg', 0.0),
                    # æ”¹è¿›ç»Ÿè®¡
                    "global_improved_count": global_stats.get('global_improved_count', 0),
                    "global_improvement_rate": global_stats.get('global_improvement_rate', 0.0),
                    "global_maintained_count": global_stats.get('global_maintained_count', 0),
                    "global_maintained_rate": global_stats.get('global_maintained_rate', 0.0),
                    "global_regression_count": global_stats.get('global_regression_count', 0),
                    "global_regression_rate": global_stats.get('global_regression_rate', 0.0),
                    "global_unchanged_count": global_stats.get('global_unchanged_count', 0),
                    "global_unchanged_rate": global_stats.get('global_unchanged_rate', 0.0)
                })
            
            # PQæŒ‡æ ‡å…¨å±€ç»Ÿè®¡ï¼ˆåªåœ¨å¯ç”¨æ—¶åŒ…å«ï¼‰
            if self.enable_pq_metric:
                if self.skip_refinement:
                    pq_overall = {"global_primary_pq_avg": global_stats.get('global_primary_pq_avg', 0.0)}
                else:
                    pq_overall = {
                        "global_refined_pq_avg": global_stats.get('global_refined_pq_avg', 0.0)
                    }
                    if self.enable_primary_scoring:
                        pq_overall.update({
                            "global_primary_pq_avg": global_stats.get('global_primary_pq_avg', 0.0),
                            "global_pq_improvement": global_stats.get('global_pq_improvement', 0.0)
                        })
                overall_statistics.update(pq_overall)
            
            # SCæŒ‡æ ‡å…¨å±€ç»Ÿè®¡ï¼ˆåªåœ¨å¯ç”¨æ—¶åŒ…å«ï¼‰
            if self.enable_sc_metric:
                if self.skip_refinement:
                    sc_overall = {"global_primary_sc_avg": global_stats.get('global_primary_sc_avg', 0.0)}
                else:
                    sc_overall = {
                        "global_refined_sc_avg": global_stats.get('global_refined_sc_avg', 0.0)
                    }
                    if self.enable_primary_scoring:
                        sc_overall.update({
                            "global_primary_sc_avg": global_stats.get('global_primary_sc_avg', 0.0),
                            "global_sc_improvement": global_stats.get('global_sc_improvement', 0.0)
                        })
                overall_statistics.update(sc_overall)
            
            # IFæŒ‡æ ‡å…¨å±€ç»Ÿè®¡ï¼ˆåªåœ¨å¯ç”¨æ—¶åŒ…å«ï¼‰
            if self.enable_instruction_following_metric:
                if self.skip_refinement:
                    if_overall = {"global_primary_if_avg": global_stats.get('global_primary_if_avg', 0.0)}
                else:
                    if_overall = {
                        "global_refined_if_avg": global_stats.get('global_refined_if_avg', 0.0)
                    }
                    if self.enable_primary_scoring:
                        if_overall.update({
                            "global_primary_if_avg": global_stats.get('global_primary_if_avg', 0.0),
                            "global_if_improvement": global_stats.get('global_if_improvement', 0.0)
                        })
                overall_statistics.update(if_overall)
        else:
            # å¦‚æœè·³è¿‡ Stage4ï¼Œä¸åŒ…å«è¯„ä»·ç›¸å…³çš„ç»Ÿè®¡ä¿¡æ¯
            overall_statistics = {
                "total_samples": global_stats['total_pairs'],
                "note": "Scoring statistics not available (Stage 4 was skipped)"
            }
        
        # æ„å»ºmetadata
        metadata = {
            "pipeline_mode": "iterative_refinement",
            "benchmark_config": self.config.get("benchmark", {}),
            "diffusion_model": self.config.get("diffusion_model", {}),
            "mllm": self.config.get("mllm", {}),
            "refinement_diffusion": self.config.get("diffusion_model", {}).get("refinement", {}),
            "reward_model": self.config.get("reward_model", {}),
            "total_pairs": global_stats['total_pairs'],
            "categories": benchmark_data.category_names,
            # è¯„åˆ†æŒ‡æ ‡é…ç½®
            "evaluation_config": {
                "enable_pq_metric": self.enable_pq_metric,
                "enable_sc_metric": self.enable_sc_metric,
                "enable_instruction_following_metric": self.enable_instruction_following_metric,
                "enable_primary_scoring": self.enable_primary_scoring,
                "skip_stage4": self.skip_stage4,
                "skip_refinement": self.skip_refinement  # æ˜¯å¦è·³è¿‡ refinement æµç¨‹
            }
        }
        
        # æ„å»ºsummaryï¼ˆå…¼å®¹standard pipelineçš„main.pyï¼‰
        if not self.skip_stage4:
            if self.skip_refinement:
                summary = {
                    "num_categories": len(category_statistics),
                    "total_samples": global_stats['total_pairs'],
                    "overall_primary_yes_rate": global_stats.get('global_primary_yes_rate', 0.0),
                    "overall_mean": global_stats.get('global_primary_avg', 0.0),
                    "category_means": {
                        cat_name: cat_stats.get("primary_avg", 0.0)
                        for cat_name, cat_stats in category_statistics.items()
                        if "primary_avg" in cat_stats
                    },
                    "category_yes_rates": {
                        cat_name: cat_stats.get("primary_yes_rate", 0.0)
                        for cat_name, cat_stats in category_statistics.items()
                        if "primary_yes_rate" in cat_stats
                    },
                    "primary_overall_mean": global_stats.get('global_primary_avg', 0.0)
                }
            else:
                summary = {
                    "num_categories": len(category_statistics),
                    "total_samples": global_stats['total_pairs'],
                    "overall_refined_yes_rate": global_stats.get('global_refined_yes_rate', 0.0),
                    "overall_mean": global_stats.get('global_refined_avg', 0.0),
                    "category_means": {
                        cat_name: cat_stats.get("refined_avg", 0.0)
                        for cat_name, cat_stats in category_statistics.items()
                        if "refined_avg" in cat_stats
                    },
                    "category_yes_rates": {
                        cat_name: cat_stats.get("refined_yes_rate", 0.0)
                        for cat_name, cat_stats in category_statistics.items()
                        if "refined_yes_rate" in cat_stats
                    },
                    "refined_overall_mean": global_stats.get('global_refined_avg', 0.0)
                }
            
            # Primaryç»Ÿè®¡ï¼ˆé skip_refinement æ—¶ï¼Œenable_primary_scoring æ—¶åŒ…å«æ”¹è¿›ç»Ÿè®¡ï¼‰
            if self.enable_primary_scoring and not self.skip_refinement:
                summary.update({
                    "overall_primary_yes_rate": global_stats.get('global_primary_yes_rate', 0.0),
                    "primary_overall_mean": global_stats.get('global_primary_avg', 0.0),
                    "category_primary_yes_rates": {
                        cat_name: cat_stats.get("primary_yes_rate", 0.0)
                        for cat_name, cat_stats in category_statistics.items()
                        if "primary_yes_rate" in cat_stats
                    },
                    "overall_improvement_rate": global_stats.get('global_improvement_rate', 0.0),
                    "overall_maintained_rate": global_stats.get('global_maintained_rate', 0.0),
                    "overall_regression_rate": global_stats.get('global_regression_rate', 0.0),
                    "overall_unchanged_rate": global_stats.get('global_unchanged_rate', 0.0),
                    "total_improved": global_stats.get('global_improved_count', 0),
                    "improvement_percentage": global_stats.get('global_improvement_rate', 0.0)
                })
            
            # PQæŒ‡æ ‡æ±‡æ€»ï¼ˆåªåœ¨å¯ç”¨æ—¶åŒ…å«ï¼‰
            if self.enable_pq_metric:
                if self.skip_refinement:
                    pq_summary = {
                        "overall_primary_pq_avg": global_stats.get('global_primary_pq_avg', 0.0),
                        "category_pq_primary_avgs": {
                            cat_name: cat_stats.get("primary_pq_avg", 0.0)
                            for cat_name, cat_stats in category_statistics.items()
                            if "primary_pq_avg" in cat_stats
                        }
                    }
                else:
                    pq_summary = {
                        "overall_refined_pq_avg": global_stats.get('global_refined_pq_avg', 0.0),
                        "category_pq_refined_avgs": {
                            cat_name: cat_stats.get("refined_pq_avg", 0.0)
                            for cat_name, cat_stats in category_statistics.items()
                            if "refined_pq_avg" in cat_stats
                        }
                    }
                    if self.enable_primary_scoring:
                        pq_summary.update({
                            "overall_primary_pq_avg": global_stats.get('global_primary_pq_avg', 0.0),
                            "category_pq_primary_avgs": {
                                cat_name: cat_stats.get("primary_pq_avg", 0.0)
                                for cat_name, cat_stats in category_statistics.items()
                                if "primary_pq_avg" in cat_stats
                            },
                            "overall_pq_improvement": global_stats.get('global_pq_improvement', 0.0)
                        })
                summary.update(pq_summary)
            
            # SCæŒ‡æ ‡æ±‡æ€»ï¼ˆåªåœ¨å¯ç”¨æ—¶åŒ…å«ï¼‰
            if self.enable_sc_metric:
                if self.skip_refinement:
                    sc_summary = {
                        "overall_primary_sc_avg": global_stats.get('global_primary_sc_avg', 0.0),
                        "category_sc_primary_avgs": {
                            cat_name: cat_stats.get("primary_sc_avg", 0.0)
                            for cat_name, cat_stats in category_statistics.items()
                            if "primary_sc_avg" in cat_stats
                        }
                    }
                else:
                    sc_summary = {
                        "overall_refined_sc_avg": global_stats.get('global_refined_sc_avg', 0.0),
                        "category_sc_refined_avgs": {
                            cat_name: cat_stats.get("refined_sc_avg", 0.0)
                            for cat_name, cat_stats in category_statistics.items()
                            if "refined_sc_avg" in cat_stats
                        }
                    }
                    if self.enable_primary_scoring:
                        sc_summary.update({
                            "overall_primary_sc_avg": global_stats.get('global_primary_sc_avg', 0.0),
                            "category_sc_primary_avgs": {
                                cat_name: cat_stats.get("primary_sc_avg", 0.0)
                                for cat_name, cat_stats in category_statistics.items()
                                if "primary_sc_avg" in cat_stats
                            },
                            "overall_sc_improvement": global_stats.get('global_sc_improvement', 0.0)
                        })
                summary.update(sc_summary)
            
            # IFæŒ‡æ ‡æ±‡æ€»ï¼ˆåªåœ¨å¯ç”¨æ—¶åŒ…å«ï¼‰
            if self.enable_instruction_following_metric:
                if self.skip_refinement:
                    if_summary = {
                        "overall_primary_if_avg": global_stats.get('global_primary_if_avg', 0.0),
                        "category_if_primary_avgs": {
                            cat_name: cat_stats.get("primary_if_avg", 0.0)
                            for cat_name, cat_stats in category_statistics.items()
                            if "primary_if_avg" in cat_stats
                        }
                    }
                else:
                    if_summary = {
                        "overall_refined_if_avg": global_stats.get('global_refined_if_avg', 0.0),
                        "category_if_refined_avgs": {
                            cat_name: cat_stats.get("refined_if_avg", 0.0)
                            for cat_name, cat_stats in category_statistics.items()
                            if "refined_if_avg" in cat_stats
                        }
                    }
                    if self.enable_primary_scoring:
                        if_summary.update({
                            "overall_primary_if_avg": global_stats.get('global_primary_if_avg', 0.0),
                            "category_if_primary_avgs": {
                                cat_name: cat_stats.get("primary_if_avg", 0.0)
                                for cat_name, cat_stats in category_statistics.items()
                                if "primary_if_avg" in cat_stats
                            },
                            "overall_if_improvement": global_stats.get('global_if_improvement', 0.0)
                        })
                summary.update(if_summary)
            
            # æ‰¾å‡ºæœ€å¥½å’Œæœ€å·®çš„ç±»åˆ«ï¼ˆåŸºäºrefined yes_rateï¼‰
            if summary.get("category_means"):
                best_category_name = max(summary["category_means"], key=summary["category_means"].get)
                worst_category_name = min(summary["category_means"], key=summary["category_means"].get)
                
                summary["best_category"] = {
                    "name": best_category_name,
                    "score": summary["category_means"][best_category_name]  # yes_rate
                }
                summary["worst_category"] = {
                    "name": worst_category_name,
                    "score": summary["category_means"][worst_category_name]  # yes_rate
                }
            
            # Primaryç±»åˆ«æ’åï¼ˆskip_refinement æ—¶ category_means å·²æ˜¯ primaryï¼›é skip_refinement æ—¶ enable_primary_scoring åŒ…å«ï¼‰
            if self.skip_refinement:
                pass  # best_category å·²åŸºäº primary
            elif self.enable_primary_scoring and summary.get("category_primary_yes_rates") and len(summary["category_primary_yes_rates"]) > 0:
                best_primary_category_name = max(summary["category_primary_yes_rates"], key=summary["category_primary_yes_rates"].get)
                worst_primary_category_name = min(summary["category_primary_yes_rates"], key=summary["category_primary_yes_rates"].get)
                
                summary["best_primary_category"] = {
                    "name": best_primary_category_name,
                    "score": summary["category_primary_yes_rates"][best_primary_category_name]
                }
                summary["worst_primary_category"] = {
                    "name": worst_primary_category_name,
                    "score": summary["category_primary_yes_rates"][worst_primary_category_name]
                }
            
            # æ‰¾å‡º PQ æŒ‡æ ‡æœ€å¥½å’Œæœ€å·®çš„ç±»åˆ«
            pq_avgs = summary.get("category_pq_primary_avgs") if self.skip_refinement else summary.get("category_pq_refined_avgs")
            if pq_avgs and len(pq_avgs) > 0:
                best_pq_category_name = max(pq_avgs, key=pq_avgs.get)
                worst_pq_category_name = min(pq_avgs, key=pq_avgs.get)
                summary["best_pq_category"] = {
                    "name": best_pq_category_name,
                    "score": pq_avgs[best_pq_category_name]
                }
                summary["worst_pq_category"] = {
                    "name": worst_pq_category_name,
                    "score": pq_avgs[worst_pq_category_name]
                }
                
                if self.skip_refinement:
                    summary["best_pq_primary_category"] = summary["best_pq_category"]
                    summary["worst_pq_primary_category"] = summary["worst_pq_category"]
                elif self.enable_primary_scoring and summary.get("category_pq_primary_avgs") and len(summary["category_pq_primary_avgs"]) > 0:
                    best_pq_primary_category_name = max(summary["category_pq_primary_avgs"], key=summary["category_pq_primary_avgs"].get)
                    worst_pq_primary_category_name = min(summary["category_pq_primary_avgs"], key=summary["category_pq_primary_avgs"].get)
                    
                    summary["best_pq_primary_category"] = {
                        "name": best_pq_primary_category_name,
                        "score": summary["category_pq_primary_avgs"][best_pq_primary_category_name]
                    }
                    summary["worst_pq_primary_category"] = {
                        "name": worst_pq_primary_category_name,
                        "score": summary["category_pq_primary_avgs"][worst_pq_primary_category_name]
                    }
            
            # æ‰¾å‡º SC æŒ‡æ ‡æœ€å¥½å’Œæœ€å·®çš„ç±»åˆ«
            sc_avgs = summary.get("category_sc_primary_avgs") if self.skip_refinement else summary.get("category_sc_refined_avgs")
            if sc_avgs and len(sc_avgs) > 0:
                best_sc_category_name = max(sc_avgs, key=sc_avgs.get)
                worst_sc_category_name = min(sc_avgs, key=sc_avgs.get)
                summary["best_sc_category"] = {
                    "name": best_sc_category_name,
                    "score": sc_avgs[best_sc_category_name]
                }
                summary["worst_sc_category"] = {
                    "name": worst_sc_category_name,
                    "score": sc_avgs[worst_sc_category_name]
                }
                
                if self.skip_refinement:
                    summary["best_sc_primary_category"] = summary["best_sc_category"]
                    summary["worst_sc_primary_category"] = summary["worst_sc_category"]
                elif self.enable_primary_scoring and summary.get("category_sc_primary_avgs") and len(summary["category_sc_primary_avgs"]) > 0:
                    best_sc_primary_category_name = max(summary["category_sc_primary_avgs"], key=summary["category_sc_primary_avgs"].get)
                    worst_sc_primary_category_name = min(summary["category_sc_primary_avgs"], key=summary["category_sc_primary_avgs"].get)
                    
                    summary["best_sc_primary_category"] = {
                        "name": best_sc_primary_category_name,
                        "score": summary["category_sc_primary_avgs"][best_sc_primary_category_name]
                    }
                    summary["worst_sc_primary_category"] = {
                        "name": worst_sc_primary_category_name,
                        "score": summary["category_sc_primary_avgs"][worst_sc_primary_category_name]
                    }
            
            # æ‰¾å‡º IF æŒ‡æ ‡æœ€å¥½å’Œæœ€å·®çš„ç±»åˆ«
            if_avgs = summary.get("category_if_primary_avgs") if self.skip_refinement else summary.get("category_if_refined_avgs")
            if if_avgs and len(if_avgs) > 0:
                best_if_category_name = max(if_avgs, key=if_avgs.get)
                worst_if_category_name = min(if_avgs, key=if_avgs.get)
                summary["best_if_category"] = {
                    "name": best_if_category_name,
                    "score": if_avgs[best_if_category_name]
                }
                summary["worst_if_category"] = {
                    "name": worst_if_category_name,
                    "score": if_avgs[worst_if_category_name]
                }
                
                if self.skip_refinement:
                    summary["best_if_primary_category"] = summary["best_if_category"]
                    summary["worst_if_primary_category"] = summary["worst_if_category"]
                elif self.enable_primary_scoring and summary.get("category_if_primary_avgs") and len(summary["category_if_primary_avgs"]) > 0:
                    best_if_primary_category_name = max(summary["category_if_primary_avgs"], key=summary["category_if_primary_avgs"].get)
                    worst_if_primary_category_name = min(summary["category_if_primary_avgs"], key=summary["category_if_primary_avgs"].get)
                    
                    summary["best_if_primary_category"] = {
                        "name": best_if_primary_category_name,
                        "score": summary["category_if_primary_avgs"][best_if_primary_category_name]
                    }
                    summary["worst_if_primary_category"] = {
                        "name": worst_if_primary_category_name,
                        "score": summary["category_if_primary_avgs"][worst_if_primary_category_name]
                    }
        else:
            # å¦‚æœè·³è¿‡ Stage4ï¼ŒåªåŒ…å«åŸºç¡€ä¿¡æ¯
            summary = {
                "num_categories": len(category_statistics),
                "total_samples": global_stats['total_pairs'],
                "note": "Scoring statistics not available (Stage 4 was skipped)"
            }
        
        # ç»„è£…å®Œæ•´æŠ¥å‘Š
        # ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶åï¼ˆå¦‚æœé…ç½®äº† use_original_as_primaryã€primary_images_dir æˆ– skip_stage4ï¼Œæ·»åŠ æ ‡è¯†ä»¥åŒºåˆ†ï¼‰
        filename_parts = []
        
        if self.use_original_as_primary:
            filename_parts.append("use_original_as_primary")
        elif self.primary_images_dir:
            primary_dir_name = os.path.basename(os.path.normpath(self.primary_images_dir))
            filename_parts.append(f"refined_only_{primary_dir_name}")
        
        if self.skip_stage4:
            filename_parts.append("skip_stage4")
        if self.skip_refinement:
            filename_parts.append("skip_refinement")
        
        if filename_parts:
            report_filename = f"{self.primary_model_type}_{'_'.join(filename_parts)}_{self.start_timestamp}.json"
        else:
            report_filename = f"{self.primary_model_type}_{self.start_timestamp}.json"
        report_path = os.path.join(self.output_dir, report_filename)
        
        # è®¡ç®—scoring_healthç»Ÿè®¡ï¼ˆå¦‚æœæœªè·³è¿‡ Stage4ï¼‰
        if not self.skip_stage4:
            scoring_health_summary = self._compute_scoring_health_summary()
        else:
            scoring_health_summary = {
                "note": "Scoring health statistics not available (Stage 4 was skipped)"
            }
        
        report = {
            "metadata": metadata,
            "category_statistics": category_statistics,
            "overall_statistics": overall_statistics,
            "summary": summary,  # å…¼å®¹standard pipeline
            "mllm_analysis": mllm_analysis_details,  # Iterativeç‰¹æœ‰ï¼šMLLMåˆ†æè¯¦æƒ…
            "scoring_health": scoring_health_summary,  # æ–¹æ¡ˆ3ï¼šè¯„åˆ†å¥åº·åº¦è¿½è¸ªï¼ˆå¦‚æœæœªè·³è¿‡ Stage4ï¼‰
            "timestamp": self._get_timestamp(),
            "report_file": {
                "filename": report_filename,
                "path": report_path,
                "model_type": self.primary_model_type,
                "start_time": self.start_timestamp
            }
        }
        
        # ä¿å­˜ç»Ÿè®¡ä¿¡æ¯åˆ°æ–‡ä»¶ï¼ˆä½¿ç”¨åŠ¨æ€æ–‡ä»¶åï¼šæ¨¡å‹ç±»å‹_æ—¶é—´æˆ³.jsonï¼‰
        import json
        
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"\n{'='*80}")
        self.logger.info("Report saved to: " + report_path)
        self.logger.info(f"Report filename: {report_filename}")
        self.logger.info(f"  Model Type: {self.primary_model_type}")
        self.logger.info(f"  Start Time: {self.start_timestamp}")
        
        # è¾“å‡ºMLLMåˆ†æç»Ÿè®¡
        self.logger.info(f"\n{'='*80}")
        self.logger.info("MLLM Analysis Summary:")
        self.logger.info(f"{'='*80}")
        
        total_samples = 0
        samples_with_cot = 0
        samples_with_different_instruction = 0
        
        for category_name, samples in mllm_analysis_details.items():
            for sample in samples:
                total_samples += 1
                if sample["cot_reasoning"] and len(sample["cot_reasoning"].strip()) > 0:
                    samples_with_cot += 1
                # æ£€æŸ¥re-editæŒ‡ä»¤æ˜¯å¦ä¸åŸå§‹æŒ‡ä»¤ä¸åŒ
                if sample["re_edit_instruction"] != sample["original_instruction"]:
                    samples_with_different_instruction += 1
        
        self.logger.info(f"Total Samples Analyzed: {total_samples}")
        self.logger.info(f"Samples with CoT Reasoning: {samples_with_cot}/{total_samples} ({samples_with_cot/total_samples*100:.1f}%)")
        self.logger.info(f"Samples with Modified Instructions: {samples_with_different_instruction}/{total_samples} ({samples_with_different_instruction/total_samples*100:.1f}%)")
        self.logger.info(f"{'='*80}")
        
        # è¾“å‡ºScoring Health Summaryï¼ˆå¦‚æœæœªè·³è¿‡ Stage4ï¼‰
        if not self.skip_stage4:
            self.logger.info(f"\n{'='*80}")
            self.logger.info("Scoring Health Summary:")
            self.logger.info(f"{'='*80}")
            
            health = scoring_health_summary
            if 'overall_status' in health:
                self.logger.info(f"Overall Status: {health['overall_status'].upper()}")
                
                # Primary Scoringï¼ˆåªåœ¨enable_primary_scoringæ—¶è¾“å‡ºï¼‰
                if self.enable_primary_scoring and 'primary_scoring' in health:
                    primary = health['primary_scoring']
                    self.logger.info(f"\nPrimary Scoring:")
                    self.logger.info(f"  Status: {primary['status'].upper()}")
                    self.logger.info(f"  Total Samples: {primary['total_samples']}")
                    self.logger.info(f"  Failed Samples: {primary['failed_samples']} ({primary['failure_rate']:.1f}%)")
                    if primary.get('failed_gpus'):
                        self.logger.info(f"  Failed GPUs: {primary['failed_gpus']}")
                
                # Refined Scoring
                if 'refined_scoring' in health:
                    refined = health['refined_scoring']
                    self.logger.info(f"\nRefined Scoring:")
                    self.logger.info(f"  Status: {refined['status'].upper()}")
                    self.logger.info(f"  Total Samples: {refined['total_samples']}")
                    self.logger.info(f"  Failed Samples: {refined['failed_samples']} ({refined['failure_rate']:.1f}%)")
                    if refined.get('failed_gpus'):
                        self.logger.info(f"  Failed GPUs: {refined['failed_gpus']}")
                
                # Warning Message
                if health.get('warning_message'):
                    self.logger.warning(f"\nâš ï¸  {health['warning_message']}")
            else:
                self.logger.info(health.get('note', 'Scoring health statistics not available'))
            
            self.logger.info(f"{'='*80}")
        
        return report
    
    def _get_timestamp(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    def _compute_scoring_health_summary(self) -> Dict[str, Any]:
        """
        è®¡ç®—è¯„åˆ†å¥åº·åº¦æ±‡æ€»ç»Ÿè®¡ï¼ˆåŒ…å« primary å’Œ refined è¯„åˆ†ï¼‰
        
        Returns:
            è¯„åˆ†å¥åº·åº¦æ±‡æ€»å­—å…¸
        """
        # åˆ¤æ–­å¥åº·çŠ¶æ€è¾…åŠ©å‡½æ•°
        def get_health_status(failure_rate):
            if failure_rate == 0:
                return "healthy"
            elif failure_rate < 10:
                return "warning"
            elif failure_rate < 50:
                return "degraded"
            else:
                return "critical"
        
        # ===== Primaryè¯„åˆ†å¥åº·åº¦ï¼ˆåªåœ¨enable_primary_scoringæ—¶è®¡ç®—ï¼‰=====
        if self.enable_primary_scoring:
            # åˆå¹¶æ‰€æœ‰å¤±è´¥çš„GPUï¼ˆå»é‡ï¼‰
            all_failed_gpus_primary = set()
            for category, failures in self.scoring_health['primary_failures'].items():
                all_failed_gpus_primary.update(failures['failed_gpus'])
            
            # è®¡ç®—å¤±è´¥ç™¾åˆ†æ¯”
            primary_failure_rate = 0.0
            if self.scoring_health['total_primary_samples'] > 0:
                primary_failure_rate = (self.scoring_health['total_primary_failures'] / 
                                       self.scoring_health['total_primary_samples'] * 100)
            
            primary_status = get_health_status(primary_failure_rate)
        
        # ===== Refinedè¯„åˆ†å¥åº·åº¦ï¼ˆæ€»æ˜¯è®¡ç®—ï¼‰=====
        # åˆå¹¶æ‰€æœ‰å¤±è´¥çš„GPUï¼ˆå»é‡ï¼‰
        all_failed_gpus_refined = set()
        for category, failures in self.scoring_health['refined_failures'].items():
            all_failed_gpus_refined.update(failures['failed_gpus'])
        
        # è®¡ç®—å¤±è´¥ç™¾åˆ†æ¯”
        refined_failure_rate = 0.0
        if self.scoring_health['total_refined_samples'] > 0:
            refined_failure_rate = (self.scoring_health['total_refined_failures'] / 
                                   self.scoring_health['total_refined_samples'] * 100)
        
        refined_status = get_health_status(refined_failure_rate)
        
        # ===== æ„å»ºè¯¦ç»†çš„å¤±è´¥ä¿¡æ¯ï¼ˆæŒ‰ç±»åˆ«ï¼‰=====
        category_details = {}
        for category in self.scoring_health['refined_failures'].keys():
            refined_fail = self.scoring_health['refined_failures'].get(category, {})
            category_detail = {
                "refined_scoring": {
                    "total_samples": refined_fail.get('total_samples', 0),
                    "failed_samples": refined_fail.get('failed_sample_count', 0),
                    "failure_rate": (refined_fail.get('failed_sample_count', 0) / 
                                    refined_fail.get('total_samples', 1) * 100) if refined_fail.get('total_samples', 0) > 0 else 0.0,
                    "failed_gpus": refined_fail.get('failed_gpus', [])
                }
            }
            
            # æ·»åŠ primaryå¤±è´¥ä¿¡æ¯ï¼ˆåªåœ¨enable_primary_scoringæ—¶ï¼‰
            if self.enable_primary_scoring:
                primary_fail = self.scoring_health['primary_failures'].get(category, {})
                category_detail["primary_scoring"] = {
                    "total_samples": primary_fail.get('total_samples', 0),
                    "failed_samples": primary_fail.get('failed_sample_count', 0),
                    "failure_rate": (primary_fail.get('failed_sample_count', 0) / 
                                    primary_fail.get('total_samples', 1) * 100) if primary_fail.get('total_samples', 0) > 0 else 0.0,
                    "failed_gpus": primary_fail.get('failed_gpus', [])
                }
            
            category_details[category] = category_detail
        
        # ===== æ„å»ºæ±‡æ€»ä¿¡æ¯ =====
        # ç»¼åˆçŠ¶æ€ï¼šå–primaryå’Œrefinedä¸­è¾ƒå·®çš„çŠ¶æ€
        if self.enable_primary_scoring:
            status_priority = {"healthy": 0, "warning": 1, "degraded": 2, "critical": 3}
            overall_status = max([primary_status, refined_status], key=lambda s: status_priority.get(s, 0))
        else:
            overall_status = refined_status
        
        summary = {
            "overall_status": overall_status,
            "refined_scoring": {
                "status": refined_status,
                "total_samples": self.scoring_health['total_refined_samples'],
                "failed_samples": self.scoring_health['total_refined_failures'],
                "failure_rate": round(refined_failure_rate, 2),
                "failed_gpus": sorted(list(all_failed_gpus_refined))
            },
            "category_details": category_details,
            "warning_message": self._generate_health_warning_message(
                primary_failure_rate if self.enable_primary_scoring else None,
                refined_failure_rate
            )
        }
        
        # æ·»åŠ primaryå¥åº·åº¦ï¼ˆåªåœ¨enable_primary_scoringæ—¶ï¼‰
        if self.enable_primary_scoring:
            summary["primary_scoring"] = {
                "status": primary_status,
                "total_samples": self.scoring_health['total_primary_samples'],
                "failed_samples": self.scoring_health['total_primary_failures'],
                "failure_rate": round(primary_failure_rate, 2),
                "failed_gpus": sorted(list(all_failed_gpus_primary))
            }
        
        return summary
    
    def _generate_health_warning_message(self, primary_failure_rate: Optional[float], refined_failure_rate: float) -> Optional[str]:
        """
        ç”Ÿæˆå¥åº·è­¦å‘Šä¿¡æ¯
        
        Args:
            primary_failure_rate: Primaryè¯„åˆ†å¤±è´¥ç‡ï¼ˆå¯é€‰ï¼Œåªåœ¨enable_primary_scoringæ—¶æä¾›ï¼‰
            refined_failure_rate: Refinedè¯„åˆ†å¤±è´¥ç‡
        
        Returns:
            è­¦å‘Šä¿¡æ¯ï¼Œå¦‚æœæ— é—®é¢˜åˆ™è¿”å›None
        """
        messages = []
        
        # Primaryå¤±è´¥ç‡æ£€æŸ¥ï¼ˆåªåœ¨enable_primary_scoringæ—¶ï¼‰
        if primary_failure_rate is not None:
            if primary_failure_rate >= 50:
                messages.append(f"CRITICAL: Primary scoring failed for {primary_failure_rate:.1f}% of samples (possible CUDA OOM)")
            elif primary_failure_rate >= 10:
                messages.append(f"WARNING: Primary scoring failed for {primary_failure_rate:.1f}% of samples")
            elif primary_failure_rate > 0:
                messages.append(f"Note: Primary scoring failed for {primary_failure_rate:.1f}% of samples")
        
        # Refinedå¤±è´¥ç‡æ£€æŸ¥
        if refined_failure_rate >= 50:
            messages.append(f"CRITICAL: Refined scoring failed for {refined_failure_rate:.1f}% of samples (possible CUDA OOM)")
        elif refined_failure_rate >= 10:
            messages.append(f"WARNING: Refined scoring failed for {refined_failure_rate:.1f}% of samples")
        elif refined_failure_rate > 0:
            messages.append(f"Note: Refined scoring failed for {refined_failure_rate:.1f}% of samples")
        
        if messages:
            messages.append("Failed samples are assigned default result 'no'")
            return " | ".join(messages)
        
        return None

